{"cells":[{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13368,"status":"ok","timestamp":1678550203279,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"lxnGMZB2FEJb","outputId":"debe03be-dc8e-4e7b-beb8-64f021583d8d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m46.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m85.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.9/dist-packages (from transformers) (6.0)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.13.1-py3-none-any.whl (199 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.2/199.2 KB\u001b[0m \u001b[31m16.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.9/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.9/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.9/dist-packages (from transformers) (4.65.0)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.9/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.9/dist-packages (from transformers) (1.22.4)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.9/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.5.0)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (1.26.14)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.9/dist-packages (from requests->transformers) (2022.12.7)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.13.1 tokenizers-0.13.2 transformers-4.26.1\n"]}],"source":["!pip install transformers\n","import pandas as pd\n","import numpy as np\n","from tqdm.auto import tqdm  # progress bar\n","import tensorflow as tf\n","from datetime import datetime"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Y9zmlJbdFJWU"},"outputs":[],"source":["from google.colab import drive\n","drive.mount('/content/drive')"]},{"cell_type":"markdown","metadata":{"id":"bjkAgpdkUkBk"},"source":["Tokenize text & create dataset"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"XBsgQcc3Faqq"},"outputs":[],"source":["from transformers import BertTokenizer\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased') # ft -> maps word to numerical value in berts vocab"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"zBF8P0lyM6k2"},"outputs":[],"source":["\n","def DatasetMapFunction(input_ids, attn_masks, labels):\n","    return {\n","        'input_ids': input_ids,\n","        'attention_mask': attn_masks\n","    }, labels\n","\n","def create_dataset(df, label_name = 'ArgumentLevel', label_number = 5, batch_size = 16, max_length = 256):\n","\n","    # Tokenize text\n","    X_input_ids = np.zeros((len(df), max_length))\n","    X_attn_masks = np.zeros((len(df), max_length))\n","\n","    for i, text in tqdm(enumerate(df['Content'])):\n","        tokenized_text = tokenizer.encode_plus(\n","            text,\n","            max_length = max_length,\n","            truncation = True,\n","            padding = 'max_length',\n","            add_special_tokens = True, #add [CLS] [PAD] [SEP] tokens\n","            return_tensors = 'tf'\n","        )\n","        \n","        X_input_ids[i,:] = tokenized_text.input_ids\n","        X_attn_masks[i,:] = tokenized_text.attention_mask\n","    \n","\n","    # Generate labels - One-hot encoding\n","    labels = np.zeros((len(df), label_number)) \n","    labels[np.arange(len(df)), df[label_name].values] = 1\n","\n","\n","    # Create dataset object\n","    dataset = tf.data.Dataset.from_tensor_slices((X_input_ids, X_attn_masks, labels))\n","\n","    # Map dataset\n","    dataset = dataset.map(DatasetMapFunction)\n","\n","    # shuffle data\n","    dataset = dataset.shuffle(400,reshuffle_each_iteration=True).batch(batch_size, drop_remainder=True) # drop remainder 179/16 = 11...3, drop 3 data\n","\n","    return dataset"]},{"cell_type":"markdown","metadata":{"id":"wO3b7tbFUggu"},"source":["Split training and validation data "]},{"cell_type":"code","execution_count":null,"metadata":{"id":"9fgWefEfGZtv"},"outputs":[],"source":["def split_dataset(dataset, p = 0.5, batch_size = 16):\n","    # p - training data size %\n","    train_size = int((len(df)//batch_size)*p)\n","    \n","    train_dataset = dataset.take(train_size) # take the first 8 batches\n","    val_dataset = dataset.skip(train_size) # take the last 3 batches\n","\n","    return train_dataset, val_dataset"]},{"cell_type":"markdown","metadata":{"id":"eNtuEyQYG_m8"},"source":["Create model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6wvpvYMZHBNO"},"outputs":[],"source":["from transformers import TFBertModel\n","bert_model = TFBertModel.from_pretrained('bert-base-uncased')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Z3CFOIZTHD-L"},"outputs":[],"source":["def create_model(label_number = 5, max_length = 256):\n","\n","    # input layer\n","    input_ids = tf.keras.layers.Input(shape = (max_length), name = 'input_ids', dtype = 'int32')\n","    # attention layer\n","    attention_masks = tf.keras.layers.Input(shape = (max_length), name = 'attention_mask', dtype = 'int32')\n","\n","    # bert layer\n","    bert_layer = bert_model.bert(input_ids, attention_mask=attention_masks)[1] \n","\n","    # intermediate layer\n","    intermediate_layer = tf.keras.layers.Dense(512, activation='relu', name = 'intermediate_layer')(bert_layer)\n","\n","    # output layer - 5 layers because 5 label classes\n","    output_layer = tf.keras.layers.Dense(label_number, activation='softmax', name = 'output_layer')(intermediate_layer)\n","\n","    # Create model object\n","    model = tf.keras.Model(inputs=[input_ids, attention_masks], outputs=output_layer)\n","\n","\n","\n","    # Add optimizer & Loss function & Accuracy metrics\n","    optim = tf.keras.optimizers.Adam(learning_rate=1e-5)\n","    loss_func = tf.keras.losses.CategoricalCrossentropy()\n","    acc = tf.keras.metrics.CategoricalAccuracy('accuracry')\n","\n","    model.compile(optimizer = optim, loss = loss_func, metrics = [acc])\n","\n","    return model"]},{"cell_type":"markdown","metadata":{"id":"_byWROoocjR3"},"source":["\n","========= 分界线 ==========\n","=========================="]},{"cell_type":"code","execution_count":null,"metadata":{"id":"eZpn5E04s7FJ"},"outputs":[],"source":["year = 'Y1'\n","label_name = 'ArgumentLevel'\n","label_number = 5\n","\n","run_num = 5\n","e = 4 # epochs = 4 default"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ptazbVjsFLPT"},"outputs":[],"source":["folderpath = '/content/drive/MyDrive/Imperial/nlp-physicseducation/outputs'\n","outputpath = '/content/drive/MyDrive/Imperial/BERT-results/20230309'\n","filepath = '/sections/labels_cleaned_{year}.csv'.format(year = year)"]},{"cell_type":"markdown","metadata":{"id":"G5oykLP4FQhE"},"source":["Load data"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7mZuz0yUFTVU"},"outputs":[],"source":["df = pd.read_csv(folderpath+filepath)\n","# df.head()\n","df['ArgumentLevel'] = df['ArgumentLevel'].replace({'superficial': 0, 'extended': 1, 'deep': 2, 'expert': 3, 'prediction': 4})\n","df['ReasoningLevel'] = df['ReasoningLevel'].replace({'bal': 0, 'the': 1, 'exp': 2, 'none': 3})\n","# df['ReasoningLevel'].value_counts()"]},{"cell_type":"markdown","metadata":{"id":"rIsiokYySxJZ"},"source":["/Users/jiayangzhang/Library/CloudStorage/GoogleDrive-jiayang.zhang@icloud.com/My Drive/Imperial/nlp-physicseducation/outputs/sections"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6kNRyCaYIC8R"},"outputs":[],"source":["train_ratios = [2,3,4,5,6,7,8,9] \n","for s in train_ratios: # training ratio if p=5, train ratio = 0.5\n","    for i in range(run_num):\n","        # about dataset\n","        dataset = create_dataset(df, label_name = label_name, label_number = label_number, batch_size = 16, max_length=512) # create dataset\n","        train_dataset, val_dataset = split_dataset(dataset, p = (0.1*s), batch_size = 16) # split training and validation data \n","\n","\n","        # about model\n","        model = create_model(label_number = label_number, max_length = 512)   # Create model\n","\n","\n","        # about train and validate\n","        history = model.fit(     # Train & Validate model (Fine-tuning)\n","            train_dataset,\n","            validation_data = val_dataset,\n","            epochs = e\n","        )\n","\n","\n","        history_saveto = outputpath + '/{year}/{label_name}/trainsize0{size}/{time}_{year}_{label_name}_{size}trainsize_{e}epochs_16batchsize_1e-5lr.npy'.format(\n","            time = datetime.now().strftime(\"%H%M%S\"), \n","            year = year, \n","            label_name = label_name, \n","            size = s,\n","            e = e\n","            )\n","        np.save(history_saveto, history.history) # save history\n","\n","\n","        # clear model after training\n","        tf.keras.backend.clear_session()\n","        import gc\n","        gc.collect()\n","        del model"]},{"cell_type":"markdown","metadata":{"id":"wDZX-AQmIW3N"},"source":["Save model"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ga4Et7SZIWja"},"outputs":[],"source":["model_saveto = './}_{}_trainsize{}'\n","model.save('Y1Y2_argument_model')"]},{"cell_type":"markdown","metadata":{"id":"exMiO1iKc7fs"},"source":[]}],"metadata":{"accelerator":"GPU","colab":{"machine_shape":"hm","provenance":[]},"gpuClass":"premium","kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}