{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" #default 'last_expr'\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import usual data analysis tools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "#np.random.seed(2018)\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import path_pdf,path_pkl,plot_freq_dist,get_top_n_words,plot_words_freq\n",
    "#from helper import path_pdf, path_pkl, plot_freq_dist, get_top_n_words, plot_words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## II. Loading in and filtering the data\n",
    "\n",
    "The datafile we use for this analysis is a pickle file containing processed versions of PERC papers from 2001 to 2018. We have scraped the available PDFs, then done the following data cleaning on the scraped text:\n",
    "1. Removed references, acknowledgments, keywords, and PACS \n",
    "2. Removed all numbers, symbols, punctuation, characters, and section headers\n",
    "3. Removed \"stop words\" (words like \"and\", \"or\", \"is\", etc. which do not carry specific meaning)\n",
    "4. Lowercased all words\n",
    "5. Lemmatized all words, reducing them to their more basic form (for example, reducing \"tests\", \"testing\", and \"tested\" to \"test\")\n",
    "6. Created bi-grams: combining commonly-associated words into one (for example, \"problem\" and \"solving\" into \"problem_solving\")\n",
    "7. Turned the resulting text into a list of individual words, or \"tokens\"\n",
    "\n",
    "This processed data was then stored in a datafile, which we now load in:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "data_words_bigrams = pd.read_pickle('Pickledfiles/Pickledfilestexts_norefs_clean.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A. Plotting top words in all documents\n",
    "\n",
    "Now, we will do some investigation and filtering based on word frequency. Our goal is to filter out the words that occur in a large number of documents, which are less likely to carry any distinct meaning for any specific theories, methods, or research traditions in PER. For example, most people in the PER community talk about \"physics\", \"education\", and \"students\" in one form or another. Those words do not carry much meaning, and so should be removed from our dataset in order to make sure that the more interesting, distinct, and meaningful words are prioritized in the analysis.\n",
    "\n",
    "We start by defining and implementing some functions to plot the word frequency distribution in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total words: \u001b[1m44\u001b[0m, unique words: \u001b[1m22\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "corpus = [ w for doc in data_words_bigrams for w in doc ]\n",
    "print('Total words: \\033[1m%d\\033[0m, unique words: \\033[1m%d\\033[0m' % (len(corpus), len(set(corpus))))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'helpers' has no attribute 'get_top_n_words'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-ccab191dd733>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstyle\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0muse\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'science'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'ieee'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'no-latex'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'font'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfamily\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'times new roman'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[1;33m[\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mids\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_top_n_words\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcorpus\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_top_words\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[0mfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_freq_dist\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrange\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m8000\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mfig\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0max\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mhelpers\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mplot_words_freq\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfreq\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m15\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: module 'helpers' has no attribute 'get_top_n_words'"
     ]
    }
   ],
   "source": [
    "# imports for scienceplots\n",
    "import matplotlib\n",
    "import scienceplots\n",
    "import helpers\n",
    "plt.style.use(['science', 'ieee','no-latex'])\n",
    "matplotlib.rc('font', family='times new roman')\n",
    "[words, freq, ids] = helpers.get_top_n_words(corpus, n_top_words=None)\n",
    "fig = helpers.plot_freq_dist(freq, range=[0,8000])\n",
    "fig, ax = helpers.plot_words_freq(words, freq, 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B. Creating a Bag of Words to filter the dataset\n",
    "\n",
    "Thankfully, Gensim already has a built-in function to do this kind of filtering, \"filter_extremes\", which allows us to filter out words that appear too many or too few times. The parameters are:\n",
    "* **no_below**: an integer. Keep tokens which are contained in at least no_below documents.  \n",
    "* **no_above**: a float number between 0 and 1. It filters out tokens which are contained in more than no_above percentage of documents. E.g. with no_above=0.5, tokens in in more than 50% of the documents get cut out.  \n",
    "* **keep_n**: an integer. It specifies how many tokens should be kept, starting with the most frequent.\n",
    "\n",
    "However, to use this function we must first create a [Gensim Dictionary](https://radimrehurek.com/gensim/corpora/dictionary.html#gensim.corpora.dictionary.Dictionary), which is a mapping between each of the entries (i.e. normalized words) and its integer id in the corpus. \n",
    "\n",
    "Based on repeated runs with different filtering values, we've set the no_above parameter to 0.55, which filters out 103 words (displayed below). We have also tried out a variety of no_below values, and have found that no_below 15 gives us more coherent topics. With the filter set to 15, we remove a large amount of the dataset (roughly 24,300 words). However, our primary goal is to find themes and topics that have been prevalent across the PER community over time, and because none of these words occurs in more than 15 documents (out of ~1300), we feel that they are unlikely to add a lot of meaning to the analysis.\n",
    "\n",
    "After the filtering we convert the remaining tokens into a \"bag of words\" (BoW)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dictionary with \u001b[1mno_above=1\u001b[0m contains \u001b[1m25073\u001b[0m unique words\n",
      "\n",
      "Dictionary filtered with \u001b[1mno_above=0.75\u001b[0m contains \u001b[1m25053\u001b[0m unique words. \u001b[1m20 words\u001b[0m removed:\n",
      " ['education', 'follow', 'give', 'group', 'include', 'learn', 'need', 'present', 'problem', 'result', 'school', 'science', 'student', 'study', 'teach', 'teacher', 'time', 'use', 'way', 'work'] \n",
      "\n",
      "Dictionary filtered with \u001b[1mno_above=0.65\u001b[0m contains \u001b[1m25025\u001b[0m unique words. \u001b[1m28 words\u001b[0m removed:\n",
      " ['base', 'change', 'class', 'course', 'different', 'experience', 'fact', 'form', 'general', 'great', 'high', 'important', 'individual', 'know', 'knowledge', 'level', 'material', 'mean', 'method', 'new', 'number', 'point', 'provide', 'question', 'research', 'state', 'understand', 'year'] \n",
      "\n",
      "Dictionary filtered with \u001b[1mno_above=0.60\u001b[0m contains \u001b[1m24996\u001b[0m unique words. \u001b[1m29 words\u001b[0m removed:\n",
      " ['activity', 'area', 'begin', 'case', 'come', 'consider', 'develop', 'development', 'example', 'help', 'ing', 'make', 'nature', 'order', 'place', 'possible', 'process', 'program', 'purpose', 'report', 'second', 'set', 'show', 'subject', 'take', 'term', 'test', 'type', 'value'] \n",
      "\n",
      "Dictionary filtered with \u001b[1mno_above=0.55\u001b[0m contains \u001b[1m24969\u001b[0m unique words. \u001b[1m27 words\u001b[0m removed:\n",
      " ['ability', 'analysis', 'appear', 'attempt', 'classroom', 'college', 'concept', 'content', 'curriculum', 'data', 'design', 'determine', 'discussion', 'idea', 'increase', 'indicate', 'information', 'involve', 'large', 'like', 'little', 'related', 'require', 'suggest', 'think', 'well', 'write'] \n",
      "\n",
      "Dictionary filtered with \u001b[1mno_above=0.50\u001b[0m contains \u001b[1m24940\u001b[0m unique words. \u001b[1m29 words\u001b[0m removed:\n",
      " ['able', 'approach', 'ask', 'certain', 'complete', 'concern', 'developed', 'difference', 'educational', 'effect', 'end', 'evidence', 'good', 'high_school', 'instruction', 'lead', 'life', 'major', 'necessary', 'note', 'particular', 'practice', 'reason', 'select', 'similar', 'situation', 'support', 'university', 'view'] \n",
      "\n",
      "Dictionary filtered with \u001b[1mno_above=0.45\u001b[0m contains \u001b[1m24896\u001b[0m unique words. \u001b[1m44 words\u001b[0m removed:\n",
      " ['addition', 'answer', 'apply', 'aspect', 'author', 'available', 'basis', 'best', 'call', 'child', 'common', 'compare', 'direct', 'effort', 'expect', 'experiment', 'factor', 'field', 'go', 'grade', 'hand', 'have', 'interest', 'kind', 'lack', 'list', 'low', 'measure', 'offer', 'people', 'physical', 'procedure', 'produce', 'relationship', 'represent', 'response', 'scientific', 'small', 'social', 'table', 'taught', 'thing', 'thought', 'world'] \n",
      "\n",
      "Dictionary filtered with \u001b[1mno_above=0.40\u001b[0m contains \u001b[1m24828\u001b[0m unique words. \u001b[1m68 words\u001b[0m removed:\n",
      " ['accord', 'age', 'allow', 'appropriate', 'basic', 'believe', 'biology', 'book', 'carry', 'cause', 'center', 'clear', 'conduct', 'control', 'current', 'day', 'degree', 'described', 'discuss', 'educator', 'establish', 'examine', 'exist', 'explain', 'focus', 'future', 'goal', 'guide', 'identify', 'importance', 'improve', 'investigation', 'issue', 'laboratory', 'long', 'look', 'matter', 'model', 'objective', 'observation', 'observe', 'obtain', 'opportunity', 'paper', 'period', 'plan', 'principle', 'project', 'range', 'reading', 'receive', 'recognize', 'role', 'say', 'scientist', 'section', 'see', 'serve', 'skill', 'source', 'statement', 'structure', 'technique', 'theory', 'topic', 'total', 'try', 'word'] \n",
      "\n",
      "Dictionary filtered with \u001b[1mno_above=0.35\u001b[0m contains \u001b[1m24756\u001b[0m unique words. \u001b[1m72 words\u001b[0m removed:\n",
      " ['achieve', 'action', 'add', 'additional', 'analyze', 'application', 'article', 'associate', 'attitude', 'background', 'characteristic', 'community', 'comparison', 'concerned', 'conclusion', 'condition', 'construct', 'continue', 'deal', 'department', 'direction', 'early', 'education_vol', 'effective', 'elementary', 'emphasis', 'encourage', 'environment', 'especially', 'evaluation', 'express', 'far', 'feel', 'function', 'gain', 'human', 'item', 'later', 'light', 'likely', 'limited', 'member', 'mind', 'national', 'occur', 'organize', 'pattern', 'person', 'physic', 'position', 'previous', 'progress', 'read', 'reference', 'relation', 'respect', 'review', 'sample', 'sense', 'series', 'simple', 'single', 'special', 'specific', 'standard', 'step', 'task', 'training', 'turn', 'unit', 'useful', 'usually'] \n",
      "\n",
      "Dictionary filtered with \u001b[1mno_above=0.30\u001b[0m contains \u001b[1m24652\u001b[0m unique words. \u001b[1m104 words\u001b[0m removed:\n",
      " ['account', 'achievement', 'act', 'actual', 'actually', 'address', 'aim', 'american', 'assume', 'attention', 'average', 'behavior', 'body', 'bring', 'challenge', 'chemistry', 'clearly', 'complex', 'consideration', 'contain', 'context', 'contribute', 'cover', 'create', 'critical', 'decide', 'demonstrate', 'description', 'desire', 'draw', 'element', 'emphasize', 'employ', 'engage', 'essential', 'evaluate', 'explanation', 'explore', 'extent', 'figure', 'finally', 'finding', 'force', 'generally', 'held', 'history', 'hold', 'illustrate', 'implication', 'instance', 'instead', 'interaction', 'interested', 'interpret', 'interpretation', 'introduce', 'line', 'literature', 'man', 'manner', 'mathematics', 'meaning', 'mention', 'natural', 'new_york', 'organization', 'participate', 'particularly', 'personal', 'phenomenon', 'potential', 'power', 'practical', 'probably', 'professional', 'pupil', 'quality', 'real', 'recent', 'record', 'refer', 'regard', 'relate', 'remain', 'researcher', 'resource', 'reveal', 'score', 'self', 'separate', 'significant', 'society', 'stand', 'start', 'strong', 'stu_dent', 'subject_matter', 'suggests', 'tend', 'text', 'textbook', 'true', 'variety', 'want'] \n",
      "\n",
      "Dictionary filtered with \u001b[1mno_above=0.25\u001b[0m contains \u001b[1m24500\u001b[0m unique words. \u001b[1m152 words\u001b[0m removed:\n",
      " ['acquire', 'affect', 'agree', 'aid', 'al', 'answer_question', 'assign', 'association', 'assumption', 'aware', 'belief', 'building', 'category', 'central', 'choice', 'city', 'close', 'combine', 'comment', 'conclude', 'connection', 'consist', 'consistent', 'construction', 'contrast', 'contribution', 'correct', 'criterion', 'decision', 'demand', 'difficult', 'difficulty', 'directly', 'discipline', 'discover', 'discus', 'earlier', 'earth', 'easily', 'easy', 'elementary_school', 'enable', 'enter', 'entire', 'et_al', 'event', 'examination', 'experimental', 'extend', 'face', 'fail', 'fall', 'feature', 'frequently', 'fundamental', 'get', 'graduate', 'ground', 'grow', 'half', 'highly', 'home', 'hypothesis', 'indicates', 'influence', 'initial', 'inquiry', 'instructional', 'instrument', 'integrate', 'intend', 'introduction', 'investigate', 'key', 'language', 'lesson', 'local', 'main', 'maintain', 'majority', 'meeting', 'modern', 'move', 'non', 'object', 'old', 'one', 'open', 'opinion', 'original', 'outcome', 'outline', 'outside', 'part', 'past', 'perform', 'performance', 'perspective', 'phase', 'picture', 'planning', 'play', 'population', 'possibility', 'preparation', 'prepare', 'prepared', 'presentation', 'previously', 'primary', 'prior', 'product', 'propose', 'public', 'raise', 'reach', 'realize', 'relevant', 'respond', 'right', 'scale', 'sci_ence', 'search', 'secondary', 'seek', 'selection', 'service', 'seven', 'share', 'short', 'simply', 'solution', 'space', 'stage', 'strategy', 'success', 'successful', 'suggestion', 'summary', 'survey', 'system', 'talk', 'tell', 'tion', 'today', 'tool', 'traditional', 'united_state', 'us', 'variable', 'water', 'week'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "no_below = 15\n",
    "no_above = 0.75\n",
    "import copy\n",
    "id2word_unfiltered_above = gensim.corpora.Dictionary(data_words_bigrams) #Needed for a bug\n",
    "id2word_unfiltered_above.filter_extremes(no_below=no_below, no_above=1, keep_n=100000)\n",
    "print(\"Dictionary with \\033[1mno_above=1\\033[0m contains \\033[1m%d\\033[0m unique words\\n\" % (len(id2word_unfiltered_above)))\n",
    "\n",
    "id2word = gensim.corpora.Dictionary(data_words_bigrams)\n",
    "id2word.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000)\n",
    "\n",
    "#Effect of using no_above\n",
    "diff_set = set(list(id2word_unfiltered_above.values())) - set(list(id2word.values()))\n",
    "print(\"Dictionary filtered with \\033[1mno_above=%.2f\\033[0m contains \\033[1m%d\\033[0m unique words. \\033[1m%d words\\033[0m removed:\\n %s \\n\" % \n",
    "      (no_above, len(id2word), len(diff_set), sorted(diff_set)))\n",
    "\n",
    "no_above = 0.65\n",
    "id2word_prev = copy.deepcopy(id2word)\n",
    "id2word.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000) #NB: no problem filtering again on no_above\n",
    "diff_set = set(list(id2word_prev.values())) - set(list(id2word.values()))\n",
    "print(\"Dictionary filtered with \\033[1mno_above=%.2f\\033[0m contains \\033[1m%d\\033[0m unique words. \\033[1m%d words\\033[0m removed:\\n %s \\n\" % \n",
    "      (no_above, len(id2word), len(diff_set), sorted(diff_set)))\n",
    "\n",
    "no_above = 0.6\n",
    "id2word_prev = copy.deepcopy(id2word)\n",
    "id2word.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000) #NB: no problem filtering again on no_above\n",
    "diff_set = set(list(id2word_prev.values())) - set(list(id2word.values()))\n",
    "print(\"Dictionary filtered with \\033[1mno_above=%.2f\\033[0m contains \\033[1m%d\\033[0m unique words. \\033[1m%d words\\033[0m removed:\\n %s \\n\" % \n",
    "      (no_above, len(id2word), len(diff_set), sorted(diff_set)))\n",
    "\n",
    "no_above = 0.55\n",
    "id2word_prev = copy.deepcopy(id2word)\n",
    "id2word.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000) #NB: no problem filtering again on no_above\n",
    "diff_set = set(list(id2word_prev.values())) - set(list(id2word.values()))\n",
    "print(\"Dictionary filtered with \\033[1mno_above=%.2f\\033[0m contains \\033[1m%d\\033[0m unique words. \\033[1m%d words\\033[0m removed:\\n %s \\n\" % \n",
    "      (no_above, len(id2word), len(diff_set), sorted(diff_set)))\n",
    "\n",
    "\n",
    "\n",
    "no_above = 0.5\n",
    "id2word_prev = copy.deepcopy(id2word)\n",
    "id2word.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000) #NB: no problem filtering again on no_above\n",
    "diff_set = set(list(id2word_prev.values())) - set(list(id2word.values()))\n",
    "print(\"Dictionary filtered with \\033[1mno_above=%.2f\\033[0m contains \\033[1m%d\\033[0m unique words. \\033[1m%d words\\033[0m removed:\\n %s \\n\" % \n",
    "      (no_above, len(id2word), len(diff_set), sorted(diff_set)))\n",
    "\n",
    "no_above = 0.45\n",
    "id2word_prev = copy.deepcopy(id2word)\n",
    "id2word.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000) #NB: no problem filtering again on no_above\n",
    "diff_set = set(list(id2word_prev.values())) - set(list(id2word.values()))\n",
    "print(\"Dictionary filtered with \\033[1mno_above=%.2f\\033[0m contains \\033[1m%d\\033[0m unique words. \\033[1m%d words\\033[0m removed:\\n %s \\n\" % \n",
    "      (no_above, len(id2word), len(diff_set), sorted(diff_set)))\n",
    "\n",
    "\n",
    "no_above = 0.4\n",
    "id2word_prev = copy.deepcopy(id2word)\n",
    "id2word.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000) #NB: no problem filtering again on no_above\n",
    "diff_set = set(list(id2word_prev.values())) - set(list(id2word.values()))\n",
    "print(\"Dictionary filtered with \\033[1mno_above=%.2f\\033[0m contains \\033[1m%d\\033[0m unique words. \\033[1m%d words\\033[0m removed:\\n %s \\n\" % \n",
    "      (no_above, len(id2word), len(diff_set), sorted(diff_set)))\n",
    "\n",
    "\n",
    "no_above = 0.35\n",
    "id2word_prev = copy.deepcopy(id2word)\n",
    "id2word.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000) #NB: no problem filtering again on no_above\n",
    "diff_set = set(list(id2word_prev.values())) - set(list(id2word.values()))\n",
    "print(\"Dictionary filtered with \\033[1mno_above=%.2f\\033[0m contains \\033[1m%d\\033[0m unique words. \\033[1m%d words\\033[0m removed:\\n %s \\n\" % \n",
    "      (no_above, len(id2word), len(diff_set), sorted(diff_set)))\n",
    "\n",
    "\n",
    "no_above = 0.3\n",
    "id2word_prev = copy.deepcopy(id2word)\n",
    "id2word.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000) #NB: no problem filtering again on no_above\n",
    "diff_set = set(list(id2word_prev.values())) - set(list(id2word.values()))\n",
    "print(\"Dictionary filtered with \\033[1mno_above=%.2f\\033[0m contains \\033[1m%d\\033[0m unique words. \\033[1m%d words\\033[0m removed:\\n %s \\n\" % \n",
    "      (no_above, len(id2word), len(diff_set), sorted(diff_set)))\n",
    "\n",
    "no_above = 0.25\n",
    "id2word_prev = copy.deepcopy(id2word)\n",
    "id2word.filter_extremes(no_below=no_below, no_above=no_above, keep_n=100000) #NB: no problem filtering again on no_above\n",
    "diff_set = set(list(id2word_prev.values())) - set(list(id2word.values()))\n",
    "print(\"Dictionary filtered with \\033[1mno_above=%.2f\\033[0m contains \\033[1m%d\\033[0m unique words. \\033[1m%d words\\033[0m removed:\\n %s \\n\" % \n",
    "      (no_above, len(id2word), len(diff_set), sorted(diff_set)))\n",
    "\n",
    "\n",
    "del id2word_prev"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8 (default, Apr 13 2021, 15:08:03) [MSC v.1916 64 bit (AMD64)]"
  },
  "vscode": {
   "interpreter": {
    "hash": "7bf9169aea3bf43030d2045b9506020b24091b3061df9b5cfee19d2eac11c6ed"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
