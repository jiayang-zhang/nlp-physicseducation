{"cells":[{"cell_type":"markdown","metadata":{"id":"PIZC4JRqGhP5"},"source":["# 1. Setup"]},{"cell_type":"markdown","metadata":{"id":"LNeYfLE7mGul"},"source":["## 1.1 Using Colab GPU for Training\n"]},{"cell_type":"markdown","metadata":{"id":"N3upHHQ1m6Zb"},"source":["Run the following the cell to confirm the GPU is detected."]},{"cell_type":"code","execution_count":1,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13090,"status":"ok","timestamp":1675428519596,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"0eZDJWhjmOcC","outputId":"b899ab4f-226f-4547-d209-6c7a5738cb76"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}],"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"]},{"cell_type":"markdown","metadata":{"id":"_bgovg_6hNCU"},"source":["In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in out training loop"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13610,"status":"ok","timestamp":1675428533203,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"3n7WnLmpg_Hj","outputId":"876e23a4-3d4e-4f66-efa2-2895cc2fad34"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}],"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"BH8PQKEhF0YD"},"source":["## 1.2 Installing Hugging Face Library"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11377,"status":"ok","timestamp":1675428544560,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"n6DvZooRmKrn","outputId":"ef849654-d122-4271-af04-37a6173d9c68"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.0-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m82.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m108.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m23.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.0\n"]}],"source":["pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"TxWOmaB9GT4q"},"source":["# 2. Retrieve Dataset"]},{"cell_type":"markdown","metadata":{"id":"L5UDujTsGY00"},"source":["## 2.1 Mount Google Drive"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":117231,"status":"ok","timestamp":1675428661782,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"4atrLvNzb0b3","outputId":"1664bafc-3f4c-4327-a2a4-897b16700622"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":11,"status":"ok","timestamp":1675428661783,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"CtXpJC70HM-H","outputId":"fff15be1-ace0-4692-bf7c-7ac09130c91b"},"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n","/root\n","/content/gdrive/MyDrive/nlp-physicseducation\n"]}],"source":["%cd\n","!pwd\n","%cd /content/gdrive/MyDrive/nlp-physicseducation/"]},{"cell_type":"markdown","metadata":{"id":"5pHTEYcyG-D9"},"source":["## 2.2 Parse Data"]},{"cell_type":"markdown","metadata":{"id":"Ln_QvsjDuynn"},"source":["Specify the directories"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1675428661783,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"s4EMjNyhu3bW"},"outputs":[],"source":["dir_csv = 'outputs/sections/labels_cleaned_y1y2.csv'"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":3053,"status":"ok","timestamp":1675428664830,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"4-gB9Op4Fts5","outputId":"29b06ac7-c732-457b-b805-1019bb5a881a"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of reports: 179\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["                  StudentID  \\\n","150  2ndINT_JWI426_Redacted   \n","26       GS_LXZ649_Redacted   \n","16       GS_PLS878_Redacted   \n","13       GS_LZH215_Redacted   \n","83       GS_KUY555_Redacted   \n","21       GS_JWW382_Redacted   \n","35       GS_HDP206_Redacted   \n","95       GS_KGR276_Redacted   \n","2        GS_NGT657_Redacted   \n","171  2ndINT_BYH124_Redacted   \n","\n","                                               Content ArgumentLevel  \\\n","150  the green laser interferogram show a clear sin...        expert   \n","26   t he graph for each colour 's d sin Œ∏ against...      extended   \n","16   i identify 3 major source of systematic error ...      extended   \n","13   the graph produce from the red spectral line b...          deep   \n","83   a expect the fourth spectral line correspond t...    prediction   \n","21   the final result obtain forr ‚àû from this exp...   superficial   \n","35   the data for the red emission line be show in ...   superficial   \n","95   for each set of measurement for Œ∏ the mean va...   superficial   \n","2    the graph below show the average of the data c...          deep   \n","171  look at the interferograms from both the green...      extended   \n","\n","    ReasoningLevel  \n","150            the  \n","26             bal  \n","16             bal  \n","13             the  \n","83             exp  \n","21             the  \n","35             bal  \n","95             bal  \n","2              bal  \n","171           none  "],"text/html":["\n","  <div id=\"df-2430c2f0-76f7-4e1c-afa2-285fd5860454\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>StudentID</th>\n","      <th>Content</th>\n","      <th>ArgumentLevel</th>\n","      <th>ReasoningLevel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>150</th>\n","      <td>2ndINT_JWI426_Redacted</td>\n","      <td>the green laser interferogram show a clear sin...</td>\n","      <td>expert</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>26</th>\n","      <td>GS_LXZ649_Redacted</td>\n","      <td>t he graph for each colour 's d sin Œ∏ against...</td>\n","      <td>extended</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>16</th>\n","      <td>GS_PLS878_Redacted</td>\n","      <td>i identify 3 major source of systematic error ...</td>\n","      <td>extended</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>13</th>\n","      <td>GS_LZH215_Redacted</td>\n","      <td>the graph produce from the red spectral line b...</td>\n","      <td>deep</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>83</th>\n","      <td>GS_KUY555_Redacted</td>\n","      <td>a expect the fourth spectral line correspond t...</td>\n","      <td>prediction</td>\n","      <td>exp</td>\n","    </tr>\n","    <tr>\n","      <th>21</th>\n","      <td>GS_JWW382_Redacted</td>\n","      <td>the final result obtain forr ‚àû from this exp...</td>\n","      <td>superficial</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>35</th>\n","      <td>GS_HDP206_Redacted</td>\n","      <td>the data for the red emission line be show in ...</td>\n","      <td>superficial</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>95</th>\n","      <td>GS_KGR276_Redacted</td>\n","      <td>for each set of measurement for Œ∏ the mean va...</td>\n","      <td>superficial</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>2</th>\n","      <td>GS_NGT657_Redacted</td>\n","      <td>the graph below show the average of the data c...</td>\n","      <td>deep</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>171</th>\n","      <td>2ndINT_BYH124_Redacted</td>\n","      <td>look at the interferograms from both the green...</td>\n","      <td>extended</td>\n","      <td>none</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-2430c2f0-76f7-4e1c-afa2-285fd5860454')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-2430c2f0-76f7-4e1c-afa2-285fd5860454 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-2430c2f0-76f7-4e1c-afa2-285fd5860454');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":7}],"source":["import pandas as pd\n","\n","# load the dataset into a pandas dataframe\n","df = pd.read_csv(\n","        dir_csv, \n","        encoding='utf-8', \n","        skiprows = 1, \n","        names=['StudentID', 'Content', 'ArgumentLevel', 'ReasoningLevel']\n",")\n","\n","# Report the number of reports\n","print('Number of reports: {:,}\\n'.format(df.shape[0]))\n","\n","# Display 10 random rows from the data\n","df.sample(10)"]},{"cell_type":"markdown","metadata":{"id":"ZfT4uprKJsRG"},"source":["The label 'ArgumentLevel' and 'ReasoningLevel' are mapped to numbers.\n","\n","Argument Level labels {'bal': 0, 'the': 1, 'exp': 2, 'none': 3}\n","\n","Reasoning Level labels {'extended': 0, 'deep': 1, 'expert': 2, 'superficial': 3, 'prediction': 4}"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1675428664830,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"3upygP5AH0NL","outputId":"2451c144-fa9a-4922-ec4b-bcde97281a3e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                  StudentID  \\\n","62       GS_WEM893_Redacted   \n","77       GS_TKK454_Redacted   \n","71       GS_GQV977_Redacted   \n","28       GS_GQR325_Redacted   \n","143  2ndINT_TAP080_Redacted   \n","142  2ndINT_GQV977_Redacted   \n","86       GS_JUA192_Redacted   \n","54       GS_DAG780_Redacted   \n","65       GS_BMS328_Redacted   \n","174  2ndINT_SKD544_Redacted   \n","\n","                                               Content  ArgumentLevel  \\\n","62   when calculate the wavelength of the light spe...              3   \n","77   a discuss in 2 the graph of sin Œ∏ against ord...              1   \n","71   the first calculation do be to find the value ...              1   \n","28   table 1 show 11 set of angle and correspond or...              3   \n","143  a. white lead a can be see the wavelength exte...              1   \n","142  the interferometer succeed in find relatively ...              3   \n","86   overall the experiment be carry out well with ...              0   \n","54   the reference angle at which the diffraction a...              1   \n","65   in conclusion despite the fact that the value ...              0   \n","174  the interferograms of a tungsten light source ...              0   \n","\n","    ReasoningLevel  \n","62             bal  \n","77             the  \n","71             the  \n","28             exp  \n","143            exp  \n","142            bal  \n","86             the  \n","54             bal  \n","65             bal  \n","174            bal  "],"text/html":["\n","  <div id=\"df-cae6daa8-d4ec-4787-925b-c1fffafd8ffc\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>StudentID</th>\n","      <th>Content</th>\n","      <th>ArgumentLevel</th>\n","      <th>ReasoningLevel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>62</th>\n","      <td>GS_WEM893_Redacted</td>\n","      <td>when calculate the wavelength of the light spe...</td>\n","      <td>3</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>77</th>\n","      <td>GS_TKK454_Redacted</td>\n","      <td>a discuss in 2 the graph of sin Œ∏ against ord...</td>\n","      <td>1</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>71</th>\n","      <td>GS_GQV977_Redacted</td>\n","      <td>the first calculation do be to find the value ...</td>\n","      <td>1</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>GS_GQR325_Redacted</td>\n","      <td>table 1 show 11 set of angle and correspond or...</td>\n","      <td>3</td>\n","      <td>exp</td>\n","    </tr>\n","    <tr>\n","      <th>143</th>\n","      <td>2ndINT_TAP080_Redacted</td>\n","      <td>a. white lead a can be see the wavelength exte...</td>\n","      <td>1</td>\n","      <td>exp</td>\n","    </tr>\n","    <tr>\n","      <th>142</th>\n","      <td>2ndINT_GQV977_Redacted</td>\n","      <td>the interferometer succeed in find relatively ...</td>\n","      <td>3</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>86</th>\n","      <td>GS_JUA192_Redacted</td>\n","      <td>overall the experiment be carry out well with ...</td>\n","      <td>0</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>GS_DAG780_Redacted</td>\n","      <td>the reference angle at which the diffraction a...</td>\n","      <td>1</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>65</th>\n","      <td>GS_BMS328_Redacted</td>\n","      <td>in conclusion despite the fact that the value ...</td>\n","      <td>0</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>174</th>\n","      <td>2ndINT_SKD544_Redacted</td>\n","      <td>the interferograms of a tungsten light source ...</td>\n","      <td>0</td>\n","      <td>bal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cae6daa8-d4ec-4787-925b-c1fffafd8ffc')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-cae6daa8-d4ec-4787-925b-c1fffafd8ffc button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-cae6daa8-d4ec-4787-925b-c1fffafd8ffc');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":8}],"source":["# define dict to code labels to numbers\n","ArgumentLevel_dict = {'extended': 0, 'deep': 1, 'expert': 2, 'superficial': 3, 'prediction': 4}\n","\n","# replace to number labels\n","df['ArgumentLevel'].replace(list(ArgumentLevel_dict.keys()), list(ArgumentLevel_dict.values()),inplace=True) \n","\n","\n","# Display 10 random rows from the data\n","df.sample(10)"]},{"cell_type":"markdown","metadata":{"id":"OmzBpfb-KMe1"},"source":["Let's extract the sentences and labels of our training set as numpy ndarrys."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1675428664831,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"e9U4SpCcQRJ8","outputId":"3c620c69-105a-4de4-a4b4-417919f116d3"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of reports: 179\n","\n","First report:     since the room be dark and it be hard to read the cramped number on the veriner the angle of diffraction could be measure imprecisely we conclude that this be the major factor that affected the inaccurate rydberg constant there could be various systematic error that affected the result we try to perfectly align the spectrometer but this could be unsuccessful also the position of the lamp should have be a close to the slit a possible since it be directly related to the intensity of the light this could make u see the cyan colour light the air pressure and humidity be also other source but we decide that they be negligible in the experiment we try to measure the rydberg constant use a hydrogen gas tube and a spectrometer and we get ùëÖ ‚àû 10.5 √ó 10 6 ¬± 4.4 √ó 10 6 ùëö ‚àí1 which be only 0.4 unit less than the theoretical value however we fail to obtain the precise value a our uncertainty be over 40 percent of our value and we conclude that the width of the slit be not narrow enough to produce the precise result this could be improve by repeat the experiment in a darker room with a narrower slit width and a clearer vernier a for reference this be the code that have be use for data analysis\n","Second report:    the data that we gather from the method be summarise in table 1 and table 2 we observe the light from the licl to be red and the light from the cuso 4 to be green. this be all do in python in conclusion the value of the wavelength for photon emit from licl be ùúÜ 682 √ó 10 ‚àí7 ¬± 800 √ó 10 ‚àí7 m the value for the wavelength of the photon emit from cuso 4 be ùúÜ 5.43 √ó 10 ‚àí7 ¬± 4.5 √ó 10 ‚àí8 m neither of the value for the wavelength that we have determine through our experiment agree with the accepted value for wavelength of the photon emit from the compound there be a percentage difference of 3 for licl and 11 for cuso 4 from the accepted value\n"]}],"source":["corpus = df.Content.values\n","ArgumentLevels = df.ArgumentLevel.values\n","\n","print('Number of reports: {:,}\\n'.format(len(corpus)))\n","print('First report:    ',corpus[0])\n","print('Second report:   ',corpus[1])"]},{"cell_type":"markdown","metadata":{"id":"PmOSw5EXRmPf"},"source":["# 3. Tokenization & Input Formatting\n","\n","In this section, we'll transform our dataset into the format that BERT can be trained on."]},{"cell_type":"markdown","metadata":{"id":"kxDtbEM0H186"},"source":["## 3.1 Tokenisation"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130,"referenced_widgets":["fcae84c2d2494fdaa57efc09b85f3aec","666a0629f575494998963fdaa6093412","f9f8d4b7a21546f0b5049465de438437","f66e05115bd644d28bde66d1e08a408d","f9db04b02bfd4be1905760a6b1c713e7","3e60b51db23e4b0aa92c265db91c23f7","31bbafeca8bd47fd9ce589f7f9dce6bf","4d31d8a86800457e88196abf3472b130","e71a56d6b7144642acc9405d098064d9","b54bc88884814bca9c64d502e4fd82fa","625391bbce68426eb8806d1e8fa9c5b8","acca920c63da4196989accd57fd63ae5","381d4fddb9a04df18a7e3a34ccdf8acb","333e5fe43a7b4e69ad378a121725498e","d26460f0c6754ff8bddc563ace1a1780","1c54c8fb8e8a471093c245d50feb7d1a","7467ce530f0b4d39bcffaa353d1ff1aa","02a08735dc5e4f0aa162dbcbf13905b2","86fefc9033744178b4c2c08e35051d0d","a145cb59a50944d4a6d0c6e4eaf667ec","df9d778edc494914bc952346800bc70c","69915fcb7a2f454ba8c7140a82f4a709","260252cc42e1427eb94303749da2e77f","956cfb9eb0d94a8cabc83372722e9c02","eb2b1161fa2e412bbfaf74023ef9acdc","50149c4fa6eb44fc8b1a796f170b969e","e730edf1e14648788717c3e4f3592a4c","13f9f2a1d74540348e5968d5bb3439a7","4b46b0cf58d345fcbcc8a84a2e636229","d2a1d2a822aa48939b482bd56bc6be9b","905b81b97baf43c0adeb5545b32aaf59","d601e237fc134b58a51cae1c891a4733","008569815bc04616857263b0c44a6720"]},"executionInfo":{"elapsed":8436,"status":"ok","timestamp":1675428673258,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"gx5dgBinVS2N","outputId":"3426cecc-ea2a-4912-ff62-8d0ff77837f4"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading BERT tokenizer...\n"]},{"output_type":"display_data","data":{"text/plain":["Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fcae84c2d2494fdaa57efc09b85f3aec"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"acca920c63da4196989accd57fd63ae5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":["Downloading (…)lve/main/config.json:   0%|          | 0.00/570 [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"260252cc42e1427eb94303749da2e77f"}},"metadata":{}}],"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer\n","print('loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"markdown","metadata":{"id":"nfxyOJYug5F1"},"source":["Now we're ready to perform the real tokenization.\n","\n","The `tokenizer.encode_plus` function combines multiple steps for us:\n","\n","1. Split the sentence into tokens.\n","2. Add the special `[CLS]` and `[SEP]` tokens.\n","3. Map the tokens to their IDs.\n","4. Pad or truncate all sentences to the same length.\n","5. Create the attention masks which explicitly differentiate real tokens from `[PAD]` tokens.\n","\n","The first four features are in `tokenizer.encode`, but I'm using `tokenizer.encode_plus` to get the fifth item (attention masks). Documentation is [here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus).\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3631,"status":"ok","timestamp":1675428676887,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"4M7BBNLMg4op","outputId":"96be0a62-45e5-41ba-a0d2-09d52ba019d9"},"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["# Tokenize all of the reports and map the tokens to their word IDs.\n","input_ids, attention_masks, lengths = [], [], []\n","\n","# For every report ...\n","for report in corpus:\n","    # 'encode_plus' will:\n","    #   (1) Tokenise the sentence.\n","    #   (2) Prepend the '[CLS]' token to the start\n","    #   (3) Append the '[SEP]' token to the end\n","    #   (4) Map tokens to their IDs\n","    #   (5) Pad or truncate the report to 'max_length'\n","    #   (6) Create attention masks for [PAD] tokens\n","    encoded_dict = tokenizer.encode_plus(\n","                        report,                     # report to encode\n","                        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n","                        max_length = 512,            # Pad & truncate all reports\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks\n","                        return_tensors = 'pt',          # return pytorch tensors\n","\n","    )\n","\n","    # Add the encoded report to the list \n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    # And its attention mask (simply differentiates padding from non-padding)\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # lengths.append(len(encoded_dict['input_ids']))\n","    lengths.append(len(encoded_dict['input_ids'][0]))\n"]},{"cell_type":"markdown","metadata":{"id":"VRmQt6adGKPW"},"source":["Set which label type to train with; and convert input lists of tensors."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":21,"status":"ok","timestamp":1675428676888,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"JxSdH_cxGN98"},"outputs":[],"source":["# change input labels here ***\n","labels = ArgumentLevels   \n","num_labels = 5    #4 for ReasoningLevels #5 for ArgumentLevels\n","\n","\n","# Convert the lists into tensors\n","input_ids = torch.cat(input_ids, dim = 0)\n","attention_masks = torch.cat(attention_masks, dim = 0)\n","labels = torch.tensor(labels)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":20,"status":"ok","timestamp":1675428676888,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"fBNaFKj8Hm4J"},"outputs":[],"source":["# Print report 0, now as a list of IDs\n","# print('Original:', corpus[0])\n","# print('Token IDs:', input_ids[0])"]},{"cell_type":"markdown","metadata":{"id":"XwBJenjTQSO6"},"source":["## 3.2 Report Length distribution (Discarded)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1675428676888,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"TaTbhoegXmiF","outputId":"f7bc1b88-373d-48e6-9bd4-2c7dde61d694"},"outputs":[{"output_type":"stream","name":"stdout","text":["Min length: 512 tokens\n","Max length: 512 tokens\n","Median length: 512.0 tokens\n"]}],"source":["import numpy as np\n","print('Min length: {:,} tokens'.format(min(lengths)))\n","print('Max length: {:,} tokens'.format(max(lengths)))\n","print('Median length: {:,} tokens'.format(np.median(lengths)))\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":2018,"status":"ok","timestamp":1675428678888,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"2EXPYdmOQR7l","outputId":"a3676b5e-089d-4835-f295-75765c69f9ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(10.314999999999998, 0.5, '# of Reports')"]},"metadata":{},"execution_count":15},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np \n","\n","sns.set(style = 'darkgrid')\n","\n","# Increase the plot size and font size\n","sns.set(font_scale = 1.5)\n","plt.rcParams[\"figure.figsize\"] = (10,5)\n","\n","# Truncate any report lengths greater than 512\n","lengths = [min(l,512) for l in lengths]\n","\n","# Plot the distribution of comment lengths\n","sns.displot(lengths, kde=False, rug=False)\n","\n","plt.title(\"Report Lengths\")\n","plt.xlabel(\"Report Length\")\n","plt.ylabel(\"# of Reports\")\n"]},{"cell_type":"markdown","metadata":{"id":"IyD2pukar1QO"},"source":["How many data (reports) could be faulty? "]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1675428678888,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"3yLVlc7Er8cI","outputId":"e7e0f4b6-36e5-46b6-ea2a-f84320de06a4"},"outputs":[{"output_type":"stream","name":"stdout","text":["# of possible faulty reports:  0\n"]}],"source":["# Count the number of suspicious reports that didn't not have full text extracted \n","counter = 0\n","for l in lengths:\n","    if l<20:\n","        counter+=1\n","print('# of possible faulty reports: ', counter)\n"]},{"cell_type":"markdown","metadata":{"id":"6haaD5vpf-Qy"},"source":["How many reports run into the 512-token limit?"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7,"status":"ok","timestamp":1675428678889,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"SKHvKyv6gExI","outputId":"5afe7c03-4621-4a55-c6e4-333b3968acc3"},"outputs":[{"output_type":"stream","name":"stdout","text":["179 of 179 sentences (100.0%) of corpus are longer than 512 tokens\n"]}],"source":["# Count the number of sentences that had to be truncated to 512 tokens\n","num_truncated = lengths.count(512)\n","\n","# Compare this to the total number of training reports\n","num_reports = len(lengths)\n","prcnt = float(num_truncated)/float(num_reports)\n","\n","print('{:,} of {:,} sentences ({:.1%}) of corpus are longer than 512 tokens'.format(\n","        num_truncated, num_reports, prcnt))\n"]},{"cell_type":"markdown","metadata":{"id":"ytev8umirwoF"},"source":["## 3.3 Training and Validation Split\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["cd5c08ca99f24f20956439f6eed6bd9a","c48138f3e1054aedbcd7ab46c2565b42","b6233021bd4145a88739055c0d7dc13e","5f25b0d563b24c3187118d2be6d8942f","cb8c6efc4b2d4e29a478d8360eea88f0","4282e0d10660487281094f4b2f7a81a5","357292e7280f430989e12b02e1330e10","685b83f12d8a4738812018903f14aa38","ca2d104f0c4b4f2984cec3c371f59a25","ee2a2723f95d4f9d83e58618d07b3d69","7c5f1a1095774bd3b39b44d5f07ced27"]},"executionInfo":{"elapsed":16581,"status":"ok","timestamp":1675428695463,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"TOUbbujd7coW","outputId":"118480f4-5a28-473a-af62-45f00c5c6dce"},"outputs":[{"output_type":"display_data","data":{"text/plain":["Downloading (…)\"pytorch_model.bin\";:   0%|          | 0.00/440M [00:00<?, ?B/s]"],"application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"cd5c08ca99f24f20956439f6eed6bd9a"}},"metadata":{}},{"output_type":"stream","name":"stderr","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.decoder.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.weight']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"output_type":"execute_result","data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=5, bias=True)\n",")"]},"metadata":{},"execution_count":18}],"source":["# Load BertForSequenceClassification, the pre-trained BERT model with a \n","# single linear classification layer on top.\n","\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","model = BertForSequenceClassification.from_pretrained(\n","        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab\n","        num_labels = num_labels, # the number of ourput labels -- 5 for five ArgumentLevel classification labels\n","        output_attentions = False, # whether the model returns attention weights\n","        output_hidden_states = False, # whether the model returns all hidden-states\n",")\n","\n","# Tell pytorch to run this model on the GPU\n","model.cuda()\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1675428695464,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"ewM603LvDo02"},"outputs":[],"source":["# Create helper function\n","import numpy as np\n","import random\n","import time\n","import datetime\n","\n","\n","def flat_accuracy(preds, labels):\n","    '''\n","    Function to calculate the accuracy of our predictions vs labels\n","    '''\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def format_time(elapsed):\n","    '''\n","    Taks a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second\n","    elapsed_rounded = int(round(elapsed))\n","\n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds = elapsed_rounded))\n","\n","def to_dataframe(dict):\n","    # Display floats with two decimal places.\n","    pd.set_option('precision', 3)\n","    df_stats = pd.DataFrame(data=dict)\n","    df_stats = df_stats.set_index('epoch')\n","    # A hack to force the column headers to wrap.\n","    # df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","    return df_stats"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1675428695464,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"1IYjV_u8AqVr"},"outputs":[],"source":["import numpy as np\n","import random\n","from torch.utils.data import TensorDataset, random_split\n","from tensorflow.python.ops.variables import validate_synchronization_aggregation_trainable\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import get_linear_schedule_with_warmup\n","\n","\n","# define a function for train-validation data splitting\n","def train_val_split(dataset, ratio):\n","    '''\n","    # Create a ratio:(1-ratio) train-validation split\n","    \n","    dataset: tensor object\n","    ratio: float <1 and >0\n","    '''\n","    # Calculate the number of samples to include in each set\n","    train_size = int(ratio * len(dataset))\n","    val_size = len(dataset) - train_size\n","\n","    # Divide the dataset by randomly selecting samples\n","    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","    print('{:>5,} training samples'.format(train_size))\n","    print('{:>5,} validation samples'.format(val_size))\n","    \n","    return train_dataset, val_dataset\n","\n","\n","\n","# Create the DataLoaders for our training and validation sets.\n","def data_loader(train_dataset, val_dataset,\n","                batch_size):\n","\n","    # We'll take training samples in random order\n","    train_dataloader = DataLoader(\n","                        train_dataset,                  # the trainig samples\n","                        sampler = RandomSampler(train_dataset), # select batches randomly\n","                        batch_size = batch_size,        # trains with this batch size\n","    )\n","\n","    # For validation the order doesn't matter, so we'll just read them sequentially\n","    validation_dataloader = DataLoader(\n","                        val_dataset,                # the validation samples\n","                        sampler = SequentialSampler(val_dataset), # Pull out batches sequentially\n","                        batch_size = batch_size     # evaluate with this batch size\n","    )\n","    return train_dataloader, validation_dataloader\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1675428695464,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"owT4X87kGYLf"},"outputs":[],"source":["def runner(training_ratio, epochs = 4,batch_size = 16, lr = 2e-5, seed_val = 42):\n","    # settings\n","    r = training_ratio\n","    # batch_size = 16 # CUDA out of memory if using 32\n","\n","\n","    # Combine the training inputs into a TensorDataset\n","    dataset = TensorDataset(input_ids, attention_masks, labels)\n","    # split data\n","    train_dataset, val_dataset = train_val_split(dataset, r)\n","    # format data\n","    train_dataloader, validation_dataloader = data_loader(train_dataset, \n","                                                        val_dataset,\n","                                                        batch_size)\n","\n","\n","\n","    # ======================================================\n","    # Main - hyperparameters\n","    # ======================================================\n","    # Create optimiser\n","    optimizer = AdamW(model.parameters(),\n","                    lr,  # args.learning rate - default is 5e-5\n","                    eps = 1e-8, # args.adam_epsilon - default is 1e-8\n","    )\n","\n","    # Number of training epochs. The BERT authors recommended between 2 and 4. \n","    # epochs = 4\n","\n","    # Total number of training steps is [number of batches] x [number of epochs]\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Create the learning rate scheduler. # 学习率预热\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps = 0, # Default value in run_glue.py\n","                                                num_training_steps = total_steps,\n","                                                )\n","\n","\n","\n","    # ======================================================\n","    # MAIN - Training loop\n","    # ======================================================\n","    # Set the seed value all over the place to make this reproducible.\n","    # seed_val = 42\n","\n","    random.seed(seed_val)\n","    np.random.seed(seed_val)\n","    torch.manual_seed(seed_val)\n","    torch.cuda.manual_seed_all(seed_val)\n","\n","    # We'll store a number of quantities such as training and validation loss, \n","    # validation accuracy, and timings.\n","    training_stats = []\n","\n","    # Measure the total training time for the whole run.\n","    total_t0 = time.time()\n","\n","\n","\n","    # For each epoch...\n","    for epoch_i in range(0, epochs):\n","        \n","        # ========================================\n","        #               Training\n","        # ========================================\n","        \n","        # Perform one full pass over the training set.\n","\n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        # Measure how long the training epoch takes.\n","        t0 = time.time()\n","\n","        # Reset the total loss for this epoch.\n","        total_train_loss = 0\n","\n","        # Put the model into training mode. \n","        model.train()\n","\n","\n","\n","        # For each batch of training data... \n","        for step, batch in enumerate(train_dataloader):\n","\n","            # Progress update every 40 batches.\n","            if step % 40 == 0 and not step == 0:\n","\n","                # Calculate elapsed time in minutes.\n","                elapsed = format_time(time.time() - t0)\n","                \n","                # Report progress.\n","                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","            # `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            # Always clear any previously calculated gradients before performing a\n","            # backward pass. \n","            model.zero_grad()        \n","\n","            # Perform a forward pass (evaluate the model on this training batch).\n","            # Specifically, we'll get the loss (because we provided labels) and the\n","            # \"logits\"-- the model outputs prior to activation.\n","            result = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels,\n","                        return_dict=True)\n","\n","            loss = result.loss\n","            logits = result.logits\n","\n","            # Accumulate the training loss over all of the batches so that we can\n","            # calculate the average loss at the end. \n","            total_train_loss += loss.item() # Tensor containing a single value\n","\n","            # Perform a backward pass to calculate the gradients.\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and take a step using the computed gradient.\n","            # The optimizer dictates the \"update rule\"--how the parameters are\n","            # modified based on their gradients, the learning rate, etc.\n","            optimizer.step()\n","\n","            # Update the learning rate\n","            scheduler.step()\n","\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_train_loss = total_train_loss / len(train_dataloader)            \n","        \n","        # Measure how long this epoch took.\n","        training_time = format_time(time.time() - t0)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","\n","\n","\n","\n","\n","        # ========================================\n","        #               Validation\n","        # ========================================\n","        # After the completion of each training epoch, measure our performance on\n","        # our validation set.\n","\n","        print(\"\")\n","        print(\"Running Validation...\")\n","\n","        t0 = time.time()\n","\n","        # Put the model in evaluation mode--the dropout layers behave differently\n","        # during evaluation.\n","        model.eval()\n","\n","        # Tracking variables \n","        total_eval_accuracy = 0\n","        total_eval_loss = 0\n","        nb_eval_steps = 0\n","\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            \n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using \n","            # the `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","            \n","            # Tell pytorch not to bother with constructing the compute graph during\n","            # the forward pass, since this is only needed for backprop (training).\n","            with torch.no_grad():        \n","\n","                # Forward pass, calculate logit predictions.\n","                # token_type_ids is the same as the \"segment ids\", which \n","                # differentiates sentence 1 and 2 in 2-sentence tasks.\n","                result = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask,\n","                            labels=b_labels,\n","                            return_dict=True)\n","\n","            # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","            # output values prior to applying an activation function like the \n","            # softmax.\n","            loss = result.loss\n","            logits = result.logits\n","                \n","            # Accumulate the validation loss.\n","            total_eval_loss += loss.item()\n","\n","            # Move logits and labels to CPU\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","\n","            # Calculate the accuracy for this batch of test sentences, and\n","            # accumulate it over all batches.\n","            total_eval_accuracy += flat_accuracy(logits, label_ids)\n","            \n","\n","        # Report the final accuracy for this validation run.\n","        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","        \n","        # Measure how long the validation run took.\n","        validation_time = format_time(time.time() - t0)\n","        \n","        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","        print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","\n","\n","        # ========================================\n","        #               Results\n","        # ========================================\n","        # Record all statistics from this epoch.\n","        training_stats.append(\n","            {\n","                'Run Number': 0,\n","                'Training ratio': r,\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss,\n","                'Valid. Loss': avg_val_loss,\n","                'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                'Validation Time': validation_time,\n","            }\n","        )\n","        df = to_dataframe(training_stats)\n","\n","        print(\"\")\n","        print(\"Training complete!\")\n","\n","        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","\n","    return df\n"]},{"cell_type":"code","execution_count":22,"metadata":{"executionInfo":{"elapsed":16,"status":"ok","timestamp":1675428695464,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"3WtGVDAhQIGp"},"outputs":[],"source":["# test\n","# df = runner(0.5)"]},{"cell_type":"code","execution_count":27,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JTd1IZlXHChA","outputId":"23f0f900-b009-44ff-df82-b1dbcbed4d3d","executionInfo":{"status":"ok","timestamp":1675428979745,"user_tz":0,"elapsed":28454,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["========= Running...  ratio: 0.8, run_num:0=============\n","  143 training samples\n","   36 validation samples\n","\n","======== Epoch 1 / 1 ========\n","Training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Average training loss: 1.30\n","  Training epcoh took: 0:00:13\n","\n","Running Validation...\n","  Accuracy: 0.42\n","  Validation Loss: 1.49\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:14 (h:mm:ss)\n","       Run Number  Training ratio  Training Loss  Valid. Loss  Valid. Accur.  \\\n","epoch                                                                          \n","1               0             0.8          1.301        1.486          0.417   \n","\n","      Training Time Validation Time  \n","epoch                                \n","1           0:00:13         0:00:01  \n","========= Running...  ratio: 0.9, run_num:0=============\n","  161 training samples\n","   18 validation samples\n","\n","======== Epoch 1 / 1 ========\n","Training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Average training loss: 1.15\n","  Training epcoh took: 0:00:14\n","\n","Running Validation...\n","  Accuracy: 0.25\n","  Validation Loss: 1.74\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:15 (h:mm:ss)\n","       Run Number  Training ratio  Training Loss  Valid. Loss  Valid. Accur.  \\\n","epoch                                                                          \n","1               0             0.9          1.148        1.738           0.25   \n","\n","      Training Time Validation Time  \n","epoch                                \n","1           0:00:14         0:00:01  \n"]}],"source":["# ****Classifier Parameters****\n","# set epochs\n","epochs = 1\n","# set batch size\n","batch_size = 16\n","# set random seed\n","seed_val = 42\n","# set learnning rate\n","lr = 10e-5   # args.learning rate - default is 5e-5\n","\n","\n","\n","# ****Training Parameters****\n","# set the number of loops \n","run_num = 1\n","# set training data ratio\n","ratios = [0.8,0.9]\n","for r in ratios:\n","\n","    # create a dataframe to saveto\n","    final_df = pd.DataFrame()\n","    \n","    for counter in range(run_num):\n","        print('========= Running...  ratio: '+str(r)+', run_num:'+str(counter)+'=============')\n","\n","        df = runner(r, epochs,batch_size, lr, seed_val)\n","        # count 'Run Number'\n","        df['Run Number'] = counter\n","        # append to the final dataframe\n","        final_df = pd.concat([df, final_df])\n","        \n","    # save dataframe\n","    print(final_df) \n","    filepath = './Pickledfiles/bert_argumentlevel/year1year2/'+str(r)+'trainratio_'+str(run_num)+'runs_'+str(epochs)+'epochs_'+str(batch_size)+'batch_size_'+str(lr)+'lr_'+str(seed_val)+'seed_val_.pkl'\n","    final_df.to_pickle(filepath)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NirHP_B-Wu8B"},"source":["# OLD CODE"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1675428751630,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"H1b87vXVDWZu"},"outputs":[],"source":["# ======================================================\n","# MAIN\n","# ======================================================\n","# Combine the training inputs into a TensorDataset\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","ratios = [0.5, 0.6, 0.7, 0.8, 0.9]\n","\n","\n","# loop training for each training-test ratios\n","for r in ratios:\n","\n","    batch_size = 16 # CUDA out of memory if using 32\n","    \n","    # split data\n","    train_dataset, val_dataset = train_val_split(dataset, r)\n","    # format data\n","    train_dataloader, validation_dataloader = data_loader(train_dataset, \n","                                                          val_dataset,\n","                                                          batch_size)\n","\n","\n","\n","    # ======================================================\n","    # Main - hyperparameters\n","    # ======================================================\n","    # Create optimiser\n","    optimizer = AdamW(model.parameters(),\n","                    lr = 2e-5,  # args.learning rate - default is 5e-5\n","                    eps = 1e-8, # args.adam_epsilon - default is 1e-8\n","    )\n","\n","    # Number of training epochs. The BERT authors recommended between 2 and 4. \n","    epochs = 2\n","  \n","    # Total number of training steps is [number of batches] x [number of epochs]\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Create the learning rate scheduler. # 学习率预热\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps = 0, # Default value in run_glue.py\n","                                                num_training_steps = total_steps,\n","                                                )\n","\n","\n","\n","    # ======================================================\n","    # MAIN - Training loop\n","    # ======================================================\n","    # Set the seed value all over the place to make this reproducible.\n","    seed_val = 42\n","\n","    random.seed(seed_val)\n","    np.random.seed(seed_val)\n","    torch.manual_seed(seed_val)\n","    torch.cuda.manual_seed_all(seed_val)\n","\n","    # We'll store a number of quantities such as training and validation loss, \n","    # validation accuracy, and timings.\n","    training_stats = []\n","\n","    # Measure the total training time for the whole run.\n","    total_t0 = time.time()\n","\n","\n","\n","    # For each epoch...\n","    for epoch_i in range(0, epochs):\n","        \n","        # ========================================\n","        #               Training\n","        # ========================================\n","        \n","        # Perform one full pass over the training set.\n","\n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        # Measure how long the training epoch takes.\n","        t0 = time.time()\n","\n","        # Reset the total loss for this epoch.\n","        total_train_loss = 0\n","\n","        # Put the model into training mode. \n","        model.train()\n","\n","\n","\n","        # For each batch of training data... \n","        for step, batch in enumerate(train_dataloader):\n","\n","            # Progress update every 40 batches.\n","            if step % 40 == 0 and not step == 0:\n","\n","                # Calculate elapsed time in minutes.\n","                elapsed = format_time(time.time() - t0)\n","                \n","                # Report progress.\n","                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","            # `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            # Always clear any previously calculated gradients before performing a\n","            # backward pass. \n","            model.zero_grad()        \n","\n","            # Perform a forward pass (evaluate the model on this training batch).\n","            # Specifically, we'll get the loss (because we provided labels) and the\n","            # \"logits\"-- the model outputs prior to activation.\n","            result = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels,\n","                        return_dict=True)\n","\n","            loss = result.loss\n","            logits = result.logits\n","\n","            # Accumulate the training loss over all of the batches so that we can\n","            # calculate the average loss at the end. \n","            total_train_loss += loss.item() # Tensor containing a single value\n","\n","            # Perform a backward pass to calculate the gradients.\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and take a step using the computed gradient.\n","            # The optimizer dictates the \"update rule\"--how the parameters are\n","            # modified based on their gradients, the learning rate, etc.\n","            optimizer.step()\n","\n","            # Update the learning rate\n","            scheduler.step()\n","\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_train_loss = total_train_loss / len(train_dataloader)            \n","        \n","        # Measure how long this epoch took.\n","        training_time = format_time(time.time() - t0)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","\n","\n","\n","\n","\n","        # ========================================\n","        #               Validation\n","        # ========================================\n","        # After the completion of each training epoch, measure our performance on\n","        # our validation set.\n","\n","        print(\"\")\n","        print(\"Running Validation...\")\n","\n","        t0 = time.time()\n","\n","        # Put the model in evaluation mode--the dropout layers behave differently\n","        # during evaluation.\n","        model.eval()\n","\n","        # Tracking variables \n","        total_eval_accuracy = 0\n","        total_eval_loss = 0\n","        nb_eval_steps = 0\n","\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            \n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using \n","            # the `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","            \n","            # Tell pytorch not to bother with constructing the compute graph during\n","            # the forward pass, since this is only needed for backprop (training).\n","            with torch.no_grad():        \n","\n","                # Forward pass, calculate logit predictions.\n","                # token_type_ids is the same as the \"segment ids\", which \n","                # differentiates sentence 1 and 2 in 2-sentence tasks.\n","                result = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask,\n","                            labels=b_labels,\n","                            return_dict=True)\n","\n","            # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","            # output values prior to applying an activation function like the \n","            # softmax.\n","            loss = result.loss\n","            logits = result.logits\n","                \n","            # Accumulate the validation loss.\n","            total_eval_loss += loss.item()\n","\n","            # Move logits and labels to CPU\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","\n","            # Calculate the accuracy for this batch of test sentences, and\n","            # accumulate it over all batches.\n","            total_eval_accuracy += flat_accuracy(logits, label_ids)\n","            \n","\n","        # Report the final accuracy for this validation run.\n","        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","        \n","        # Measure how long the validation run took.\n","        validation_time = format_time(time.time() - t0)\n","        \n","        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","        print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","\n","\n","\n","        # Record all statistics from this epoch.\n","        training_stats.append(\n","            {\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss,\n","                'Valid. Loss': avg_val_loss,\n","                'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                'Validation Time': validation_time\n","            }\n","        )\n","\n","        # save dataframe\n","        df = to_dataframe(training_stats)\n","        filepath = 'Pickledfiles/bert_reasoninglevel_trainratio'+str(r)+'_epochs'+str(epochs)+'_seedval42'+'_batchsize16'\n","        df.to_pickle(filepath)\n","\n","    print(\"\")\n","    print(\"Training complete!\")\n","\n","    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"markdown","metadata":{"id":"-arhBqEY4I-r"},"source":["\n","4.3 Training loop\n","\n","Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. \n","\n","> *Thank you to [Stas Bekman](https://ca.linkedin.com/in/stasbekman) for contributing the insights and code for using validation loss to detect over-fitting!*\n","\n","Training: \n","* Unpack our data inputs and labels \n","* Load data onto GPU for acceleration\n","* Clear out the gradients calculated in the previous pass\n","    * In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out\n","* Forward pass (feed input data through the network)\n","* Backward pass (backpropagation)\n","* Tell the network to update parameters with optimizer.step()\n","* Track variables for minitoring progress\n","\n","Evaluation:\n","* Unpack our data inputs and labels\n","* Load data onto the GPU for acceleration\n","* Forward pass (feed input data through the network)\n","* Compute loss on our validation data and track variables for monitoring process\n","\n","\n","Pytorch hides all the detailed calculations from us, but we've commented the code to print out which of the above steps are happening on each line.\n","\n","> *PyTorch also has some [beginner tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) which you may also find helpful.*"]},{"cell_type":"markdown","metadata":{"id":"-eJilZ9_1W7A"},"source":["Let's view the summary of the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":11,"status":"aborted","timestamp":1675428751631,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"ZJgACViioLT1"},"outputs":[],"source":["import pandas as pd\n","ratios = [0.5,0.6,0.7,0.8,0.9]\n","validacc_list = []\n","\n","for i in ratios:\n","    # path = 'Pickledfiles/bert_reasoninglevel_trainratio'+str(i)+'_epochs1_seedval42_batchsize16.pkl'\n","    df = pd.read_pickle('bert_reasoninglevel_trainratio0.5_epochs1_seedval42_batchsize16')\n","    validacc_list.append(df.iloc[0]['Valid. Accur.'])\n","    print(df)\n","\n","print(validacc_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1gSwd0WwLTf","executionInfo":{"status":"aborted","timestamp":1675428751631,"user_tz":0,"elapsed":11,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"}}},"outputs":[],"source":["pip install git+https://github.com/garrettj403/SciencePlots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WtqOdDP5vt4N","executionInfo":{"status":"aborted","timestamp":1675428751632,"user_tz":0,"elapsed":12,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"}}},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBwtofW32kdt","executionInfo":{"status":"aborted","timestamp":1675428751632,"user_tz":0,"elapsed":12,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"}}},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"YsdFtpZ94gPu"},"source":["5. Performance on Test Set"]},{"cell_type":"markdown","metadata":{"id":"rU_BBWgy4lK_"},"source":["5.1 Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PRXxjlsM5RZ2","executionInfo":{"status":"aborted","timestamp":1675428751632,"user_tz":0,"elapsed":11,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"}}},"outputs":[],"source":["# Create the DataLoader.\n","prediction_data = val_dataset\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = validation_dataloader"]},{"cell_type":"markdown","metadata":{"id":"DHFs1AkB434s"},"source":["5.2 Evaluate on Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KMdiBA9422k","executionInfo":{"status":"aborted","timestamp":1675428751633,"user_tz":0,"elapsed":12,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"}}},"outputs":[],"source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test reports...'.format(len(val_dataset)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions.\n","      result = model(b_input_ids, \n","                     token_type_ids=None, \n","                     attention_mask=b_input_mask,\n","                     return_dict=True)\n","\n","  logits = result.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"]},{"cell_type":"markdown","metadata":{"id":"SXlId81P6Kh2"},"source":["Accuracy on the CoLA benchmark is measured using the \"[Matthews correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)\" (MCC).\n","\n","We use MCC here because the classes are imbalanced:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJD1D-g86NUs","executionInfo":{"status":"aborted","timestamp":1675428751633,"user_tz":0,"elapsed":12,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"}}},"outputs":[],"source":["from sklearn.metrics import matthews_corrcoef\n","\n","matthews_set = []\n","\n","# Evaluate each test batch using Matthew's correlation coefficient\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  \n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  \n","  # Calculate and store the coef for this batch.  \n","  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n","  matthews_set.append(matthews)"]},{"cell_type":"markdown","metadata":{"id":"Kx7kqXn6D6VY"},"source":["The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches. \n","\n","Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"poxIZ-UiD-GZ","executionInfo":{"status":"aborted","timestamp":1675428751633,"user_tz":0,"elapsed":12,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"}}},"outputs":[],"source":["# Create a barplot showing the MCC score for each batch of test samples.\n","ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n","\n","plt.title('MCC Score per Batch')\n","plt.ylabel('MCC Score (-1 to +1)')\n","plt.xlabel('Batch #')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nyuwg22cEFWy","executionInfo":{"status":"aborted","timestamp":1675428751633,"user_tz":0,"elapsed":12,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"}}},"outputs":[],"source":["# Combine the results across all batches. \n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","# For each sample, pick the label (0 or 1) with the higher score.\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('Total MCC: %.3f' % mcc)"]},{"cell_type":"markdown","metadata":{"id":"dJOEigZaddtC"},"source":["When we actually convert all of our sentences, we'll use the `tokenize.encode` function to handle both steps, rather than calling `tokenize` and `convert_tokens_to_ids` separately. \n","\n","Before we can do that, though, we need to talk about some of BERT's formatting requirements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGYv9paXSDm9","executionInfo":{"status":"aborted","timestamp":1675428751634,"user_tz":0,"elapsed":12,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"}}},"outputs":[],"source":["# Print the original report.\n","print('length:',len(corpus[0]),';  Original: ', corpus[0])\n","\n","# Print the report split into tokens.\n","print('length:',len(tokenizer.tokenize(corpus[0])),';  Tokenized: ', tokenizer.tokenize(corpus[0]))\n","\n","# Print the report mapped to token ids.\n","print('length:',len(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(corpus[0]))) , ';  Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(corpus[0])))"]},{"cell_type":"markdown","metadata":{"id":"RgPW9ujVSFLv"},"source":["Let's apply the tokenizer to one report just to see the output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrik8ScLRlkd","executionInfo":{"status":"aborted","timestamp":1675428751634,"user_tz":0,"elapsed":12,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"}}},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["XwBJenjTQSO6","NirHP_B-Wu8B"],"provenance":[{"file_id":"1ebp9aVPlje_yA1gfGveLdjmBTCbyT-8d","timestamp":1670259501349}],"mount_file_id":"1zl0AkYFg34_0Z5JQHg4nSJD93v_ugfuZ","authorship_tag":"ABX9TyM3iUVn+mSAw90UL9eJUCOY"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.13 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13 (main, Aug 29 2022, 05:50:54) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"},"vscode":{"interpreter":{"hash":"397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"fcae84c2d2494fdaa57efc09b85f3aec":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_666a0629f575494998963fdaa6093412","IPY_MODEL_f9f8d4b7a21546f0b5049465de438437","IPY_MODEL_f66e05115bd644d28bde66d1e08a408d"],"layout":"IPY_MODEL_f9db04b02bfd4be1905760a6b1c713e7"}},"666a0629f575494998963fdaa6093412":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3e60b51db23e4b0aa92c265db91c23f7","placeholder":"​","style":"IPY_MODEL_31bbafeca8bd47fd9ce589f7f9dce6bf","value":"Downloading (…)solve/main/vocab.txt: 100%"}},"f9f8d4b7a21546f0b5049465de438437":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_4d31d8a86800457e88196abf3472b130","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_e71a56d6b7144642acc9405d098064d9","value":231508}},"f66e05115bd644d28bde66d1e08a408d":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b54bc88884814bca9c64d502e4fd82fa","placeholder":"​","style":"IPY_MODEL_625391bbce68426eb8806d1e8fa9c5b8","value":" 232k/232k [00:00&lt;00:00, 262kB/s]"}},"f9db04b02bfd4be1905760a6b1c713e7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3e60b51db23e4b0aa92c265db91c23f7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"31bbafeca8bd47fd9ce589f7f9dce6bf":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4d31d8a86800457e88196abf3472b130":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"e71a56d6b7144642acc9405d098064d9":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"b54bc88884814bca9c64d502e4fd82fa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"625391bbce68426eb8806d1e8fa9c5b8":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"acca920c63da4196989accd57fd63ae5":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_381d4fddb9a04df18a7e3a34ccdf8acb","IPY_MODEL_333e5fe43a7b4e69ad378a121725498e","IPY_MODEL_d26460f0c6754ff8bddc563ace1a1780"],"layout":"IPY_MODEL_1c54c8fb8e8a471093c245d50feb7d1a"}},"381d4fddb9a04df18a7e3a34ccdf8acb":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_7467ce530f0b4d39bcffaa353d1ff1aa","placeholder":"​","style":"IPY_MODEL_02a08735dc5e4f0aa162dbcbf13905b2","value":"Downloading (…)okenizer_config.json: 100%"}},"333e5fe43a7b4e69ad378a121725498e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_86fefc9033744178b4c2c08e35051d0d","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a145cb59a50944d4a6d0c6e4eaf667ec","value":28}},"d26460f0c6754ff8bddc563ace1a1780":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_df9d778edc494914bc952346800bc70c","placeholder":"​","style":"IPY_MODEL_69915fcb7a2f454ba8c7140a82f4a709","value":" 28.0/28.0 [00:00&lt;00:00, 876B/s]"}},"1c54c8fb8e8a471093c245d50feb7d1a":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7467ce530f0b4d39bcffaa353d1ff1aa":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"02a08735dc5e4f0aa162dbcbf13905b2":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"86fefc9033744178b4c2c08e35051d0d":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a145cb59a50944d4a6d0c6e4eaf667ec":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"df9d778edc494914bc952346800bc70c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"69915fcb7a2f454ba8c7140a82f4a709":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"260252cc42e1427eb94303749da2e77f":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_956cfb9eb0d94a8cabc83372722e9c02","IPY_MODEL_eb2b1161fa2e412bbfaf74023ef9acdc","IPY_MODEL_50149c4fa6eb44fc8b1a796f170b969e"],"layout":"IPY_MODEL_e730edf1e14648788717c3e4f3592a4c"}},"956cfb9eb0d94a8cabc83372722e9c02":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_13f9f2a1d74540348e5968d5bb3439a7","placeholder":"​","style":"IPY_MODEL_4b46b0cf58d345fcbcc8a84a2e636229","value":"Downloading (…)lve/main/config.json: 100%"}},"eb2b1161fa2e412bbfaf74023ef9acdc":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_d2a1d2a822aa48939b482bd56bc6be9b","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_905b81b97baf43c0adeb5545b32aaf59","value":570}},"50149c4fa6eb44fc8b1a796f170b969e":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_d601e237fc134b58a51cae1c891a4733","placeholder":"​","style":"IPY_MODEL_008569815bc04616857263b0c44a6720","value":" 570/570 [00:00&lt;00:00, 11.8kB/s]"}},"e730edf1e14648788717c3e4f3592a4c":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"13f9f2a1d74540348e5968d5bb3439a7":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4b46b0cf58d345fcbcc8a84a2e636229":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"d2a1d2a822aa48939b482bd56bc6be9b":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"905b81b97baf43c0adeb5545b32aaf59":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"d601e237fc134b58a51cae1c891a4733":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"008569815bc04616857263b0c44a6720":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cd5c08ca99f24f20956439f6eed6bd9a":{"model_module":"@jupyter-widgets/controls","model_name":"HBoxModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_c48138f3e1054aedbcd7ab46c2565b42","IPY_MODEL_b6233021bd4145a88739055c0d7dc13e","IPY_MODEL_5f25b0d563b24c3187118d2be6d8942f"],"layout":"IPY_MODEL_cb8c6efc4b2d4e29a478d8360eea88f0"}},"c48138f3e1054aedbcd7ab46c2565b42":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_4282e0d10660487281094f4b2f7a81a5","placeholder":"​","style":"IPY_MODEL_357292e7280f430989e12b02e1330e10","value":"Downloading (…)&quot;pytorch_model.bin&quot;;: 100%"}},"b6233021bd4145a88739055c0d7dc13e":{"model_module":"@jupyter-widgets/controls","model_name":"FloatProgressModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_685b83f12d8a4738812018903f14aa38","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_ca2d104f0c4b4f2984cec3c371f59a25","value":440473133}},"5f25b0d563b24c3187118d2be6d8942f":{"model_module":"@jupyter-widgets/controls","model_name":"HTMLModel","model_module_version":"1.5.0","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ee2a2723f95d4f9d83e58618d07b3d69","placeholder":"​","style":"IPY_MODEL_7c5f1a1095774bd3b39b44d5f07ced27","value":" 440M/440M [00:11&lt;00:00, 40.8MB/s]"}},"cb8c6efc4b2d4e29a478d8360eea88f0":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4282e0d10660487281094f4b2f7a81a5":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"357292e7280f430989e12b02e1330e10":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"685b83f12d8a4738812018903f14aa38":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"ca2d104f0c4b4f2984cec3c371f59a25":{"model_module":"@jupyter-widgets/controls","model_name":"ProgressStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"ee2a2723f95d4f9d83e58618d07b3d69":{"model_module":"@jupyter-widgets/base","model_name":"LayoutModel","model_module_version":"1.2.0","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7c5f1a1095774bd3b39b44d5f07ced27":{"model_module":"@jupyter-widgets/controls","model_name":"DescriptionStyleModel","model_module_version":"1.5.0","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}}}},"nbformat":4,"nbformat_minor":0}