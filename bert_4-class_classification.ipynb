{"cells":[{"cell_type":"markdown","metadata":{"id":"PIZC4JRqGhP5"},"source":["# 1. Setup"]},{"cell_type":"markdown","metadata":{"id":"LNeYfLE7mGul"},"source":["## 1.1 Using Colab GPU for Training\n"]},{"cell_type":"markdown","metadata":{"id":"N3upHHQ1m6Zb"},"source":["Run the following the cell to confirm the GPU is detected."]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":4249,"status":"ok","timestamp":1673305116124,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"0eZDJWhjmOcC","outputId":"496e2ad3-2706-4725-ae2f-ee2dc24b9e7b"},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'tensorflow'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn[3], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtensorflow\u001b[39;00m \u001b[39mas\u001b[39;00m \u001b[39mtf\u001b[39;00m\n\u001b[1;32m      3\u001b[0m \u001b[39m# Get the GPU device name.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m device_name \u001b[39m=\u001b[39m tf\u001b[39m.\u001b[39mtest\u001b[39m.\u001b[39mgpu_device_name()\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"]}],"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"]},{"cell_type":"markdown","metadata":{"id":"_bgovg_6hNCU"},"source":["In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in out training loop"]},{"cell_type":"code","execution_count":2,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6776,"status":"ok","timestamp":1673305122893,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"3n7WnLmpg_Hj","outputId":"f2e268d3-7072-45c3-f98e-20caa3a04f6d"},"outputs":[{"name":"stdout","output_type":"stream","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}],"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"BH8PQKEhF0YD"},"source":["## 1.2 Installing Hugging Face Library"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":16824,"status":"ok","timestamp":1673305139693,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"n6DvZooRmKrn","outputId":"ac3f41c3-f51f-4722-f7f7-5cd2a3059980"},"outputs":[{"name":"stdout","output_type":"stream","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.8.2)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.10.0\n","  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m22.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m78.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n","Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"]}],"source":["pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"TxWOmaB9GT4q"},"source":["# 2. Retrieve Dataset"]},{"cell_type":"markdown","metadata":{"id":"L5UDujTsGY00"},"source":["## 2.1 Mount Google Drive"]},{"cell_type":"code","execution_count":4,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":58351,"status":"ok","timestamp":1673305198038,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"4atrLvNzb0b3","outputId":"0b3092af-938f-40f0-944b-37a6203fdc0b"},"outputs":[{"name":"stdout","output_type":"stream","text":["Mounted at /content/gdrive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":5,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1673305198039,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"CtXpJC70HM-H","outputId":"40ca6863-98fd-466c-8720-e98bde65dc45"},"outputs":[{"name":"stdout","output_type":"stream","text":["/root\n","/root\n","/content/gdrive/MyDrive/nlp-physicseducation\n"]}],"source":["%cd\n","!pwd\n","%cd /content/gdrive/MyDrive/nlp-physicseducation/"]},{"cell_type":"markdown","metadata":{"id":"5pHTEYcyG-D9"},"source":["## 2.2 Parse Data"]},{"cell_type":"markdown","metadata":{"id":"Ln_QvsjDuynn"},"source":["Specify the directories"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1673305198039,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"s4EMjNyhu3bW"},"outputs":[],"source":["dir_csv = 'outputs/sections/labels_cleaned_y1c1c2.csv'"]},{"cell_type":"code","execution_count":7,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":398},"executionInfo":{"elapsed":3944,"status":"ok","timestamp":1673305201978,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"4-gB9Op4Fts5","outputId":"109af3d8-8e93-4758-cd24-78579a9e95c8"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of reports: 96\n","\n"]},{"data":{"text/html":["\n","  <div id=\"df-a1545ed5-ee46-4ced-923b-ce589b2a39c4\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>StudentID</th>\n","      <th>Content</th>\n","      <th>ArgumentLevel</th>\n","      <th>ReasoningLevel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>39</th>\n","      <td>GS_DAR896_Redacted</td>\n","      <td>the main source of random error which be intro...</td>\n","      <td>superficial</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>71</th>\n","      <td>GS_GQV977_Redacted</td>\n","      <td>the first calculation do be to find the value ...</td>\n","      <td>deep</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>7</th>\n","      <td>GS_TGO908_Redacted</td>\n","      <td>after obtain value for ùúÉ for different order n...</td>\n","      <td>superficial</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>GS_OQP549_Redacted</td>\n","      <td>in the experiment conduct we calculate with an...</td>\n","      <td>superficial</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>34</th>\n","      <td>GS_MJV182_Redacted</td>\n","      <td>this investigation involve the determination o...</td>\n","      <td>extended</td>\n","      <td>exp</td>\n","    </tr>\n","    <tr>\n","      <th>38</th>\n","      <td>GS_GOO868_Redacted</td>\n","      <td>the data that we collect for each colour can b...</td>\n","      <td>superficial</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>54</th>\n","      <td>GS_DAG780_Redacted</td>\n","      <td>the reference angle at which the diffraction a...</td>\n","      <td>deep</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>52</th>\n","      <td>GS_SKD544_Redacted</td>\n","      <td>the emission spectmm from the hydrogen dischar...</td>\n","      <td>extended</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>GS_TAP080_Redacted</td>\n","      <td>a see in figure 6 the relationship between ord...</td>\n","      <td>prediction</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>43</th>\n","      <td>GS_HSB934_Redacted</td>\n","      <td>he rydberg constant rÔÇ• relates the transition ...</td>\n","      <td>superficial</td>\n","      <td>bal</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-a1545ed5-ee46-4ced-923b-ce589b2a39c4')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-a1545ed5-ee46-4ced-923b-ce589b2a39c4 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-a1545ed5-ee46-4ced-923b-ce589b2a39c4');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["             StudentID                                            Content  \\\n","39  GS_DAR896_Redacted  the main source of random error which be intro...   \n","71  GS_GQV977_Redacted  the first calculation do be to find the value ...   \n","7   GS_TGO908_Redacted  after obtain value for ùúÉ for different order n...   \n","60  GS_OQP549_Redacted  in the experiment conduct we calculate with an...   \n","34  GS_MJV182_Redacted  this investigation involve the determination o...   \n","38  GS_GOO868_Redacted  the data that we collect for each colour can b...   \n","54  GS_DAG780_Redacted  the reference angle at which the diffraction a...   \n","52  GS_SKD544_Redacted  the emission spectmm from the hydrogen dischar...   \n","22  GS_TAP080_Redacted  a see in figure 6 the relationship between ord...   \n","43  GS_HSB934_Redacted  he rydberg constant rÔÇ• relates the transition ...   \n","\n","   ArgumentLevel ReasoningLevel  \n","39   superficial            the  \n","71          deep            the  \n","7    superficial            bal  \n","60   superficial            bal  \n","34      extended            exp  \n","38   superficial            the  \n","54          deep            bal  \n","52      extended            bal  \n","22    prediction            bal  \n","43   superficial            bal  "]},"execution_count":7,"metadata":{},"output_type":"execute_result"}],"source":["import pandas as pd\n","\n","# load the dataset into a pandas dataframe\n","df = pd.read_csv(\n","        dir_csv, \n","        encoding='utf-8', \n","        skiprows = 1, \n","        names=['StudentID', 'Content', 'ArgumentLevel', 'ReasoningLevel']\n",")\n","\n","# Report the number of reports\n","print('Number of reports: {:,}\\n'.format(df.shape[0]))\n","\n","# Display 10 random rows from the data\n","df.sample(10)"]},{"cell_type":"markdown","metadata":{"id":"ZfT4uprKJsRG"},"source":["The label 'ArgumentLevel' and 'ReasoningLevel' are mapped to numbers.\n","\n","Argument Level labels {'bal': 0, 'the': 1, 'exp': 2, 'none': 3}\n","\n","Reasoning Level labels {'extended': 0, 'deep': 1, 'expert': 2, 'superficial': 3, 'prediction': 4}"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":160,"status":"ok","timestamp":1673305201983,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"3upygP5AH0NL","outputId":"2b6d8ea0-d2a0-40fc-e964-924644425980"},"outputs":[{"data":{"text/html":["\n","  <div id=\"df-c2e5b8cb-a49d-4056-a561-1dfbf5d6a444\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>StudentID</th>\n","      <th>Content</th>\n","      <th>ArgumentLevel</th>\n","      <th>ReasoningLevel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>54</th>\n","      <td>GS_DAG780_Redacted</td>\n","      <td>the reference angle at which the diffraction a...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>91</th>\n","      <td>GS_LSK572_Redacted</td>\n","      <td>the rydberg constant be calculate by use a gra...</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>45</th>\n","      <td>GS_HBQ145_Redacted</td>\n","      <td>spectral line have some thickness and be not i...</td>\n","      <td>0</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>30</th>\n","      <td>GS_YUK005_Redacted</td>\n","      <td>‚àÜùúÉ ùëñùëõ ùëëùëíùëîùëüùëíùëí</td>\n","      <td>1</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>3</th>\n","      <td>GS_HHM124_Redacted</td>\n","      <td>in this experiment a grate spectrometer be use...</td>\n","      <td>2</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>GS_VJL711_Redacted</td>\n","      <td>where z be the atom 's atomic number ùúÄ 0 be th...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>GS_PAO564_Redacted</td>\n","      <td>the angle that be collect in the measurement f...</td>\n","      <td>3</td>\n","      <td>1</td>\n","    </tr>\n","    <tr>\n","      <th>36</th>\n","      <td>GS_FVA673_Redacted</td>\n","      <td>upon my lab partner and i take two individual ...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>66</th>\n","      <td>GS_QFQ712_Redacted</td>\n","      <td>the measurement be take by two person individu...</td>\n","      <td>0</td>\n","      <td>0</td>\n","    </tr>\n","    <tr>\n","      <th>78</th>\n","      <td>GS_TFY667_Redacted</td>\n","      <td>this project investigates the measurement of w...</td>\n","      <td>1</td>\n","      <td>2</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-c2e5b8cb-a49d-4056-a561-1dfbf5d6a444')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-c2e5b8cb-a49d-4056-a561-1dfbf5d6a444 button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-c2e5b8cb-a49d-4056-a561-1dfbf5d6a444');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "],"text/plain":["             StudentID                                            Content  \\\n","54  GS_DAG780_Redacted  the reference angle at which the diffraction a...   \n","91  GS_LSK572_Redacted  the rydberg constant be calculate by use a gra...   \n","45  GS_HBQ145_Redacted  spectral line have some thickness and be not i...   \n","30  GS_YUK005_Redacted                                       ‚àÜùúÉ ùëñùëõ ùëëùëíùëîùëüùëíùëí   \n","3   GS_HHM124_Redacted  in this experiment a grate spectrometer be use...   \n","0   GS_VJL711_Redacted  where z be the atom 's atomic number ùúÄ 0 be th...   \n","20  GS_PAO564_Redacted  the angle that be collect in the measurement f...   \n","36  GS_FVA673_Redacted  upon my lab partner and i take two individual ...   \n","66  GS_QFQ712_Redacted  the measurement be take by two person individu...   \n","78  GS_TFY667_Redacted  this project investigates the measurement of w...   \n","\n","    ArgumentLevel  ReasoningLevel  \n","54              1               0  \n","91              1               0  \n","45              0               1  \n","30              1               0  \n","3               2               0  \n","0               0               0  \n","20              3               1  \n","36              0               0  \n","66              0               0  \n","78              1               2  "]},"execution_count":8,"metadata":{},"output_type":"execute_result"}],"source":["# define dict to code labels to numbers\n","ReasoningLevel_dict = {'bal': 0, 'the': 1, 'exp': 2, 'none': 3}\n","ArgumentLevel_dict = {'extended': 0, 'deep': 1, 'expert': 2, 'superficial': 3, 'prediction': 4}\n","\n","# replace to number labels\n","df['ReasoningLevel'].replace(list(ReasoningLevel_dict.keys()), list(ReasoningLevel_dict.values()),inplace=True) \n","df['ArgumentLevel'].replace(list(ArgumentLevel_dict.keys()), list(ArgumentLevel_dict.values()),inplace=True) \n","\n","\n","# Display 10 random rows from the data\n","df.sample(10)"]},{"cell_type":"markdown","metadata":{"id":"OmzBpfb-KMe1"},"source":["Let's extract the sentences and labels of our training set as numpy ndarrys."]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":157,"status":"ok","timestamp":1673305201984,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"e9U4SpCcQRJ8","outputId":"e20e4ed7-f3dd-4125-a328-63486e22d456"},"outputs":[{"name":"stdout","output_type":"stream","text":["Number of reports: 96\n","\n","First report:     where z be the atom 's atomic number ùúÄ 0 be the vacuum permittivity me be the electron 's mass and e be the charge of the electron if we divide eq 2.2 by eq 2 2.6 where v and t be potential the kinetic energy of electron respectively 5 since the energy of a photon emit by a hydrogen atom be give by the difference of two hydrogen energy level where n and m be positive integer and n be great than m. from the relationship between wavelength and frequency of a photon ùúà ùëê ùúÜ 2.9 where c be the speed of light eq 6 can be rewrite a for hydrogen atom the atomic number z be 1. therefore the rydberg constant be define a in this experiment we experimentally measure the rydberg constant if hydrogen gas be heat or electrically charge the electron in the atom be excite and the add energy push the electron to high energy orbitals when the electron fall back to their original position the add energy be re-emitted in the form of photon and the wavelength of the photon be determine by the differnece in energy between the orbitals a show in where m can be call the order number we set up the spectrometer a show in fig 3.1 use a collimator a grate and a camera with a crosshair and ensure that they be in parallel the beam of photon from the light source be diffract by the grate so we set the angular position of the camera when they be parallel a an origin and rotate it until a light fringe be observe then we measure the angle of diffraction by measure the change in angle from the origin use a vernier we repeat this with a many other fringe a possible and statisticised the result to find the rydberg constant although the rydberg constant obtain from the experiment be reasonably close to the theoretical value the uncertainty of the constant be unacceptably huge and we found that there be various reason that affected our value since the room be dark and it be hard to read the cramped number on the veriner the angle of diffraction could be measure imprecisely we conclude that this be the major factor that affected the inaccurate rydberg constant we found that the major uncertainty emerge in consequence of the wide slit size since this cause the thicker width of the light fringe this could be improve by use a narrow slit there could be various systematic error that affected the result we try to perfectly align the spectrometer but this could be unsuccessful also the position of the lamp should have be a close to the slit a possible since it be directly related to the intensity of the light this could make u see the cyan colour light the air pressure and humidity be also other source but we decide that they be negligible there be background light which be inevitably require to measure the angle of diffraction we thought this could be the reason why the cyan colour of light and the 3 rd order light fringe of red and violet light be not detect and test without any light but we still could not see the line in the experiment we try to measure the rydberg constant use a hydrogen gas tube and a spectrometer and we get ùëÖ ‚àû 10.5 √ó 10 6 ¬± 4.4 √ó 10 6 ùëö ‚àí1 which be only 0.4 unit less than the theoretical value however we fail to obtain the precise value a our uncertainty be over 40 percent of our value and we conclude that the width of the slit be not narrow enough to produce the precise result this could be improve by repeat the experiment in a darker room with a narrower slit width and a clearer vernier a for reference this be the code that have be use for data analysis\n","Second report:    the data that we gather from the method be summarise in table 1 and table 2 we observe the light from the licl to be red and the light from the cuso 4 to be green. this be all do in python\n"]}],"source":["corpus = df.Content.values\n","ArgumentLevels = df.ArgumentLevel.values\n","ReasoningLevels = df.ReasoningLevel.values\n","\n","print('Number of reports: {:,}\\n'.format(len(corpus)))\n","print('First report:    ',corpus[0])\n","print('Second report:   ',corpus[1])"]},{"cell_type":"markdown","metadata":{"id":"PmOSw5EXRmPf"},"source":["# 3. Tokenization & Input Formatting\n","\n","In this section, we'll transform our dataset into the format that BERT can be trained on."]},{"cell_type":"markdown","metadata":{"id":"kxDtbEM0H186"},"source":["## 3.1 Tokenisation"]},{"cell_type":"code","execution_count":10,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":130,"referenced_widgets":["86e0f7a7218e45e29999376741ce4985","7584aaec02f84f8c8cedacb7f6f17e1b","68dd44ee33514a07be821a31c1d47291","f88e3510a5074c499607b7b7256ed6c4","fa712872f41d4d559e89bab854dab599","2a4a4a7896bf411dab3cb996b0badac4","c0c930eff5164a65b0288178132a2128","5ce05275cd194aca95e572e138ff23f1","b7103b7f388f4548b2bdc37b94a7d5e4","97fd3934e71f413d918907186c52b8e5","7faac40a063a425fac5e5a2496f11eef","09408edd50494d75b70f7df286e6a23e","e088ece32f5a41aab2c358a26cf19a05","4622a48bc06b4b2bacb5c7b462d0ce1e","eaf4f2f39c8748e48da576352ea8d348","9b5092461a794cdc92a4ceb41093b64b","f2033804ed5346ae8f0a0bf55f811d8c","5b62ab20a9d846e8a8ace0a938b98711","261788bd52f44d8e81934bbd1a9b1b60","63eb06bf8ea2417485c91350f8f8ae9d","3edac39cfc2249f3841043f2f47df672","1135443edbc8488eb0f61b02f0d67f32","4a4c419f4bbc4225b3a7c2a09ee061da","3c9d03d962a44453b06f90194569a879","5ea18a3abb104f7984521fc6fc1dbe5a","54d58499ac044d3ab19603639bf8a18d","793a754372614d46bc7814f16d3a6b4b","ef02cf1d3c444e949f1dfb95f9e855e8","f1da04fac7614f7884861f7328518c4e","8601b810b324448ba0fd60eb57cc6b8d","76699a9d22fa4e2e8fd8d0801fa3c6a2","6b742789ed19406d83683b09af7b2330","91efdfc8128f44c699a3d560429d011d"]},"executionInfo":{"elapsed":7854,"status":"ok","timestamp":1673305209685,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"gx5dgBinVS2N","outputId":"ab767bf0-4ed2-4bc6-92a3-230021cee1a1"},"outputs":[{"name":"stdout","output_type":"stream","text":["loading BERT tokenizer...\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"86e0f7a7218e45e29999376741ce4985","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"09408edd50494d75b70f7df286e6a23e","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"4a4c419f4bbc4225b3a7c2a09ee061da","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"}],"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer\n","print('loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"markdown","metadata":{"id":"nfxyOJYug5F1"},"source":["Now we're ready to perform the real tokenization.\n","\n","The `tokenizer.encode_plus` function combines multiple steps for us:\n","\n","1. Split the sentence into tokens.\n","2. Add the special `[CLS]` and `[SEP]` tokens.\n","3. Map the tokens to their IDs.\n","4. Pad or truncate all sentences to the same length.\n","5. Create the attention masks which explicitly differentiate real tokens from `[PAD]` tokens.\n","\n","The first four features are in `tokenizer.encode`, but I'm using `tokenizer.encode_plus` to get the fifth item (attention masks). Documentation is [here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus).\n"]},{"cell_type":"code","execution_count":11,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":3035,"status":"ok","timestamp":1673305212713,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"4M7BBNLMg4op","outputId":"53f8b805-72e0-437f-f09a-135fd5ebced1"},"outputs":[{"name":"stderr","output_type":"stream","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["# Tokenize all of the reports and map the tokens to their word IDs.\n","input_ids, attention_masks, lengths = [], [], []\n","\n","# For every report ...\n","for report in corpus:\n","    # 'encode_plus' will:\n","    #   (1) Tokenise the sentence.\n","    #   (2) Prepend the '[CLS]' token to the start\n","    #   (3) Append the '[SEP]' token to the end\n","    #   (4) Map tokens to their IDs\n","    #   (5) Pad or truncate the report to 'max_length'\n","    #   (6) Create attention masks for [PAD] tokens\n","    encoded_dict = tokenizer.encode_plus(\n","                        report,                     # report to encode\n","                        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n","                        max_length = 512,            # Pad & truncate all reports\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks\n","                        return_tensors = 'pt',          # return pytorch tensors\n","\n","    )\n","\n","    # Add the encoded report to the list \n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    # And its attention mask (simply differentiates padding from non-padding)\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # lengths.append(len(encoded_dict['input_ids']))\n","    lengths.append(len(encoded_dict['input_ids'][0]))\n"]},{"cell_type":"markdown","metadata":{"id":"VRmQt6adGKPW"},"source":["Set which label type to train with; and convert input lists of tensors."]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1673305212721,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"JxSdH_cxGN98"},"outputs":[],"source":["# change input labels here ***\n","labels = ReasoningLevels   \n","num_labels = 4    #4 for ReasoningLevels #5 for ArgumentLevels\n","\n","\n","# Convert the lists into tensors\n","input_ids = torch.cat(input_ids, dim = 0)\n","attention_masks = torch.cat(attention_masks, dim = 0)\n","labels = torch.tensor(labels)"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":17,"status":"ok","timestamp":1673305212722,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"fBNaFKj8Hm4J"},"outputs":[],"source":["# Print report 0, now as a list of IDs\n","# print('Original:', corpus[0])\n","# print('Token IDs:', input_ids[0])"]},{"cell_type":"markdown","metadata":{"id":"XwBJenjTQSO6"},"source":["## 3.2 Report Length distribution (Discarded)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":18,"status":"ok","timestamp":1673305212724,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"TaTbhoegXmiF","outputId":"758e786e-885c-4d66-84a2-f515d8d92525"},"outputs":[{"name":"stdout","output_type":"stream","text":["Min length: 512 tokens\n","Max length: 512 tokens\n","Median length: 512.0 tokens\n"]}],"source":["import numpy as np\n","print('Min length: {:,} tokens'.format(min(lengths)))\n","print('Max length: {:,} tokens'.format(max(lengths)))\n","print('Median length: {:,} tokens'.format(np.median(lengths)))\n"]},{"cell_type":"code","execution_count":15,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":0},"executionInfo":{"elapsed":2506,"status":"ok","timestamp":1673305215215,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"2EXPYdmOQR7l","outputId":"aee5e636-bbcc-44ca-f585-6b2c34b73dd4"},"outputs":[{"data":{"text/plain":["Text(10.314999999999998, 0.5, '# of Reports')"]},"execution_count":15,"metadata":{},"output_type":"execute_result"},{"data":{"image/png":"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","text/plain":["<Figure size 360x360 with 1 Axes>"]},"metadata":{},"output_type":"display_data"}],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np \n","\n","sns.set(style = 'darkgrid')\n","\n","# Increase the plot size and font size\n","sns.set(font_scale = 1.5)\n","plt.rcParams[\"figure.figsize\"] = (10,5)\n","\n","# Truncate any report lengths greater than 512\n","lengths = [min(l,512) for l in lengths]\n","\n","# Plot the distribution of comment lengths\n","sns.displot(lengths, kde=False, rug=False)\n","\n","plt.title(\"Report Lengths\")\n","plt.xlabel(\"Report Length\")\n","plt.ylabel(\"# of Reports\")\n"]},{"cell_type":"markdown","metadata":{"id":"IyD2pukar1QO"},"source":["How many data (reports) could be faulty? "]},{"cell_type":"code","execution_count":16,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":15,"status":"ok","timestamp":1673305215216,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"3yLVlc7Er8cI","outputId":"79c46ce2-deb4-4a0f-b602-eb0eee349e8f"},"outputs":[{"name":"stdout","output_type":"stream","text":["# of possible faulty reports:  0\n"]}],"source":["# Count the number of suspicious reports that didn't not have full text extracted \n","counter = 0\n","for l in lengths:\n","    if l<20:\n","        counter+=1\n","print('# of possible faulty reports: ', counter)\n"]},{"cell_type":"markdown","metadata":{"id":"6haaD5vpf-Qy"},"source":["How many reports run into the 512-token limit?"]},{"cell_type":"code","execution_count":17,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":12,"status":"ok","timestamp":1673305215217,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"SKHvKyv6gExI","outputId":"aa226add-2789-4b48-da49-a5e8fbf75cdb"},"outputs":[{"name":"stdout","output_type":"stream","text":["96 of 96 sentences (100.0%) of corpus are longer than 512 tokens\n"]}],"source":["# Count the number of sentences that had to be truncated to 512 tokens\n","num_truncated = lengths.count(512)\n","\n","# Compare this to the total number of training reports\n","num_reports = len(lengths)\n","prcnt = float(num_truncated)/float(num_reports)\n","\n","print('{:,} of {:,} sentences ({:.1%}) of corpus are longer than 512 tokens'.format(\n","        num_truncated, num_reports, prcnt))\n"]},{"cell_type":"markdown","metadata":{"id":"ytev8umirwoF"},"source":["## 3.3 Training and Validation Split\n"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000,"referenced_widgets":["0a1c18fe9cad4c3e94e89925623768ac","0273e18cd0fc4067941bd14fd4912672","6dad9135e1c74d9fb7ba0e7632f6bb4a","200d806eead24cc8859fe2d46306d414","cc2230712c0641b09bc7cd1d21a75ad8","f43834dd2a9041b9a1682b97cb7b53ba","42594abd60ca4cd99903db92603d433d","69a7706ce17d4cf1817daaeecec01e91","cc20592b4dbf45e99743f2831a333e79","b137db95ace74a2591f64e7f2eb36975","cce7f11c38a34d75ab46e3a128cc0205"]},"executionInfo":{"elapsed":16826,"status":"ok","timestamp":1673305232033,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"TOUbbujd7coW","outputId":"28c31099-998f-4d7e-ef94-1d01f0a325af"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a1c18fe9cad4c3e94e89925623768ac","version_major":2,"version_minor":0},"text/plain":["Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n","- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n","- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n","Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n","You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"]},{"data":{"text/plain":["BertForSequenceClassification(\n","  (bert): BertModel(\n","    (embeddings): BertEmbeddings(\n","      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n","      (position_embeddings): Embedding(512, 768)\n","      (token_type_embeddings): Embedding(2, 768)\n","      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","      (dropout): Dropout(p=0.1, inplace=False)\n","    )\n","    (encoder): BertEncoder(\n","      (layer): ModuleList(\n","        (0): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (1): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (2): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (3): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (4): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (5): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (6): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (7): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (8): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (9): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (10): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","        (11): BertLayer(\n","          (attention): BertAttention(\n","            (self): BertSelfAttention(\n","              (query): Linear(in_features=768, out_features=768, bias=True)\n","              (key): Linear(in_features=768, out_features=768, bias=True)\n","              (value): Linear(in_features=768, out_features=768, bias=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","            (output): BertSelfOutput(\n","              (dense): Linear(in_features=768, out_features=768, bias=True)\n","              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","              (dropout): Dropout(p=0.1, inplace=False)\n","            )\n","          )\n","          (intermediate): BertIntermediate(\n","            (dense): Linear(in_features=768, out_features=3072, bias=True)\n","            (intermediate_act_fn): GELUActivation()\n","          )\n","          (output): BertOutput(\n","            (dense): Linear(in_features=3072, out_features=768, bias=True)\n","            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n","            (dropout): Dropout(p=0.1, inplace=False)\n","          )\n","        )\n","      )\n","    )\n","    (pooler): BertPooler(\n","      (dense): Linear(in_features=768, out_features=768, bias=True)\n","      (activation): Tanh()\n","    )\n","  )\n","  (dropout): Dropout(p=0.1, inplace=False)\n","  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",")"]},"execution_count":18,"metadata":{},"output_type":"execute_result"}],"source":["# Load BertForSequenceClassification, the pre-trained BERT model with a \n","# single linear classification layer on top.\n","\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","model = BertForSequenceClassification.from_pretrained(\n","        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab\n","        num_labels = num_labels, # the number of ourput labels -- 5 for five ArgumentLevel classification labels\n","        output_attentions = False, # whether the model returns attention weights\n","        output_hidden_states = False, # whether the model returns all hidden-states\n",")\n","\n","# Tell pytorch to run this model on the GPU\n","model.cuda()\n","\n"]},{"cell_type":"code","execution_count":19,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1673305232034,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"ewM603LvDo02"},"outputs":[],"source":["# Create helper function\n","import numpy as np\n","import random\n","import time\n","import datetime\n","\n","\n","def flat_accuracy(preds, labels):\n","    '''\n","    Function to calculate the accuracy of our predictions vs labels\n","    '''\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def format_time(elapsed):\n","    '''\n","    Taks a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second\n","    elapsed_rounded = int(round(elapsed))\n","\n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds = elapsed_rounded))\n","\n","def to_dataframe(dict):\n","    # Display floats with two decimal places.\n","    pd.set_option('precision', 3)\n","    df_stats = pd.DataFrame(data=dict)\n","    df_stats = df_stats.set_index('epoch')\n","    # A hack to force the column headers to wrap.\n","    # df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","    return df_stats"]},{"cell_type":"code","execution_count":20,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1673305232035,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"1IYjV_u8AqVr"},"outputs":[],"source":["import numpy as np\n","import random\n","from torch.utils.data import TensorDataset, random_split\n","from tensorflow.python.ops.variables import validate_synchronization_aggregation_trainable\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import get_linear_schedule_with_warmup\n","\n","\n","# define a function for train-validation data splitting\n","def train_val_split(dataset, ratio):\n","    '''\n","    # Create a ratio:(1-ratio) train-validation split\n","    \n","    dataset: tensor object\n","    ratio: float <1 and >0\n","    '''\n","    # Calculate the number of samples to include in each set\n","    train_size = int(ratio * len(dataset))\n","    val_size = len(dataset) - train_size\n","\n","    # Divide the dataset by randomly selecting samples\n","    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","    print('{:>5,} training samples'.format(train_size))\n","    print('{:>5,} validation samples'.format(val_size))\n","    \n","    return train_dataset, val_dataset\n","\n","\n","\n","# Create the DataLoaders for our training and validation sets.\n","def data_loader(train_dataset, val_dataset,\n","                batch_size):\n","\n","    # We'll take training samples in random order\n","    train_dataloader = DataLoader(\n","                        train_dataset,                  # the trainig samples\n","                        sampler = RandomSampler(train_dataset), # select batches randomly\n","                        batch_size = batch_size,        # trains with this batch size\n","    )\n","\n","    # For validation the order doesn't matter, so we'll just read them sequentially\n","    validation_dataloader = DataLoader(\n","                        val_dataset,                # the validation samples\n","                        sampler = SequentialSampler(val_dataset), # Pull out batches sequentially\n","                        batch_size = batch_size     # evaluate with this batch size\n","    )\n","    return train_dataloader, validation_dataloader\n","\n"]},{"cell_type":"code","execution_count":21,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1673305232035,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"owT4X87kGYLf"},"outputs":[],"source":["def runner(training_ratio):\n","    # settings\n","    r = training_ratio\n","    batch_size = 16 # CUDA out of memory if using 32\n","\n","\n","    # Combine the training inputs into a TensorDataset\n","    dataset = TensorDataset(input_ids, attention_masks, labels)\n","    # split data\n","    train_dataset, val_dataset = train_val_split(dataset, r)\n","    # format data\n","    train_dataloader, validation_dataloader = data_loader(train_dataset, \n","                                                        val_dataset,\n","                                                        batch_size)\n","\n","\n","\n","    # ======================================================\n","    # Main - hyperparameters\n","    # ======================================================\n","    # Create optimiser\n","    optimizer = AdamW(model.parameters(),\n","                    lr = 2e-5,  # args.learning rate - default is 5e-5\n","                    eps = 1e-8, # args.adam_epsilon - default is 1e-8\n","    )\n","\n","    # Number of training epochs. The BERT authors recommended between 2 and 4. \n","    epochs = 4\n","\n","    # Total number of training steps is [number of batches] x [number of epochs]\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Create the learning rate scheduler. # Â≠¶‰π†ÁéáÈ¢ÑÁÉ≠\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps = 0, # Default value in run_glue.py\n","                                                num_training_steps = total_steps,\n","                                                )\n","\n","\n","\n","    # ======================================================\n","    # MAIN - Training loop\n","    # ======================================================\n","    # Set the seed value all over the place to make this reproducible.\n","    seed_val = 42\n","\n","    random.seed(seed_val)\n","    np.random.seed(seed_val)\n","    torch.manual_seed(seed_val)\n","    torch.cuda.manual_seed_all(seed_val)\n","\n","    # We'll store a number of quantities such as training and validation loss, \n","    # validation accuracy, and timings.\n","    training_stats = []\n","\n","    # Measure the total training time for the whole run.\n","    total_t0 = time.time()\n","\n","\n","\n","    # For each epoch...\n","    for epoch_i in range(0, epochs):\n","        \n","        # ========================================\n","        #               Training\n","        # ========================================\n","        \n","        # Perform one full pass over the training set.\n","\n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        # Measure how long the training epoch takes.\n","        t0 = time.time()\n","\n","        # Reset the total loss for this epoch.\n","        total_train_loss = 0\n","\n","        # Put the model into training mode. \n","        model.train()\n","\n","\n","\n","        # For each batch of training data... \n","        for step, batch in enumerate(train_dataloader):\n","\n","            # Progress update every 40 batches.\n","            if step % 40 == 0 and not step == 0:\n","\n","                # Calculate elapsed time in minutes.\n","                elapsed = format_time(time.time() - t0)\n","                \n","                # Report progress.\n","                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","            # `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            # Always clear any previously calculated gradients before performing a\n","            # backward pass. \n","            model.zero_grad()        \n","\n","            # Perform a forward pass (evaluate the model on this training batch).\n","            # Specifically, we'll get the loss (because we provided labels) and the\n","            # \"logits\"-- the model outputs prior to activation.\n","            result = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels,\n","                        return_dict=True)\n","\n","            loss = result.loss\n","            logits = result.logits\n","\n","            # Accumulate the training loss over all of the batches so that we can\n","            # calculate the average loss at the end. \n","            total_train_loss += loss.item() # Tensor containing a single value\n","\n","            # Perform a backward pass to calculate the gradients.\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and take a step using the computed gradient.\n","            # The optimizer dictates the \"update rule\"--how the parameters are\n","            # modified based on their gradients, the learning rate, etc.\n","            optimizer.step()\n","\n","            # Update the learning rate\n","            scheduler.step()\n","\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_train_loss = total_train_loss / len(train_dataloader)            \n","        \n","        # Measure how long this epoch took.\n","        training_time = format_time(time.time() - t0)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","\n","\n","\n","\n","\n","        # ========================================\n","        #               Validation\n","        # ========================================\n","        # After the completion of each training epoch, measure our performance on\n","        # our validation set.\n","\n","        print(\"\")\n","        print(\"Running Validation...\")\n","\n","        t0 = time.time()\n","\n","        # Put the model in evaluation mode--the dropout layers behave differently\n","        # during evaluation.\n","        model.eval()\n","\n","        # Tracking variables \n","        total_eval_accuracy = 0\n","        total_eval_loss = 0\n","        nb_eval_steps = 0\n","\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            \n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using \n","            # the `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","            \n","            # Tell pytorch not to bother with constructing the compute graph during\n","            # the forward pass, since this is only needed for backprop (training).\n","            with torch.no_grad():        \n","\n","                # Forward pass, calculate logit predictions.\n","                # token_type_ids is the same as the \"segment ids\", which \n","                # differentiates sentence 1 and 2 in 2-sentence tasks.\n","                result = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask,\n","                            labels=b_labels,\n","                            return_dict=True)\n","\n","            # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","            # output values prior to applying an activation function like the \n","            # softmax.\n","            loss = result.loss\n","            logits = result.logits\n","                \n","            # Accumulate the validation loss.\n","            total_eval_loss += loss.item()\n","\n","            # Move logits and labels to CPU\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","\n","            # Calculate the accuracy for this batch of test sentences, and\n","            # accumulate it over all batches.\n","            total_eval_accuracy += flat_accuracy(logits, label_ids)\n","            \n","\n","        # Report the final accuracy for this validation run.\n","        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","        \n","        # Measure how long the validation run took.\n","        validation_time = format_time(time.time() - t0)\n","        \n","        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","        print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","\n","\n","        # ========================================\n","        #               Results\n","        # ========================================\n","        # Record all statistics from this epoch.\n","        training_stats.append(\n","            {\n","                'Run Number': 0,\n","                'Training ratio': r,\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss,\n","                'Valid. Loss': avg_val_loss,\n","                'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                'Validation Time': validation_time,\n","            }\n","        )\n","        df = to_dataframe(training_stats)\n","\n","        print(\"\")\n","        print(\"Training complete!\")\n","\n","        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","\n","    return df\n"]},{"cell_type":"code","execution_count":22,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":488},"executionInfo":{"elapsed":7254,"status":"error","timestamp":1673305239278,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"3WtGVDAhQIGp","outputId":"2a37da2b-cd53-4ab1-cebd-ffc70ff034e1"},"outputs":[{"name":"stdout","output_type":"stream","text":["   48 training samples\n","   48 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"ename":"KeyboardInterrupt","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-22-8c5adab48428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m","\u001b[0;32m<ipython-input-21-33c83fd5e38d>\u001b[0m in \u001b[0;36mrunner\u001b[0;34m(training_ratio)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;31m# Perform a backward pass to calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# Clip the norm of the gradients to 1.0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n","\u001b[0;31mKeyboardInterrupt\u001b[0m: "]}],"source":["# test\n","# df = runner(0.5)"]},{"cell_type":"code","execution_count":23,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":321271,"status":"ok","timestamp":1673305568365,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"JTd1IZlXHChA","outputId":"3fee486a-0669-4c14-f370-11b74f44761c"},"outputs":[{"name":"stdout","output_type":"stream","text":["========= Running...=============\n","=== ratio: 0.9, run_num:0===\n","   86 training samples\n","   10 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 1.17\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.50\n","  Validation Loss: 1.13\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:08 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 1.06\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.50\n","  Validation Loss: 1.13\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:16 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.99\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.50\n","  Validation Loss: 1.11\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:24 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.95\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.50\n","  Validation Loss: 1.10\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:32 (h:mm:ss)\n","========= Running...=============\n","=== ratio: 0.9, run_num:1===\n","   86 training samples\n","   10 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 1.01\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.80\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:08 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.91\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.78\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:16 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.85\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.76\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:24 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.82\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.76\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:32 (h:mm:ss)\n","========= Running...=============\n","=== ratio: 0.9, run_num:2===\n","   86 training samples\n","   10 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.74\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.74\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:08 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.70\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.60\n","  Validation Loss: 0.79\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:16 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.64\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.68\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:24 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.60\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.68\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:32 (h:mm:ss)\n","========= Running...=============\n","=== ratio: 0.9, run_num:3===\n","   86 training samples\n","   10 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.50\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.60\n","  Validation Loss: 0.96\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:08 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.61\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.69\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:16 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.49\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.71\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:24 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.52\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.72\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:32 (h:mm:ss)\n","========= Running...=============\n","=== ratio: 0.9, run_num:4===\n","   86 training samples\n","   10 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.39\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.60\n","  Validation Loss: 0.96\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:08 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.47\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.63\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:16 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.37\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.61\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:24 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.38\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.65\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:32 (h:mm:ss)\n","========= Running...=============\n","=== ratio: 0.9, run_num:5===\n","   86 training samples\n","   10 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.28\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.71\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:08 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.31\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.69\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:16 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.27\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.69\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:24 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.27\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.63\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:32 (h:mm:ss)\n","========= Running...=============\n","=== ratio: 0.9, run_num:6===\n","   86 training samples\n","   10 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.20\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.60\n","  Validation Loss: 0.80\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:08 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.19\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.84\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:16 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.17\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.64\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:24 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.18\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.67\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:32 (h:mm:ss)\n","========= Running...=============\n","=== ratio: 0.9, run_num:7===\n","   86 training samples\n","   10 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.10\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.76\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:08 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.11\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.77\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:16 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.09\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.83\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:24 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.09\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.87\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:32 (h:mm:ss)\n","========= Running...=============\n","=== ratio: 0.9, run_num:8===\n","   86 training samples\n","   10 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.05\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.79\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:08 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.05\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.98\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:16 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.04\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.90\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:24 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.05\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.91\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:32 (h:mm:ss)\n","========= Running...=============\n","=== ratio: 0.9, run_num:9===\n","   86 training samples\n","   10 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.03\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.60\n","  Validation Loss: 1.03\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:08 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.02\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.60\n","  Validation Loss: 1.07\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:16 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.02\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.60\n","  Validation Loss: 1.00\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:24 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.02\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.60\n","  Validation Loss: 1.07\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:32 (h:mm:ss)\n","       Run Number  Training ratio  Training Loss  Valid. Loss  Valid. Accur.  \\\n","epoch                                                                          \n","1               9             0.9          0.030        1.030            0.6   \n","2               9             0.9          0.024        1.074            0.6   \n","3               9             0.9          0.022        0.996            0.6   \n","4               9             0.9          0.024        1.075            0.6   \n","1               8             0.9          0.054        0.791            0.8   \n","2               8             0.9          0.053        0.976            0.7   \n","3               8             0.9          0.043        0.899            0.8   \n","4               8             0.9          0.050        0.911            0.7   \n","1               7             0.9          0.101        0.761            0.7   \n","2               7             0.9          0.108        0.774            0.8   \n","3               7             0.9          0.092        0.833            0.7   \n","4               7             0.9          0.089        0.870            0.7   \n","1               6             0.9          0.199        0.798            0.6   \n","2               6             0.9          0.187        0.836            0.7   \n","3               6             0.9          0.173        0.640            0.7   \n","4               6             0.9          0.178        0.670            0.7   \n","1               5             0.9          0.276        0.712            0.7   \n","2               5             0.9          0.315        0.686            0.8   \n","3               5             0.9          0.267        0.693            0.7   \n","4               5             0.9          0.272        0.632            0.7   \n","1               4             0.9          0.385        0.956            0.6   \n","2               4             0.9          0.473        0.633            0.8   \n","3               4             0.9          0.372        0.606            0.8   \n","4               4             0.9          0.381        0.651            0.8   \n","1               3             0.9          0.496        0.964            0.6   \n","2               3             0.9          0.609        0.690            0.8   \n","3               3             0.9          0.491        0.705            0.8   \n","4               3             0.9          0.519        0.717            0.8   \n","1               2             0.9          0.743        0.738            0.8   \n","2               2             0.9          0.696        0.789            0.6   \n","3               2             0.9          0.643        0.684            0.8   \n","4               2             0.9          0.602        0.683            0.8   \n","1               1             0.9          1.008        0.797            0.8   \n","2               1             0.9          0.912        0.778            0.7   \n","3               1             0.9          0.848        0.763            0.8   \n","4               1             0.9          0.819        0.756            0.7   \n","1               0             0.9          1.168        1.128            0.5   \n","2               0             0.9          1.061        1.125            0.5   \n","3               0             0.9          0.993        1.108            0.5   \n","4               0             0.9          0.955        1.095            0.5   \n","\n","      Training Time Validation Time  \n","epoch                                \n","1           0:00:08         0:00:00  \n","2           0:00:08         0:00:00  \n","3           0:00:08         0:00:00  \n","4           0:00:08         0:00:00  \n","1           0:00:08         0:00:00  \n","2           0:00:08         0:00:00  \n","3           0:00:08         0:00:00  \n","4           0:00:08         0:00:00  \n","1           0:00:08         0:00:00  \n","2           0:00:08         0:00:00  \n","3           0:00:08         0:00:00  \n","4           0:00:08         0:00:00  \n","1           0:00:08         0:00:00  \n","2           0:00:08         0:00:00  \n","3           0:00:08         0:00:00  \n","4           0:00:08         0:00:00  \n","1           0:00:08         0:00:00  \n","2           0:00:08         0:00:00  \n","3           0:00:08         0:00:00  \n","4           0:00:08         0:00:00  \n","1           0:00:08         0:00:00  \n","2           0:00:08         0:00:00  \n","3           0:00:08         0:00:00  \n","4           0:00:08         0:00:00  \n","1           0:00:08         0:00:00  \n","2           0:00:08         0:00:00  \n","3           0:00:08         0:00:00  \n","4           0:00:08         0:00:00  \n","1           0:00:08         0:00:00  \n","2           0:00:08         0:00:00  \n","3           0:00:08         0:00:00  \n","4           0:00:08         0:00:00  \n","1           0:00:08         0:00:00  \n","2           0:00:08         0:00:00  \n","3           0:00:08         0:00:00  \n","4           0:00:08         0:00:00  \n","1           0:00:08         0:00:00  \n","2           0:00:08         0:00:00  \n","3           0:00:08         0:00:00  \n","4           0:00:08         0:00:00  \n"]}],"source":["# set the number of loops \n","run_num = 10\n","# set training data ratio\n","ratios = [0.9]\n","for r in ratios:\n","\n","    # create a dataframe to saveto\n","    final_df = pd.DataFrame()\n","    \n","    for counter in range(run_num):\n","        print('========= Running...=============')\n","        print('=== ratio: '+str(r)+', run_num:'+str(counter)+'===')\n","\n","        df = runner(r)\n","        # count 'Run Number'\n","        df['Run Number'] = counter\n","        # append to the final dataframe\n","        final_df = pd.concat([df, final_df])\n","        \n","    # save dataframe\n","    print(final_df) \n","    filepath = './Pickledfiles/bert_reasoninglevel/'+str(r)+'trainratio_'+str(run_num)+'runs_'+'4epochs.pkl'\n","    final_df.to_pickle(filepath)\n","\n"]},{"cell_type":"code","execution_count":74,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":20,"status":"ok","timestamp":1673281508859,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"X1ztpnamU7qp","outputId":"680b001c-8864-49e4-b6ca-dc2a65ba4cb1"},"outputs":[{"name":"stdout","output_type":"stream","text":["       Run Number  Training ratio  Training Loss  Valid. Loss  Valid. Accur.  \\\n","epoch                                                                          \n","1               0             0.5      5.948e-05    2.286e-05            1.0   \n","2               0             0.5      3.409e-05    2.309e-05            1.0   \n","\n","      Training Time Validation Time  \n","epoch                                \n","1           0:00:04         0:00:02  \n","2           0:00:04         0:00:02  \n"]}],"source":["df = pd.read_pickle(r'./Pickledfiles/bert_reasoninglevel/0.5trainratio_1runs.pkl')\n","print(df)"]},{"cell_type":"markdown","metadata":{"id":"NirHP_B-Wu8B"},"source":["# OLD CODE"]},{"cell_type":"code","execution_count":33,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":383796,"status":"ok","timestamp":1673277002178,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"H1b87vXVDWZu","outputId":"6bf54dd5-667d-492e-8334-e09752c25f34"},"outputs":[{"name":"stdout","output_type":"stream","text":["   48 training samples\n","   48 validation samples\n","\n","======== Epoch 1 / 10 ========\n","Training...\n"]},{"name":"stderr","output_type":"stream","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"name":"stdout","output_type":"stream","text":["\n","  Average training loss: 1.19\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.62\n","  Validation Loss: 1.01\n","  Validation took: 0:00:02\n","\n","======== Epoch 2 / 10 ========\n","Training...\n","\n","  Average training loss: 1.11\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.62\n","  Validation Loss: 0.98\n","  Validation took: 0:00:02\n","\n","======== Epoch 3 / 10 ========\n","Training...\n","\n","  Average training loss: 1.07\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.60\n","  Validation Loss: 0.99\n","  Validation took: 0:00:02\n","\n","======== Epoch 4 / 10 ========\n","Training...\n","\n","  Average training loss: 0.97\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.58\n","  Validation Loss: 1.03\n","  Validation took: 0:00:02\n","\n","======== Epoch 5 / 10 ========\n","Training...\n","\n","  Average training loss: 0.92\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.60\n","  Validation Loss: 0.98\n","  Validation took: 0:00:02\n","\n","======== Epoch 6 / 10 ========\n","Training...\n","\n","  Average training loss: 0.86\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.52\n","  Validation Loss: 0.98\n","  Validation took: 0:00:02\n","\n","======== Epoch 7 / 10 ========\n","Training...\n","\n","  Average training loss: 0.81\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.52\n","  Validation Loss: 1.00\n","  Validation took: 0:00:02\n","\n","======== Epoch 8 / 10 ========\n","Training...\n","\n","  Average training loss: 0.77\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.52\n","  Validation Loss: 0.99\n","  Validation took: 0:00:02\n","\n","======== Epoch 9 / 10 ========\n","Training...\n","\n","  Average training loss: 0.77\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.52\n","  Validation Loss: 0.98\n","  Validation took: 0:00:02\n","\n","======== Epoch 10 / 10 ========\n","Training...\n","\n","  Average training loss: 0.73\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.52\n","  Validation Loss: 0.98\n","  Validation took: 0:00:02\n","\n","Training complete!\n","Total training took 0:01:05 (h:mm:ss)\n","   57 training samples\n","   39 validation samples\n","\n","======== Epoch 1 / 10 ========\n","Training...\n","\n","  Average training loss: 0.88\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.75\n","  Validation Loss: 0.73\n","  Validation took: 0:00:01\n","\n","======== Epoch 2 / 10 ========\n","Training...\n","\n","  Average training loss: 0.79\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.68\n","  Validation Loss: 0.84\n","  Validation took: 0:00:01\n","\n","======== Epoch 3 / 10 ========\n","Training...\n","\n","  Average training loss: 0.74\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.77\n","  Validation Loss: 0.74\n","  Validation took: 0:00:01\n","\n","======== Epoch 4 / 10 ========\n","Training...\n","\n","  Average training loss: 0.69\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.77\n","  Validation Loss: 0.72\n","  Validation took: 0:00:01\n","\n","======== Epoch 5 / 10 ========\n","Training...\n","\n","  Average training loss: 0.60\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.66\n","  Validation Loss: 0.78\n","  Validation took: 0:00:01\n","\n","======== Epoch 6 / 10 ========\n","Training...\n","\n","  Average training loss: 0.55\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.71\n","  Validation Loss: 0.82\n","  Validation took: 0:00:01\n","\n","======== Epoch 7 / 10 ========\n","Training...\n","\n","  Average training loss: 0.51\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.75\n","  Validation Loss: 0.73\n","  Validation took: 0:00:01\n","\n","======== Epoch 8 / 10 ========\n","Training...\n","\n","  Average training loss: 0.50\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.75\n","  Validation Loss: 0.71\n","  Validation took: 0:00:01\n","\n","======== Epoch 9 / 10 ========\n","Training...\n","\n","  Average training loss: 0.48\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.75\n","  Validation Loss: 0.73\n","  Validation took: 0:00:01\n","\n","======== Epoch 10 / 10 ========\n","Training...\n","\n","  Average training loss: 0.44\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.73\n","  Validation Loss: 0.74\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:01:11 (h:mm:ss)\n","   67 training samples\n","   29 validation samples\n","\n","======== Epoch 1 / 10 ========\n","Training...\n","\n","  Average training loss: 0.62\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.41\n","  Validation Loss: 1.23\n","  Validation took: 0:00:01\n","\n","======== Epoch 2 / 10 ========\n","Training...\n","\n","  Average training loss: 0.55\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.84\n","  Validation took: 0:00:01\n","\n","======== Epoch 3 / 10 ========\n","Training...\n","\n","  Average training loss: 0.50\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.92\n","  Validation took: 0:00:01\n","\n","======== Epoch 4 / 10 ========\n","Training...\n","\n","  Average training loss: 0.37\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.40\n","  Validation Loss: 1.21\n","  Validation took: 0:00:01\n","\n","======== Epoch 5 / 10 ========\n","Training...\n","\n","  Average training loss: 0.33\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.66\n","  Validation Loss: 0.96\n","  Validation took: 0:00:01\n","\n","======== Epoch 6 / 10 ========\n","Training...\n","\n","  Average training loss: 0.28\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.66\n","  Validation Loss: 0.94\n","  Validation took: 0:00:01\n","\n","======== Epoch 7 / 10 ========\n","Training...\n","\n","  Average training loss: 0.27\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.65\n","  Validation Loss: 1.05\n","  Validation took: 0:00:01\n","\n","======== Epoch 8 / 10 ========\n","Training...\n","\n","  Average training loss: 0.37\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.65\n","  Validation Loss: 1.08\n","  Validation took: 0:00:01\n","\n","======== Epoch 9 / 10 ========\n","Training...\n","\n","  Average training loss: 0.20\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.65\n","  Validation Loss: 1.05\n","  Validation took: 0:00:01\n","\n","======== Epoch 10 / 10 ========\n","Training...\n","\n","  Average training loss: 0.20\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.65\n","  Validation Loss: 1.05\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:01:17 (h:mm:ss)\n","   76 training samples\n","   20 validation samples\n","\n","======== Epoch 1 / 10 ========\n","Training...\n","\n","  Average training loss: 0.41\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.84\n","  Validation Loss: 0.61\n","  Validation took: 0:00:01\n","\n","======== Epoch 2 / 10 ========\n","Training...\n","\n","  Average training loss: 0.31\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.78\n","  Validation Loss: 0.70\n","  Validation took: 0:00:01\n","\n","======== Epoch 3 / 10 ========\n","Training...\n","\n","  Average training loss: 0.23\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.81\n","  Validation Loss: 0.63\n","  Validation took: 0:00:01\n","\n","======== Epoch 4 / 10 ========\n","Training...\n","\n","  Average training loss: 0.21\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.81\n","  Validation Loss: 0.66\n","  Validation took: 0:00:01\n","\n","======== Epoch 5 / 10 ========\n","Training...\n","\n","  Average training loss: 0.18\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.72\n","  Validation Loss: 0.83\n","  Validation took: 0:00:01\n","\n","======== Epoch 6 / 10 ========\n","Training...\n","\n","  Average training loss: 0.16\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.81\n","  Validation Loss: 0.70\n","  Validation took: 0:00:01\n","\n","======== Epoch 7 / 10 ========\n","Training...\n","\n","  Average training loss: 0.16\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.81\n","  Validation Loss: 0.69\n","  Validation took: 0:00:01\n","\n","======== Epoch 8 / 10 ========\n","Training...\n","\n","  Average training loss: 0.14\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.81\n","  Validation Loss: 0.73\n","  Validation took: 0:00:01\n","\n","======== Epoch 9 / 10 ========\n","Training...\n","\n","  Average training loss: 0.13\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.81\n","  Validation Loss: 0.75\n","  Validation took: 0:00:01\n","\n","======== Epoch 10 / 10 ========\n","Training...\n","\n","  Average training loss: 0.13\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.81\n","  Validation Loss: 0.74\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:01:22 (h:mm:ss)\n","   86 training samples\n","   10 validation samples\n","\n","======== Epoch 1 / 10 ========\n","Training...\n","\n","  Average training loss: 0.32\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.20\n","  Validation Loss: 1.79\n","  Validation took: 0:00:00\n","\n","======== Epoch 2 / 10 ========\n","Training...\n","\n","  Average training loss: 0.31\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation Loss: 0.55\n","  Validation took: 0:00:00\n","\n","======== Epoch 3 / 10 ========\n","Training...\n","\n","  Average training loss: 0.20\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation Loss: 0.61\n","  Validation took: 0:00:00\n","\n","======== Epoch 4 / 10 ========\n","Training...\n","\n","  Average training loss: 0.19\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.71\n","  Validation took: 0:00:00\n","\n","======== Epoch 5 / 10 ========\n","Training...\n","\n","  Average training loss: 0.20\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.65\n","  Validation took: 0:00:00\n","\n","======== Epoch 6 / 10 ========\n","Training...\n","\n","  Average training loss: 0.16\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.90\n","  Validation Loss: 0.66\n","  Validation took: 0:00:00\n","\n","======== Epoch 7 / 10 ========\n","Training...\n","\n","  Average training loss: 0.13\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.81\n","  Validation took: 0:00:00\n","\n","======== Epoch 8 / 10 ========\n","Training...\n","\n","  Average training loss: 0.11\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.70\n","  Validation Loss: 0.87\n","  Validation took: 0:00:00\n","\n","======== Epoch 9 / 10 ========\n","Training...\n","\n","  Average training loss: 0.13\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.84\n","  Validation took: 0:00:00\n","\n","======== Epoch 10 / 10 ========\n","Training...\n","\n","  Average training loss: 0.10\n","  Training epcoh took: 0:00:08\n","\n","Running Validation...\n","  Accuracy: 0.80\n","  Validation Loss: 0.80\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:01:28 (h:mm:ss)\n"]}],"source":["# ======================================================\n","# MAIN\n","# ======================================================\n","# Combine the training inputs into a TensorDataset\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","ratios = [0.5, 0.6, 0.7, 0.8, 0.9]\n","\n","\n","# loop training for each training-test ratios\n","for r in ratios:\n","\n","    batch_size = 16 # CUDA out of memory if using 32\n","    \n","    # split data\n","    train_dataset, val_dataset = train_val_split(dataset, r)\n","    # format data\n","    train_dataloader, validation_dataloader = data_loader(train_dataset, \n","                                                          val_dataset,\n","                                                          batch_size)\n","\n","\n","\n","    # ======================================================\n","    # Main - hyperparameters\n","    # ======================================================\n","    # Create optimiser\n","    optimizer = AdamW(model.parameters(),\n","                    lr = 2e-5,  # args.learning rate - default is 5e-5\n","                    eps = 1e-8, # args.adam_epsilon - default is 1e-8\n","    )\n","\n","    # Number of training epochs. The BERT authors recommended between 2 and 4. \n","    epochs = 2\n","  \n","    # Total number of training steps is [number of batches] x [number of epochs]\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Create the learning rate scheduler. # Â≠¶‰π†ÁéáÈ¢ÑÁÉ≠\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps = 0, # Default value in run_glue.py\n","                                                num_training_steps = total_steps,\n","                                                )\n","\n","\n","\n","    # ======================================================\n","    # MAIN - Training loop\n","    # ======================================================\n","    # Set the seed value all over the place to make this reproducible.\n","    seed_val = 42\n","\n","    random.seed(seed_val)\n","    np.random.seed(seed_val)\n","    torch.manual_seed(seed_val)\n","    torch.cuda.manual_seed_all(seed_val)\n","\n","    # We'll store a number of quantities such as training and validation loss, \n","    # validation accuracy, and timings.\n","    training_stats = []\n","\n","    # Measure the total training time for the whole run.\n","    total_t0 = time.time()\n","\n","\n","\n","    # For each epoch...\n","    for epoch_i in range(0, epochs):\n","        \n","        # ========================================\n","        #               Training\n","        # ========================================\n","        \n","        # Perform one full pass over the training set.\n","\n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        # Measure how long the training epoch takes.\n","        t0 = time.time()\n","\n","        # Reset the total loss for this epoch.\n","        total_train_loss = 0\n","\n","        # Put the model into training mode. \n","        model.train()\n","\n","\n","\n","        # For each batch of training data... \n","        for step, batch in enumerate(train_dataloader):\n","\n","            # Progress update every 40 batches.\n","            if step % 40 == 0 and not step == 0:\n","\n","                # Calculate elapsed time in minutes.\n","                elapsed = format_time(time.time() - t0)\n","                \n","                # Report progress.\n","                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","            # `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            # Always clear any previously calculated gradients before performing a\n","            # backward pass. \n","            model.zero_grad()        \n","\n","            # Perform a forward pass (evaluate the model on this training batch).\n","            # Specifically, we'll get the loss (because we provided labels) and the\n","            # \"logits\"-- the model outputs prior to activation.\n","            result = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels,\n","                        return_dict=True)\n","\n","            loss = result.loss\n","            logits = result.logits\n","\n","            # Accumulate the training loss over all of the batches so that we can\n","            # calculate the average loss at the end. \n","            total_train_loss += loss.item() # Tensor containing a single value\n","\n","            # Perform a backward pass to calculate the gradients.\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and take a step using the computed gradient.\n","            # The optimizer dictates the \"update rule\"--how the parameters are\n","            # modified based on their gradients, the learning rate, etc.\n","            optimizer.step()\n","\n","            # Update the learning rate\n","            scheduler.step()\n","\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_train_loss = total_train_loss / len(train_dataloader)            \n","        \n","        # Measure how long this epoch took.\n","        training_time = format_time(time.time() - t0)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","\n","\n","\n","\n","\n","        # ========================================\n","        #               Validation\n","        # ========================================\n","        # After the completion of each training epoch, measure our performance on\n","        # our validation set.\n","\n","        print(\"\")\n","        print(\"Running Validation...\")\n","\n","        t0 = time.time()\n","\n","        # Put the model in evaluation mode--the dropout layers behave differently\n","        # during evaluation.\n","        model.eval()\n","\n","        # Tracking variables \n","        total_eval_accuracy = 0\n","        total_eval_loss = 0\n","        nb_eval_steps = 0\n","\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            \n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using \n","            # the `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","            \n","            # Tell pytorch not to bother with constructing the compute graph during\n","            # the forward pass, since this is only needed for backprop (training).\n","            with torch.no_grad():        \n","\n","                # Forward pass, calculate logit predictions.\n","                # token_type_ids is the same as the \"segment ids\", which \n","                # differentiates sentence 1 and 2 in 2-sentence tasks.\n","                result = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask,\n","                            labels=b_labels,\n","                            return_dict=True)\n","\n","            # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","            # output values prior to applying an activation function like the \n","            # softmax.\n","            loss = result.loss\n","            logits = result.logits\n","                \n","            # Accumulate the validation loss.\n","            total_eval_loss += loss.item()\n","\n","            # Move logits and labels to CPU\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","\n","            # Calculate the accuracy for this batch of test sentences, and\n","            # accumulate it over all batches.\n","            total_eval_accuracy += flat_accuracy(logits, label_ids)\n","            \n","\n","        # Report the final accuracy for this validation run.\n","        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","        \n","        # Measure how long the validation run took.\n","        validation_time = format_time(time.time() - t0)\n","        \n","        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","        print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","\n","\n","\n","        # Record all statistics from this epoch.\n","        training_stats.append(\n","            {\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss,\n","                'Valid. Loss': avg_val_loss,\n","                'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                'Validation Time': validation_time\n","            }\n","        )\n","\n","        # save dataframe\n","        df = to_dataframe(training_stats)\n","        filepath = 'Pickledfiles/bert_reasoninglevel_trainratio'+str(r)+'_epochs'+str(epochs)+'_seedval42'+'_batchsize16'\n","        df.to_pickle(filepath)\n","\n","    print(\"\")\n","    print(\"Training complete!\")\n","\n","    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"markdown","metadata":{"id":"-arhBqEY4I-r"},"source":["\n","4.3 Training loop\n","\n","Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. \n","\n","> *Thank you to [Stas Bekman](https://ca.linkedin.com/in/stasbekman) for contributing the insights and code for using validation loss to detect over-fitting!*\n","\n","Training: \n","* Unpack our data inputs and labels \n","* Load data onto GPU for acceleration\n","* Clear out the gradients calculated in the previous pass\n","    * In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out\n","* Forward pass (feed input data through the network)\n","* Backward pass (backpropagation)\n","* Tell the network to update parameters with optimizer.step()\n","* Track variables for minitoring progress\n","\n","Evaluation:\n","* Unpack our data inputs and labels\n","* Load data onto the GPU for acceleration\n","* Forward pass (feed input data through the network)\n","* Compute loss on our validation data and track variables for monitoring process\n","\n","\n","Pytorch hides all the detailed calculations from us, but we've commented the code to print out which of the above steps are happening on each line.\n","\n","> *PyTorch also has some [beginner tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) which you may also find helpful.*"]},{"cell_type":"markdown","metadata":{"id":"-eJilZ9_1W7A"},"source":["Let's view the summary of the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":450},"executionInfo":{"elapsed":22,"status":"error","timestamp":1670943700551,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"ZJgACViioLT1","outputId":"972986e9-cec9-4941-cdb4-236abdb0f892"},"outputs":[{"ename":"FileNotFoundError","evalue":"ignored","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-21-32c8bb6fecb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mratios\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# path = 'Pickledfiles/bert_reasoninglevel_trainratio'+str(i)+'_epochs1_seedval42_batchsize16.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert_reasoninglevel_trainratio0.5_epochs1_seedval42_batchsize16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvalidacc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Valid. Accur.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \"\"\"\n\u001b[1;32m    195\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bert_reasoninglevel_trainratio0.5_epochs1_seedval42_batchsize16'"]}],"source":["import pandas as pd\n","ratios = [0.5,0.6,0.7,0.8,0.9]\n","validacc_list = []\n","\n","for i in ratios:\n","    # path = 'Pickledfiles/bert_reasoninglevel_trainratio'+str(i)+'_epochs1_seedval42_batchsize16.pkl'\n","    df = pd.read_pickle('bert_reasoninglevel_trainratio0.5_epochs1_seedval42_batchsize16')\n","    validacc_list.append(df.iloc[0]['Valid. Accur.'])\n","    print(df)\n","\n","print(validacc_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1gSwd0WwLTf"},"outputs":[],"source":["pip install git+https://github.com/garrettj403/SciencePlots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WtqOdDP5vt4N"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBwtofW32kdt"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"YsdFtpZ94gPu"},"source":["5. Performance on Test Set"]},{"cell_type":"markdown","metadata":{"id":"rU_BBWgy4lK_"},"source":["5.1 Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PRXxjlsM5RZ2"},"outputs":[],"source":["# Create the DataLoader.\n","prediction_data = val_dataset\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = validation_dataloader"]},{"cell_type":"markdown","metadata":{"id":"DHFs1AkB434s"},"source":["5.2 Evaluate on Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KMdiBA9422k"},"outputs":[],"source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test reports...'.format(len(val_dataset)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions.\n","      result = model(b_input_ids, \n","                     token_type_ids=None, \n","                     attention_mask=b_input_mask,\n","                     return_dict=True)\n","\n","  logits = result.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"]},{"cell_type":"markdown","metadata":{"id":"SXlId81P6Kh2"},"source":["Accuracy on the CoLA benchmark is measured using the \"[Matthews correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)\" (MCC).\n","\n","We use MCC here because the classes are imbalanced:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJD1D-g86NUs"},"outputs":[],"source":["from sklearn.metrics import matthews_corrcoef\n","\n","matthews_set = []\n","\n","# Evaluate each test batch using Matthew's correlation coefficient\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  \n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  \n","  # Calculate and store the coef for this batch.  \n","  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n","  matthews_set.append(matthews)"]},{"cell_type":"markdown","metadata":{"id":"Kx7kqXn6D6VY"},"source":["The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches. \n","\n","Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"poxIZ-UiD-GZ"},"outputs":[],"source":["# Create a barplot showing the MCC score for each batch of test samples.\n","ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n","\n","plt.title('MCC Score per Batch')\n","plt.ylabel('MCC Score (-1 to +1)')\n","plt.xlabel('Batch #')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nyuwg22cEFWy"},"outputs":[],"source":["# Combine the results across all batches. \n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","# For each sample, pick the label (0 or 1) with the higher score.\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('Total MCC: %.3f' % mcc)"]},{"cell_type":"markdown","metadata":{"id":"dJOEigZaddtC"},"source":["When we actually convert all of our sentences, we'll use the `tokenize.encode` function to handle both steps, rather than calling `tokenize` and `convert_tokens_to_ids` separately. \n","\n","Before we can do that, though, we need to talk about some of BERT's formatting requirements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGYv9paXSDm9"},"outputs":[],"source":["# Print the original report.\n","print('length:',len(corpus[0]),';  Original: ', corpus[0])\n","\n","# Print the report split into tokens.\n","print('length:',len(tokenizer.tokenize(corpus[0])),';  Tokenized: ', tokenizer.tokenize(corpus[0]))\n","\n","# Print the report mapped to token ids.\n","print('length:',len(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(corpus[0]))) , ';  Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(corpus[0])))"]},{"cell_type":"markdown","metadata":{"id":"RgPW9ujVSFLv"},"source":["Let's apply the tokenizer to one report just to see the output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrik8ScLRlkd"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]}],"metadata":{"accelerator":"GPU","colab":{"authorship_tag":"ABX9TyNTqKnKuUUYe8nhgHIBATcN","collapsed_sections":["XwBJenjTQSO6","NirHP_B-Wu8B"],"mount_file_id":"1zl0AkYFg34_0Z5JQHg4nSJD93v_ugfuZ","provenance":[{"file_id":"1ebp9aVPlje_yA1gfGveLdjmBTCbyT-8d","timestamp":1670259501349}]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.13 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.9.13"},"vscode":{"interpreter":{"hash":"397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"}},"widgets":{"application/vnd.jupyter.widget-state+json":{"0273e18cd0fc4067941bd14fd4912672":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f43834dd2a9041b9a1682b97cb7b53ba","placeholder":"‚Äã","style":"IPY_MODEL_42594abd60ca4cd99903db92603d433d","value":"Downloading: 100%"}},"09408edd50494d75b70f7df286e6a23e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_e088ece32f5a41aab2c358a26cf19a05","IPY_MODEL_4622a48bc06b4b2bacb5c7b462d0ce1e","IPY_MODEL_eaf4f2f39c8748e48da576352ea8d348"],"layout":"IPY_MODEL_9b5092461a794cdc92a4ceb41093b64b"}},"0a1c18fe9cad4c3e94e89925623768ac":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_0273e18cd0fc4067941bd14fd4912672","IPY_MODEL_6dad9135e1c74d9fb7ba0e7632f6bb4a","IPY_MODEL_200d806eead24cc8859fe2d46306d414"],"layout":"IPY_MODEL_cc2230712c0641b09bc7cd1d21a75ad8"}},"1135443edbc8488eb0f61b02f0d67f32":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"200d806eead24cc8859fe2d46306d414":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_b137db95ace74a2591f64e7f2eb36975","placeholder":"‚Äã","style":"IPY_MODEL_cce7f11c38a34d75ab46e3a128cc0205","value":" 440M/440M [00:11&lt;00:00, 38.7MB/s]"}},"261788bd52f44d8e81934bbd1a9b1b60":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"2a4a4a7896bf411dab3cb996b0badac4":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"3c9d03d962a44453b06f90194569a879":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_ef02cf1d3c444e949f1dfb95f9e855e8","placeholder":"‚Äã","style":"IPY_MODEL_f1da04fac7614f7884861f7328518c4e","value":"Downloading: 100%"}},"3edac39cfc2249f3841043f2f47df672":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"42594abd60ca4cd99903db92603d433d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"4622a48bc06b4b2bacb5c7b462d0ce1e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_261788bd52f44d8e81934bbd1a9b1b60","max":28,"min":0,"orientation":"horizontal","style":"IPY_MODEL_63eb06bf8ea2417485c91350f8f8ae9d","value":28}},"4a4c419f4bbc4225b3a7c2a09ee061da":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_3c9d03d962a44453b06f90194569a879","IPY_MODEL_5ea18a3abb104f7984521fc6fc1dbe5a","IPY_MODEL_54d58499ac044d3ab19603639bf8a18d"],"layout":"IPY_MODEL_793a754372614d46bc7814f16d3a6b4b"}},"54d58499ac044d3ab19603639bf8a18d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_6b742789ed19406d83683b09af7b2330","placeholder":"‚Äã","style":"IPY_MODEL_91efdfc8128f44c699a3d560429d011d","value":" 570/570 [00:00&lt;00:00, 12.5kB/s]"}},"5b62ab20a9d846e8a8ace0a938b98711":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"5ce05275cd194aca95e572e138ff23f1":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5ea18a3abb104f7984521fc6fc1dbe5a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_8601b810b324448ba0fd60eb57cc6b8d","max":570,"min":0,"orientation":"horizontal","style":"IPY_MODEL_76699a9d22fa4e2e8fd8d0801fa3c6a2","value":570}},"63eb06bf8ea2417485c91350f8f8ae9d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"68dd44ee33514a07be821a31c1d47291":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_5ce05275cd194aca95e572e138ff23f1","max":231508,"min":0,"orientation":"horizontal","style":"IPY_MODEL_b7103b7f388f4548b2bdc37b94a7d5e4","value":231508}},"69a7706ce17d4cf1817daaeecec01e91":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6b742789ed19406d83683b09af7b2330":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"6dad9135e1c74d9fb7ba0e7632f6bb4a":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"FloatProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"FloatProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"success","description":"","description_tooltip":null,"layout":"IPY_MODEL_69a7706ce17d4cf1817daaeecec01e91","max":440473133,"min":0,"orientation":"horizontal","style":"IPY_MODEL_cc20592b4dbf45e99743f2831a333e79","value":440473133}},"7584aaec02f84f8c8cedacb7f6f17e1b":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_2a4a4a7896bf411dab3cb996b0badac4","placeholder":"‚Äã","style":"IPY_MODEL_c0c930eff5164a65b0288178132a2128","value":"Downloading: 100%"}},"76699a9d22fa4e2e8fd8d0801fa3c6a2":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"793a754372614d46bc7814f16d3a6b4b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"7faac40a063a425fac5e5a2496f11eef":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8601b810b324448ba0fd60eb57cc6b8d":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"86e0f7a7218e45e29999376741ce4985":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_7584aaec02f84f8c8cedacb7f6f17e1b","IPY_MODEL_68dd44ee33514a07be821a31c1d47291","IPY_MODEL_f88e3510a5074c499607b7b7256ed6c4"],"layout":"IPY_MODEL_fa712872f41d4d559e89bab854dab599"}},"91efdfc8128f44c699a3d560429d011d":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"97fd3934e71f413d918907186c52b8e5":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"9b5092461a794cdc92a4ceb41093b64b":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b137db95ace74a2591f64e7f2eb36975":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"b7103b7f388f4548b2bdc37b94a7d5e4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"c0c930eff5164a65b0288178132a2128":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"cc20592b4dbf45e99743f2831a333e79":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":""}},"cc2230712c0641b09bc7cd1d21a75ad8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cce7f11c38a34d75ab46e3a128cc0205":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"e088ece32f5a41aab2c358a26cf19a05":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_f2033804ed5346ae8f0a0bf55f811d8c","placeholder":"‚Äã","style":"IPY_MODEL_5b62ab20a9d846e8a8ace0a938b98711","value":"Downloading: 100%"}},"eaf4f2f39c8748e48da576352ea8d348":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3edac39cfc2249f3841043f2f47df672","placeholder":"‚Äã","style":"IPY_MODEL_1135443edbc8488eb0f61b02f0d67f32","value":" 28.0/28.0 [00:00&lt;00:00, 557B/s]"}},"ef02cf1d3c444e949f1dfb95f9e855e8":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f1da04fac7614f7884861f7328518c4e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"f2033804ed5346ae8f0a0bf55f811d8c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f43834dd2a9041b9a1682b97cb7b53ba":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"f88e3510a5074c499607b7b7256ed6c4":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_97fd3934e71f413d918907186c52b8e5","placeholder":"‚Äã","style":"IPY_MODEL_7faac40a063a425fac5e5a2496f11eef","value":" 232k/232k [00:00&lt;00:00, 273kB/s]"}},"fa712872f41d4d559e89bab854dab599":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}}}}},"nbformat":4,"nbformat_minor":0}
