{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PIZC4JRqGhP5"
      },
      "source": [
        "# 1. Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LNeYfLE7mGul"
      },
      "source": [
        "## 1.1 Using Colab GPU for Training\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N3upHHQ1m6Zb"
      },
      "source": [
        "Run the following the cell to confirm the GPU is detected."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0eZDJWhjmOcC",
        "outputId": "77c06691-7c05-4fba-fe35-49d8815e063e"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Found GPU at: /device:GPU:0\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "# Get the GPU device name.\n",
        "device_name = tf.test.gpu_device_name()\n",
        "\n",
        "# The device name should look like the following:\n",
        "if device_name == '/device:GPU:0':\n",
        "    print('Found GPU at: {}'.format(device_name))\n",
        "else:\n",
        "    raise SystemError('GPU device not found')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_bgovg_6hNCU"
      },
      "source": [
        "In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in out training loop"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3n7WnLmpg_Hj",
        "outputId": "8309a105-50c2-4998-d02e-7e04f2254432"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "There are 1 GPU(s) available.\n",
            "We will use the GPU: Tesla T4\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "\n",
        "# If there's a GPU available...\n",
        "if torch.cuda.is_available():    \n",
        "\n",
        "    # Tell PyTorch to use the GPU.    \n",
        "    device = torch.device(\"cuda\")\n",
        "\n",
        "    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n",
        "\n",
        "    print('We will use the GPU:', torch.cuda.get_device_name(0))\n",
        "\n",
        "# If not...\n",
        "else:\n",
        "    print('No GPU available, using the CPU instead.')\n",
        "    device = torch.device(\"cpu\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BH8PQKEhF0YD"
      },
      "source": [
        "## 1.2 Installing Hugging Face Library"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n6DvZooRmKrn",
        "outputId": "6abf3cb3-4d9d-4941-f663-3ec141b13102"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting transformers\n",
            "  Downloading transformers-4.25.1-py3-none-any.whl (5.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.8/5.8 MB\u001b[0m \u001b[31m92.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting tokenizers!=0.11.3,<0.14,>=0.11.1\n",
            "  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m105.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n",
            "Collecting huggingface-hub<1.0,>=0.10.0\n",
            "  Downloading huggingface_hub-0.11.1-py3-none-any.whl (182 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m182.4/182.4 KB\u001b[0m \u001b[31m11.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (21.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.10.0->transformers) (4.4.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.8/dist-packages (from packaging>=20.0->transformers) (3.0.9)\n",
            "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n",
            "Installing collected packages: tokenizers, huggingface-hub, transformers\n",
            "Successfully installed huggingface-hub-0.11.1 tokenizers-0.13.2 transformers-4.25.1\n"
          ]
        }
      ],
      "source": [
        "pip install transformers"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TxWOmaB9GT4q"
      },
      "source": [
        "# 2. Retrieve Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L5UDujTsGY00"
      },
      "source": [
        "## 2.1 Mount Google Drive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "4atrLvNzb0b3"
      },
      "outputs": [],
      "source": [
        "# from google.colab import drive\n",
        "# drive.mount('/content/gdrive/')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "CtXpJC70HM-H"
      },
      "outputs": [],
      "source": [
        "# %cd\n",
        "# !pwd\n",
        "# %cd /content/gdrive/MyDrive/nlp-physicseducation/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5pHTEYcyG-D9"
      },
      "source": [
        "## 2.2 Parse Data"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ln_QvsjDuynn"
      },
      "source": [
        "Specify the directories"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import io\n",
        "import requests\n",
        "import pandas as pd"
      ],
      "metadata": {
        "id": "IhFyQCTchZop"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# use Github raw url\n",
        "url = requests.get('https://raw.githubusercontent.com/jiayang-zhang/nlp-physicseducation/main/outputs/labels_cleaned_y2.csv?token=GHSAT0AAAAAAB5ZRP3HCGP34ULRE7XN5PH6Y6O7VGQ').content\n",
        "df = pd.read_csv(io.StringIO(url.decode('utf-8')),\n",
        "                encoding='utf-8', \n",
        "            skiprows = 1, \n",
        "            names=['StudentID', 'Content', 'ArgumentLevel', 'ReasoningLevel']\n",
        "            \n",
        "    )\n",
        "\n",
        "# use Gdrive folder\n",
        "# load the dataset into a pandas dataframe\n",
        "# df = pd.read_csv(\n",
        "#         dir_csv, \n",
        "#         encoding='utf-8', \n",
        "#         skiprows = 1, \n",
        "#         names=['StudentID', 'Content', 'ArgumentLevel', 'ReasoningLevel']\n",
        "# )\n",
        "\n",
        "# Report the number of reports\n",
        "print('Number of reports: {:,}\\n'.format(df.shape[0]))\n",
        "\n",
        "# Display 10 random rows from the data\n",
        "df.sample(10)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "GhqmLv3rhGZM",
        "outputId": "bb4e35bd-8f9b-4f4f-8fda-9c31aa0a6c2d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reports: 83\n",
            "\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 StudentID                                            Content  \\\n",
              "30  2ndINT_TKK454_Redacted  michelson interferometer be use in conjunction...   \n",
              "40  2ndINT_WFX625_Redacted  in this paper we investigate four different li...   \n",
              "12  2ndINT_VEJ488_Redacted  michelson interferometer have be use a a fouri...   \n",
              "34  2ndINT_GQR325_Redacted  abstra `` ct-lnterferometer be very useful in ...   \n",
              "55  2ndINT_PLS878_Redacted  the aim of the experiment be to determine prop...   \n",
              "61  2ndINT_OQP549_Redacted  in the experiment present a michelson interfer...   \n",
              "39  2ndINT_GEI074_Redacted  in this experiment a michelson interferometer ...   \n",
              "35  2ndINT_LUU804_Redacted  the wavelength spectrum and coherence length o...   \n",
              "52  2ndINT_YMJ516_Redacted  he application of device which emit radiation ...   \n",
              "9   2ndINT_GTD911_Redacted  we investigate different light source by the m...   \n",
              "\n",
              "   ArgumentLevel ReasoningLevel  \n",
              "30          deep            exp  \n",
              "40          deep           none  \n",
              "12   superficial            exp  \n",
              "34   superficial            the  \n",
              "55      extended            the  \n",
              "61          deep            exp  \n",
              "39   superficial           none  \n",
              "35      extended            bal  \n",
              "52   superficial            the  \n",
              "9       extended            the  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-432441c8-d819-423e-a3a2-3abf9824b53f\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>StudentID</th>\n",
              "      <th>Content</th>\n",
              "      <th>ArgumentLevel</th>\n",
              "      <th>ReasoningLevel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>30</th>\n",
              "      <td>2ndINT_TKK454_Redacted</td>\n",
              "      <td>michelson interferometer be use in conjunction...</td>\n",
              "      <td>deep</td>\n",
              "      <td>exp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>40</th>\n",
              "      <td>2ndINT_WFX625_Redacted</td>\n",
              "      <td>in this paper we investigate four different li...</td>\n",
              "      <td>deep</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>2ndINT_VEJ488_Redacted</td>\n",
              "      <td>michelson interferometer have be use a a fouri...</td>\n",
              "      <td>superficial</td>\n",
              "      <td>exp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>2ndINT_GQR325_Redacted</td>\n",
              "      <td>abstra `` ct-lnterferometer be very useful in ...</td>\n",
              "      <td>superficial</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>55</th>\n",
              "      <td>2ndINT_PLS878_Redacted</td>\n",
              "      <td>the aim of the experiment be to determine prop...</td>\n",
              "      <td>extended</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>61</th>\n",
              "      <td>2ndINT_OQP549_Redacted</td>\n",
              "      <td>in the experiment present a michelson interfer...</td>\n",
              "      <td>deep</td>\n",
              "      <td>exp</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>39</th>\n",
              "      <td>2ndINT_GEI074_Redacted</td>\n",
              "      <td>in this experiment a michelson interferometer ...</td>\n",
              "      <td>superficial</td>\n",
              "      <td>none</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>35</th>\n",
              "      <td>2ndINT_LUU804_Redacted</td>\n",
              "      <td>the wavelength spectrum and coherence length o...</td>\n",
              "      <td>extended</td>\n",
              "      <td>bal</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>52</th>\n",
              "      <td>2ndINT_YMJ516_Redacted</td>\n",
              "      <td>he application of device which emit radiation ...</td>\n",
              "      <td>superficial</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2ndINT_GTD911_Redacted</td>\n",
              "      <td>we investigate different light source by the m...</td>\n",
              "      <td>extended</td>\n",
              "      <td>the</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-432441c8-d819-423e-a3a2-3abf9824b53f')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-432441c8-d819-423e-a3a2-3abf9824b53f button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-432441c8-d819-423e-a3a2-3abf9824b53f');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 28
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZfT4uprKJsRG"
      },
      "source": [
        "The label 'ArgumentLevel' and 'ReasoningLevel' are mapped to numbers.\n",
        "\n",
        "Argument Level labels {'bal': 0, 'the': 1, 'exp': 2, 'none': 3}\n",
        "\n",
        "Reasoning Level labels {'extended': 0, 'deep': 1, 'expert': 2, 'superficial': 3, 'prediction': 4}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 363
        },
        "id": "3upygP5AH0NL",
        "outputId": "9d2533ff-e238-485c-c155-3d10898a6aec"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "                 StudentID                                            Content  \\\n",
              "67  2ndINT_LEH982_Redacted  correction of the systematic error due to a sl...   \n",
              "9   2ndINT_GTD911_Redacted  we investigate different light source by the m...   \n",
              "34  2ndINT_GQR325_Redacted  abstra `` ct-lnterferometer be very useful in ...   \n",
              "29  2ndINT_HDP206_Redacted  michelson interferometer be use a a spectromet...   \n",
              "46  2ndINT_GQV977_Redacted  the interferometer have be a vital tool in sev...   \n",
              "32  2ndINT_TFY667_Redacted  the aim of the experiment be to investigate th...   \n",
              "59  2ndINT_WEM893_Redacted  abstract-when light be pas through a michelson...   \n",
              "31  2ndINT_BQD756_Redacted  separation of the frequency in fourier space 4...   \n",
              "69  2ndINT_PUK317_Redacted  a fourier transform spectrometer be construct ...   \n",
              "28  2ndINT_XUL605_Redacted  a mirror in the michelson interferometer be mo...   \n",
              "\n",
              "    ArgumentLevel  ReasoningLevel  \n",
              "67              2               2  \n",
              "9               0               1  \n",
              "34              3               1  \n",
              "29              0               1  \n",
              "46              3               0  \n",
              "32              3               1  \n",
              "59              0               0  \n",
              "31              1               2  \n",
              "69              1               1  \n",
              "28              0               0  "
            ],
            "text/html": [
              "\n",
              "  <div id=\"df-91d292c0-019b-410e-8136-41fa54492aa5\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>StudentID</th>\n",
              "      <th>Content</th>\n",
              "      <th>ArgumentLevel</th>\n",
              "      <th>ReasoningLevel</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>67</th>\n",
              "      <td>2ndINT_LEH982_Redacted</td>\n",
              "      <td>correction of the systematic error due to a sl...</td>\n",
              "      <td>2</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>2ndINT_GTD911_Redacted</td>\n",
              "      <td>we investigate different light source by the m...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>34</th>\n",
              "      <td>2ndINT_GQR325_Redacted</td>\n",
              "      <td>abstra `` ct-lnterferometer be very useful in ...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>2ndINT_HDP206_Redacted</td>\n",
              "      <td>michelson interferometer be use a a spectromet...</td>\n",
              "      <td>0</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>46</th>\n",
              "      <td>2ndINT_GQV977_Redacted</td>\n",
              "      <td>the interferometer have be a vital tool in sev...</td>\n",
              "      <td>3</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>32</th>\n",
              "      <td>2ndINT_TFY667_Redacted</td>\n",
              "      <td>the aim of the experiment be to investigate th...</td>\n",
              "      <td>3</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>59</th>\n",
              "      <td>2ndINT_WEM893_Redacted</td>\n",
              "      <td>abstract-when light be pas through a michelson...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>31</th>\n",
              "      <td>2ndINT_BQD756_Redacted</td>\n",
              "      <td>separation of the frequency in fourier space 4...</td>\n",
              "      <td>1</td>\n",
              "      <td>2</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>69</th>\n",
              "      <td>2ndINT_PUK317_Redacted</td>\n",
              "      <td>a fourier transform spectrometer be construct ...</td>\n",
              "      <td>1</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>2ndINT_XUL605_Redacted</td>\n",
              "      <td>a mirror in the michelson interferometer be mo...</td>\n",
              "      <td>0</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-91d292c0-019b-410e-8136-41fa54492aa5')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-91d292c0-019b-410e-8136-41fa54492aa5 button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-91d292c0-019b-410e-8136-41fa54492aa5');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ]
          },
          "metadata": {},
          "execution_count": 29
        }
      ],
      "source": [
        "# define dict to code labels to numbers\n",
        "ReasoningLevel_dict = {'bal': 0, 'the': 1, 'exp': 2, 'none': 3}\n",
        "ArgumentLevel_dict = {'extended': 0, 'deep': 1, 'expert': 2, 'superficial': 3, 'prediction': 4}\n",
        "\n",
        "# replace to number labels\n",
        "df['ReasoningLevel'].replace(list(ReasoningLevel_dict.keys()), list(ReasoningLevel_dict.values()),inplace=True) \n",
        "df['ArgumentLevel'].replace(list(ArgumentLevel_dict.keys()), list(ArgumentLevel_dict.values()),inplace=True) \n",
        "\n",
        "\n",
        "# Display 10 random rows from the data\n",
        "df.sample(10)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OmzBpfb-KMe1"
      },
      "source": [
        "Let's extract the sentences and labels of our training set as numpy ndarrys."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e9U4SpCcQRJ8",
        "outputId": "18bfffa2-311f-4b2f-d78c-ad1ddb3bd7ba"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of reports: 83\n",
            "\n",
            "First report:     a michelson interferometer be built and use to analyze a laser lead a tungsten lamp and a mercury lamp the movement of mirror close to the null point allow for the use of the michelson interferometer a a spectrometer use a silicon pin photodiode detector and measurement program operating on linus o inteferograms be produce fourier transform with method detailed be use to find wavelength the property of the light source be found and discus white light source be usually found to have shorter coherence length than colour source due to frequency superposition the tungsten lamp exhibit a black-body emission spectrum with a calculate operating temperature of 3420±40k through comparison with expect result and far data limitation in apparatus be discover and where appropriate correct for mercury lamp data displayed a large amount of noise in the reading and thus the detector use be deem unsuitable for the analysis of this emission i nvented in 1880 the michelson interferometer have form an integral part of many scientific discovery and discipline include special relativity spectroscopy and interferometry 1 the simple set up allows for record of the interference of electromagnetic wave a single light source be input and the beam split use a beamsplitter this be follow by beam recombination with the use of two mirror at the detector the movement of one of the two mirror adjusts the path difference between the two light beam this creates interference pattern from which conclusion can be drawn about the source and it constituent frequency a michelson interferometer be built and use to analyze four different type of light source with vary filter these include a green laser two lead a mercury lamp and a tungsten lamp the filter use be green and yellow the variation in light production technique and coherence among source allow for analysis of the suitability of this interferometer for different situation a schematic of a michelson interferometer be show in fig 1. when a beam of light be incident on a beamsplitter it be split into two separate beam one reflect one transmit the reflect beam a travel to mirror m1 and back and the transmit beam b travel to mirror m2 and back for the extend source use in this investigation the interference pattern be dependent on the angle θ between the incoming fig 1 a michelson interferometer m1 and m2 be mirror m2 represent equivalent position of m2 in a parallel setup d be the distance between the beamsplitter and closest mirror and t be difference in parallel position of the mirror 2 p.4 ray and the optical axis through basic trigonometric manipulation it can be see that difference in path length φ of the two beam be where t be the difference in length of the interferometer arm a show this path difference cause interference upon beam recombination the principle of linear superposition state constructive interference occurs when φ be an integer multiple of wavelength and complete destructive interference occurs when φ be an integer plus a half wavelength the interference pattern vary with orientation of the mirror if m1 and m2 be at perfect right angle concentric circular ring call haidinger fringe will be see if they be not at right angle symmetry will be broken and straight bright line of uniform thickness form call frizeau fringe the interferometer may be use a a spectrometer when the distance between the two mirror be approximately equal the null point when use monochromatic light this be see a haidinger fringe reduce to a singularity 3 by take measurement close to the null point the coherence length l of a source may be see on an interferogram though a precise definition of coherence length be not give this be the length over which one arm of the michelson interferometer may be scan before the contrast of the interferogram becomes poor the spectral width ∆ν of the source be a measure of the range of frequency emit by the source and be where c be the speed of light and l be in metre spectral width be measure in hz 2 wavelength can be found by fourier transform of the interferogram an electromagnetic wave with electric field strength e may be express in the form where r be the position vector ω be angular frequency and t be time k be the wave vector whose modulus be 2πν where ν be inverse wavelength or wavenumber use euler 's identity the time dependant part of this function may be represent a a summation of trignometric function ie the potential for representation of any function use sine and cosine allows for fourier transform 4 the shape of an interferogram be dependant on the frequency present and the amplitude or intensity of each frequency the intensity of a signal be proportional to the square of it amplitude the interferogram contains all physical information about a wave fourier transform be use to make interferograms directly interpretable though provide no extra information a fourier transform f ω of the interferogram function in the time domain f t be use to represent data in the frequency domain these two function form a fourier transform pair and be related by this fourier transform return both positive and negative frequency the negative frequency be due to contribution from the imaginary part of the wave take an inverse function of positive frequency reconstructs the original wavelength spectrum fourier transform be carry out on a computer use summation over a large but finite number of point n 5 an interferometer be built follow the configuration in fig 1. the two mirror use be a manually adjust kinematic mirror and a motorize mirror adjust through input command in the linus terminal a thorlab 30 mm cage cube-mounted non-polarizing beamsplitter be use manufacturer specification describe a dielectric anti-reflective coat apply face with a great transmission tolerance to wavelength in the 400 -700 nm range the dielectric coat cause a phase change for one beam lead to destructive interference at zero path difference and constructive interference when λ 2 6 the silicon pin photodiode dectector use have a detection range of 400 -1100nm 7 light source be mount on a v-clamp depend on the intensity of the light source plano-convex lens of focal length 24.5 and 100.0mm be insert between source and beamsplitter to widen beam and between beamsplitter and detector to refocus the alignment of the interferometer be checked and adjust use green laser light point beam be project onto a screen place in front of the detector and the kinematic mirror move until fizeau fringe be see a magnify lens be then insert and the motorize mirror adjust until a minimum of haidinger fringe be see a white light source be then insert and an interferogram take the peak of this interferogram be indicative of a null point at the position −0.6663 ± 0.0005mm and all subsequent data be take in a 100µm range about this point the velocity of the motorize mirror be set to 0.5µms −1 theoretical calculation use the nyquist theorem 4 of the minimum speed imply a much great speed may be use however a the oscillation record by the interferogram do not form perfect sine wave many more than two data point per oscillation be necessary to ensure complete information the velocity of the mirror be also slow to reduce beating between frequency present beating be a result of the superposition of the two interference pattern this beating form an envelop function that corrupt the data follow collection of data interferograms be plot and gaussian fit overlaid the coherence length be deduce to be roughly equal to the full width half maximum of the gaussian curve and from this spectral width calculate where appropriate mean wavelength be found use a similar gaussian-overlay method on the fourier transform and estimate the mean value in case where this be not possible a value be estimate use visual approximation fourier transform spectroscopy approximates light source a point source in many case this be a good approximation however it would be beneficial to insert a small aperture between the source and the beamsplitter fig 2 show an example interferogram table 1 show key result relate to property of light source error be usually calculate through visual estimation of goodness of fit where wavelength be a single value this be the mean wavelength take at peak spectral intensity and error be the standard deviation approximate through a gaussian fit for the blue lead it be possible to computationally fit a gaussian function this produce a covarence matrix from which error parameter be found where wavelength be a range of value this be the range over which there be high spectral intensity error on the coherence length of the white lead be high due to the presence of a secondary gaussian envelope function on the interferogram error on the coherence length of unfiltered tungsten be high due minimal peak in the secondary gaussian lead to highly subjective fitting of the gaussian overlay the coherence length for the mercury lamp be not present due to excessive noise in data no data be take for unfiltered mercury this would be beneficial a a comparative tool the range of wavelength detect whilst use green and yellow filter on the tungsten and mercury lamp be surpasses what be expect for the respective light range despite this clear cut off frequency be demonstrate by the sharp depreciation in spectral intensity at the edge of the allow range a show in fig 4. far discussion on filter be in present in section 4b the white lead exhibit two peak of wavelength a show in fig 3 with wavelength document in table 1. this be indicative of the lead be make up of a monochromatic light source and fluorescent phosphorous coat the agreement of wavelength within range of error of the blue peak of the white lead and the blue lead imply similar source be use when look at the white lead the light emit be obviously non-uniform imply vary thickness in the phosphorus coat a ground glass diffuser be add between the source and beamsplitter in an attempt to minimize this inconsistency and blend frequency despite this when compare interferograms of the light with and without the diffuser no significant change be see it would be beneficial to reduce the aperture of the incident white light use a pin hole to ensure uniformity the coherence length of colour source be see to be large than for white light in most case this be due to the extensive superposition of multiple wave of different frequency a show by large spectral width for white light source many frequency be superimpose to contribute to the final pulse on the interferogram a the number of contribute frequency to a source tends to infinity the resultant pulse of intensity in the time domain narrow the tungsten lamp produce a black body emission spectrum partially show in fig 5 with a peak at 845 ± 10nm use wien 's displacement law where t be absolute temperature of the source the temperature of the tungsten lamp be found to be 3420 ± 40k this value be outside the range expect 2500 -3300 k however it be important to note that the nature of emission of a blackbody mean wavelength deep into the infrared will be emit and will give rise to intensity peak outside of the detector 's sensitivity so this peak in intensity in the visible zone should be treat with some skepticism 8 the majority of discharge of a mercury lamp be in the ultraviolet zone 10 and thus not picked up by the detector this be due to the de-excitation of electron from quantize energy level cause highly energetic photon emission emission in the visible zone be weak and thus highly susceptible to noise when record this be demonstrate by fig 4a data take for mercury be include however should be treat with caution for this reason the expect presence of a doublet under the cut of frequency be demonstrate by sharp decrease in spectral intensity however be different to those expect from manufacturer specification 9 yellow filter be not distinguishable a more sensitive detector be necessary for accurate analysis of mercury emission for low input velocity of the motorize mirror the motion of the mirror be not constant a the input velocity use be 0.5µs −1 the expect positional change between sample be 0.001µm through analysis of a data set of 43422 measurement the mean positional deviation be found to be 1.02 ± 0.64 × 10 −2 µm the error on this result be the standard deviation in addition to this large standard deviation the stage do not move between reading in 4.46 of case this lack of movement demonstrates the lack of suitability of the motorize mirror extensive analysis of light source the inconsistent motion of the mirror may have result in false peak of spectral intensity the beamsplitter use be coat in an anti-reflection coat state to transmit between 40 and 55 of incident light in the wavelength range 400-700nm below this range percentage transmission decay exponentially however above there be a gradual roughly linear decrease the lack of equality between reflection and transmission of wave will effect interference pattern the resultant effect in the fourier transforms have be ignore however quantify resultant error would be beneficial the spectral sensitivity s of the detector be a function of wavelength this lead to a skewed weight and bias towards longer wavelength in the final wavelength fourier transforms of the interferograms through manual fitting of the manufacturer publish graph of spectral sensitivity against wavelength 7 it be possible to approximate the function to where f w be an inverse quadratic function of wavelength w the technique use to fit the curve be estimate through visual analysis to have an error of ±5 by multiply the fourier transform wavelength spectrum by this function intensity be alter to demonstrate 'true experimental situation this be demonstrate in fig 3 where fig 3a be the spectral intensity prior to correction and fig 3b be the spectral intensity after the large range of wavelength show in fig 4 for both the mercury and tungsten lamp be not wholly indicative of the presence of a yellow filter the filter use be thorlab uv/vis bandpass laser line filter the manufacturer specification state a tolerance of 500 − 590 ± 2nm for yellow light the rejection of wavelength outside this range be state to be 99.9 the state lifetime of the filter be two year due to break down of a component dielectric 9 far information be require on the age of the filter to determine the extent this contributes to error in result v. conclusion a michelson interferometer have be create use standard laboratory equipment various light source have be analyze and equipment suitability ass for this line of investigation whilst high quality interferograms have be create and use to find wavelength coherence length and spectral width of light high error be present limitation in various apparatus affect result and reduce precision the filter use have a great tolerance than expect and the motorize mirror have inconstant motion for the mercury lamp weak emission in the range of visible light result in error too great for exact analysis coherence length have be found to be great for colour light source and the reason understood in the majority of case emission have reflect know result within boundary of error far investigation should be undertaken with more suit equipment to fully as inaccuracy approximation in calculation of wavelength should be far minimize and a wider range of light source and filter analyze\n",
            "Second report:    in this experiment a series of light spectl 3 from various light source be produce with use of a mic.helson intel'feromete1• the light source use we1•e a white lead blue lead and a 2796k tungsten lamp additionally green and yellow filtel 's we1•e use in conjunction with the tungsten lamp to obse1-ve the effect they have on the tungsten spectrum these spectrum be described quantitatively by several value wavelength coherence length and spectral width with use of these value in addition to the genel'31 shape of the spectrum the light source can be characterise and compa1•ed to the theory all spectl 3 produce we1•e of the same shape a their theoretical counterpart and most disc1•epandes could be account for by the 1•esponse function of the detector use the white lead have the b1•oadest spectl•al width of 137.9nm and thus also the large coherence length whic.h be a expect a white light contains all frequency of light the colour light source have narrower spectl•a and the light filter be show to shift and narrow the specn•a in accordance with their expect bandpass region t he aim of this experiment be to measure and characterise light spectrum from various light source through find their mean wavelength coherence length and spectral width and see if these result coincide with the theory the physical aim be to construct and operate a michelson interferometer in order to record interferograms of various light source these interferograms can then be use to obtain the various spectrum of the light source a far goal be to explore how different light filter would affect the light spectnun produce to do this a simple michelson interferometer be built and by emit the light source onto the interferometer and measure the light intensity over a few millimetre in distance an interferogram be produce to produce the spectrum the interferogram be fourier transform and then through fitting a gaussian the characteristic value previously mention can be calculate the michelson interferometer be a collllllon configuration for an optical interferometer and have many u from study the atmosphere measure the diameter of star test optical component and be even use in the detection of gravitational wave pl it relies on the use of a beam splitter which split the incident light from the source into two separate beam and down different path these beam be then reflect and re combine accord to the superposition principal at the beam splitter and an interference pattem be measure at the detector the superposition principle be a fundamental and very important idea with many application in physic and engineering though principle such a this be very well understood and have be for a long period of time it be always important to revisit these idea a it help develop our understand of them interferometry and optic will always be relevant and essential area of physic -not only be the fundamental of these branch apply in everyday life such a in photography and medicine but they be also crucial in the study of quantum mechanic astronomy and more this experiment relies on the use of a michelson interferometer the basis of this interferometer be that the light from the source be emit onto a beam splitter in this case a half-silvered mirrnr which can send the light dm v11 two possible path one of the beam will be reflect by 90 degree and one will be transmit a see in figure 1 below it be important to note that when light hit the beam-splitter and be reflect on the side of the dielectric plate the light will undergo a phase shift of n radian the reflect beam travel along arm a to the first rnirtor m 1 and be reflect back towards the beam splitter likewise the transmit beam travel along aim b to the mirror m2 the two wave then recombine once they reach the beam splitter where their amplitude will combine in accordance with the superposition prir1ciple some of this light will be transmit in the dire ction of the source and some will travel towards the detector sir1ce the light travel to the detector be a supe1position of two wave an interference pattem will be observe at the detector the nature of which be detennir1ed by the path difference and relative phase of the two wave due to the phase shift that the wave that travel along arm b experience if the path difference of the two wave be zero or integer multiple of wavelength i.e the distance of each min-or from the beam splitter be the same the wave be in fact out of phase and have a phase difference of 7c radian this mean the wave will undergo complete destructive interference and a dark fringe will be see meanwhile if there be a path difference of multiple half wavelength they will ultimately be in phase when they reach the detector and constmctive interference will occur so a bright fringe be see the path difference of the two wave be dependent on the difference in arm length for the two 1nitrnrs figure 2 show a simplify diagram of the michelson interferometer assume the 1nittors be perfectly pe1pendicular when observe from the position of the detector see figure 1 the light simply appear to be reflect from two min-ors in line w ith each other with a distance t between them which be equal to the difference in aim length use simple trigonometiy it be easy to see that the light beam reflect off the second miirnr m2 travel an exti•a distance equal to 2tcos 0 before reach the observer a aforementioned constructive interference occurs at half integer wavelength in a michelson interferometer hence for constmctive ii1terference where a. be the wavelength and m be the order of inte1ference which be an integer there be two type of visible fringe that may be observe haidii1ger and fizeau haidinger frii1ges ai•e cfrculai• whereas fizeau fringe fonn sti•aight line 1 2 1. haidinger frii1ges be produce when the two 1nittors appeai• to be perfectly pai•allel a in figure 2 such that the angle at which the light be incident and reflect from the two 1nittors be the same and hence there be cylii1drical symmetiy this symmetly be disturbed when one of the lllllrnrs be tilt and the fringe pattem tends to fonn more of a stl'aight line pattem to understand the spectrn of different light source it be necessaiy to understand how each light source work in this experiment the light source use be a white lead a blue lead and a white tungsten source ffrstly lead create light by electroluminescence electroluminescence be what occurs when an elect1•ic cun-ent or field be pas through a semiconductor material which then emits light the lead contains two layer of material which have be doped such that one layer have an excess of electi•ons and the second an excess of positive electron hole fonning a p-n junction this junction only allows electron to flow from the positive layer to the negative a the electi•on move into the electi•on hole a photon be release because the energy level of the positive hole be low the difference in energy between the layer be call the bandgap and be specific to each combination of material therefore the frequency of light emit and hence the colour of lead be detennined by the material use in the p-n junction and be only over a short range of wavelength to create a white lead you coat a blue lead with phosphor these phosphor absorb the light and ai•e then reemitted at a different large wavelength of a different colour thus by combii1ing a blue lead with a mixture of different phosphor a combination of different wavelength of light be emit produce white light 1 3 1. the process through which a tungsten lamp emits light be know a incandescence the emission of light be a result of a tungsten filament be heat to high temperature when a c1utent pas through it this be thermal radiation and thus a tungsten lamp can be approximate to a blackbody 1 5 1. a theoretical blackbody spectmm can be found usii1g p lanck s law 2hv 3 1 where b be the spectral radiance density h be planck 's constant v be the frequency of radiation c be the speed of light k the boltzmann constant and t the temperatme of the blackbody 1 6 1. the spectral shape of the emission spectrum from these light source be approximately a gaussian the normal distribution of a gaussian have the fom1 u ... fzii where µ be the mean value and u be the standard deviation so through fitting a gaussian to a spectrnm the mean/central wavelength can be found precisely fwthennore the spectral width be equal to the full width at half maximwn fwhm of the gaussian the fwhm and thus the spectral width can be fou11d from the follow equation 21 the spectral width be a measure of the range of frequency or wavelength emit by the light source use this value in conjunction with the mean wavelength the coherence length l of the source can be found l ~2 x 5 where a. be the mean wavelength and til be the spectral width 8 1. the coherence length be the distance over which a light wave remains a specific degree of coherence when the path difference of interfere wave be within the coherence length the interference pattem be strong beyond this the signal becomes poor an interferogram be a record of the signal across an interference pattem a spectnun of light be just a fourier transfo1m of the interferogram go from the temporal domain to the spectral frequency domain due to the fact the spectmm be a gaussian the interferogram will also be in the f01m of a gaussian 2 1. an important feature of this be that the nairnwer the spectral width of the gaussian the wider the interferogram hence the michelson interferometer can be use a a spectrometer a will be discus more in the method ill. me1hod the interferometer be built in agreement with the set up show in figure 1 specifically with a silicon detector the detector be attach to a computer which sainpled the data at a rate of 50hz the first mitrnr ml be place on a motorise stage so that the interference pattem could be scan over a range of path difference the speed at which the stage move to ai1y position could be specify the interferometer must be almost perfectly align to get the con-ect interference pattems to ensw•e this the two miffors must be at the exact same distance from the beam splitter and all equipment must be level and parallel with the optical bench the two 1nitrors be place at approximately the same distance from the beam splitter to guarantee the distance be exactly equal a monochromatic laser be use this produce two laser spot one from ea.eh miffor which be then reflect onto a surface some distance away such that they be small and in do so increase the precision of the alignment the miffors be adjust tmtil the two spot be dfrectly on top of ea.eh other to ensure that the 1nitrors be parallel a lens be insert so that the interference pattem could be see and the mi1rnr be tilt tmtil haidinger fringe be observe before takit1g any data it be necessary to find the null point the null point of the interferometer be when the aim length a1•e equal so the path difference of the two light wave be zero i.e t=o arou11d the null point the it1te1ference pattem of light will be most prominent this be because the space of tbe frit1ges be proportional to the wavelength of light so the fringe will only coincide near the region where the path difference be zero 9 1. once the null point be fotmd these bright and dark fringe can be record to give an interferogram to find the null point ml be move in very slowly in ve1y small increment and the fringe pattem observe refening to equation one a the null point be approach t decrease and thus the angle of the reflect light get large so fringe appear to move in towards the centre of the cfrcle the null point be at the position where the dfrection of movement of the fringe revers once this position be roughly fow1d the reduce rai1ge be scan over use a white lead source at the null point this will produce a large dark frit1ge but just arow1d it a distit1ctive interference pattem of a few different colour fringe be see a the many wavelength withit1 white light have be sepai•ated to find the exact position it be ne cessaiy to use the white lead interferogram create a simulation of this exact setup it be possible to find the shape of the interferogram arntmd the null point so once this shape of it1terferogram have be fotmd it be conclude that the coitect length have be en scan over and the null point lie it1 the position of the centre of the interferogram once the null poit1t be found it be possible to find light spectrum of various light source from thefr interferograms repeat the scan process that be carry out for the white lead interferograms of a blue lead and white tungsten lamp also obtain the interference pattems be scan over small distance at slow speed such that many data point be take to extract the light spectrum from the interferogram a fast fourier transform fft be pe1fo1med on the data within python this rely upon the fact that there be equal space between each data point in the interferogram once the specflum be found it be possible to fit a gaussian to the transfonned data use an inbuilt function on python which give the standard deviation and the value of the mean wavelength to investigate how light filter affected the light spectrum the same process be can-ied out but with two different filter fit inside the tungsten lamp yellow and green firstly from the interferogram produce from tlte white lead show in figure 7 it be possible to find the null point compare this to the simulated interferogram in figure 6 it be of a similar shape and symmetly just with more fringe the cenfl•e of this graph and hence the position of the null point of the interferometer lie at -2.872±0.0005mm a gaussian be fit to each of the peak in the spectrum the first one centi•ed at 450nm coll'esponds to the light from the blue lead and the second one centre at 585nm coll'esponds to the mixture of wavelength emit from the phosphor compare this to the theoretical spectrum in figure 3 the position of the spectrum be accurate however the relative intensity of the peak be unexpected this however be a direct result of the responsivity of the silicon detector this be it effectiveness at produce an electi•ic signal at different wavelength this varies with each detector but in general be high for large wavelength a show in figure 9 when take this into account the peak at 450nm would in fact be of a high intensity than the one at 585run which would match with the tlteory • 1000 1100 fig 9. typical response function of a silicon detector iioj_ when find the spectral length of the white light only the peak at 585nm be consider since tltis contain a large number of wavelength and be more representative of a white light source the standard deviation of this gaussian be 59run so from 4 the specfl•al width be 137.srun use this in 5 in conjw1ction with tlte mean wavelength the value for the coherence length come out a 0.0039mm -by look a t the distance the interferogram span in figure 7 this value seem quite accurate next the interference of a blue lead be measure a this only contains one colour of light it emits light over a small range of frequency so it be expect that the spectral width of this source will be nall'ower than that of the white lead -- -- -- -- -- -- -- -- -- -- this gaussian fit to this spectrum be centre at 4 7 6run which be within the range of wavelength for blue light the shape of this spectrum match that of the blue lead peak show in figure 3. the standard deviation of the gaussian be equal to 13run and thus the spectra.i width be 30.4nm this be much less than that of the white lead which be consistent witlt the theo1y the coherence lengtlt be found to be 0.0050mm again this be more than that of the white lead and be thus consistent with the tlteory a for a nairnwer spectral width the broader the interferogram produce a mention in the theory section the spectrum for the tungsten lamp be more consistent with that of a blackbody than a gaussian it be immediately clear that this do not give the expect result this can be explain through consideration of the response function show in figure 9 -the responsivity of the detector rapidly cut off around the 1000nm mark which con-esponds to where the spectrum cut off so realistically the spectnun do continue past 1000nm just the detector do not respond to the signal secondly from figure 4 you would expect the spectrum to be more jagged -this could simply be due to the fact that not enough data point be measure so the resolution be not good enough to pick up the rise and fall in the spectrum a a gaussian be not fit to the spectrnm the value for wavelength and coherence length must be read off the above graph the spectrum suggests that the mean wavelength be at approximately 800mn and the width of the interferogram and hence the coherence length be approximately 0.005mm by rean-anging 5 the spectral width come at to be 85mn a light filter be a piece of material which absorbs some frequency of light but allows others to pa through essentially passing through just one specific colour the range of frequency it do transmit be it bandwidth in this experiment a yellow and a green filter be use with the tungsten lamp a show in figure 13 when the green filter be use the tungsten spectmm be spectrum be shift such that it be centre at 555nm which be within the range of green light ~520-560nm furthennore it be much na1tower than the original tungsten spectnun with cj 8nm and the spectral width equal to 18.7mn this be what you would expect a the light pas through the filter with few frequency the coherence length be 0.026mm -a much large coherence length for a much nan-ow spectrnm similarly for the yellow filter the spectnun be shift to 590mn with an equal standard deviation and thus spectral width a the green filter yellow light be within the of ~560-5 90mn so this be what you would expect from the theo1y also since the spectral width of this spectrnm be equal to that of the green filter it can be conclude that the size of the bandwidth of the two filter be the same again this give a large value for the coherence length l=0.029mm i 1 000 the main source of e1tor in this experiment come from the fact the fourier transfo1m require that the distance between each sample be equal when in fact when look at the raw data for the position of the stage this be not the case a lot of the time when transfonning to the spectrnl wavelength domain this could lead to a shift in the position of the spectmm potentially make it wider in some part and na1tower in others this would have an effect on the mean value of the wavelength and the spectral width and thus also the coherence length furthennore potential misalignment of the apparatus throughout the experiment could affect the interference pattern and interferogram record which would also adjust the shape of the spectrum produce a silicon detector typically have a tolerance of around 5 so there be some e1tor in the value show for the intensity however since only the relative intensity be of interest and this will have the same effect on all the sample this do not directly affect the result and can be take into account within the response function the aim of this experiment be to use a michelson interferometer a a fourier transform spectrometer in order to obtain and compare spectrum of different light source the interferometer be successful in obtain fairly accurate spectrum for each source and the characteristic of each spectrum i.e mean wavelength and spectral width varied in agreement with the theory the second aspect of this experiment be to show how light filter change the shape of the light spectrum and how the bandwidth of the filter change the spectral width both the blue and yellow filter create a large decrease in the spectral width of the tungsten spectrum and shift the peak accordingly to the wavelength that align with their colour a an improvement more sample should be take over the distance of the interferogram a this will in turn produce a spectrum of great resolution furthermore a response function of the detector should be make which considers before the responsivity and tolerance of the detector such that the spectrum could be adjust to give a more accurate view of the relative intensity to gain a great understand of the error in the calculation some analysis could be carry out to see how unequal space would affect the fft and consequently the spectral shape\n"
          ]
        }
      ],
      "source": [
        "corpus = df.Content.values\n",
        "ArgumentLevels = df.ArgumentLevel.values\n",
        "ReasoningLevels = df.ReasoningLevel.values\n",
        "\n",
        "print('Number of reports: {:,}\\n'.format(len(corpus)))\n",
        "print('First report:    ',corpus[0])\n",
        "print('Second report:   ',corpus[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PmOSw5EXRmPf"
      },
      "source": [
        "# 3. Tokenization & Input Formatting\n",
        "\n",
        "In this section, we'll transform our dataset into the format that BERT can be trained on."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kxDtbEM0H186"
      },
      "source": [
        "## 3.1 Tokenisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 130,
          "referenced_widgets": [
            "3fe9622160bf4b4aa346dfdb786236dd",
            "5104b3f7576b4c95a3fae8d37fd47534",
            "9e699d84c85546f2ba82dff6a2ec3a19",
            "79fb072b5d88410385dcaa5c0fb740f9",
            "c82804405e8e41069403e19c510def6e",
            "e3ab37cd6bf0421a8af668235997c01b",
            "eea4c1878d0c479986b552931a6f7b8b",
            "f9b7ebb484264912a65a4b85d7895338",
            "970c1296a3ca415fb017f4a45031f9ed",
            "f949c3c595d9428f9b8cb56298a571ba",
            "d9a49b3bc8114ca296a76006ff32160c",
            "9f62793bcc7a430b9ceccac72d501012",
            "8f33f55e537841ecaf8db46c0c05e80a",
            "2dd28c97230748c6812f4334deb41e47",
            "9f90c62b0f174472ab95e18d9b8cf8a9",
            "d7b144ebb0df4c91af832295a7509df9",
            "7e9a4c2984954664b3e7bc3cf0661aa9",
            "ea68e35c95e34b459fa92257cb4949fa",
            "a87e716ae8824411866de1929a75eefd",
            "aab2eaf84f1c4588ab9b786d140c51a4",
            "865480437aeb4f53912502d4565c30dd",
            "ced7bedc6a7b44569050c76938d56008",
            "82f28769103b47c0999e476c6894e69c",
            "457659cc5c1d4b399061715e2764f6a1",
            "e5571e309d364b2a843467fa2c74e4f9",
            "7312aaed002c4579b8ab6fe018b70a2d",
            "be0be0d2337e41c29c63dd1bfd898a6d",
            "214d8fa8f39b46d1a7741652adefed8f",
            "6f6c6b44a9ab44a59cd5352d8001d649",
            "a1d1e3fdd83f430884d691e25cd329a5",
            "5e8b6b69e28149df9a3edd507d7ce05e",
            "6a7caf1697dd4ced89210ca3b8fd2b4c",
            "f866772200e04950b7da19f8f5f9b669"
          ]
        },
        "id": "gx5dgBinVS2N",
        "outputId": "2208cea8-af2e-45c6-bbe3-3529cba737a8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "loading BERT tokenizer...\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/232k [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "3fe9622160bf4b4aa346dfdb786236dd"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "9f62793bcc7a430b9ceccac72d501012"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Downloading:   0%|          | 0.00/570 [00:00<?, ?B/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "82f28769103b47c0999e476c6894e69c"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer\n",
        "print('loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfxyOJYug5F1"
      },
      "source": [
        "Now we're ready to perform the real tokenization.\n",
        "\n",
        "The `tokenizer.encode_plus` function combines multiple steps for us:\n",
        "\n",
        "1. Split the sentence into tokens.\n",
        "2. Add the special `[CLS]` and `[SEP]` tokens.\n",
        "3. Map the tokens to their IDs.\n",
        "4. Pad or truncate all sentences to the same length.\n",
        "5. Create the attention masks which explicitly differentiate real tokens from `[PAD]` tokens.\n",
        "\n",
        "The first four features are in `tokenizer.encode`, but I'm using `tokenizer.encode_plus` to get the fifth item (attention masks). Documentation is [here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4M7BBNLMg4op",
        "outputId": "60bc339e-4ed4-4569-dd4b-313aee6c3539"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n",
            "/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2336: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "# Tokenize all of the reports and map the tokens to their word IDs.\n",
        "input_ids, attention_masks, lengths = [], [], []\n",
        "\n",
        "# For every report ...\n",
        "for report in corpus:\n",
        "    # 'encode_plus' will:\n",
        "    #   (1) Tokenise the sentence.\n",
        "    #   (2) Prepend the '[CLS]' token to the start\n",
        "    #   (3) Append the '[SEP]' token to the end\n",
        "    #   (4) Map tokens to their IDs\n",
        "    #   (5) Pad or truncate the report to 'max_length'\n",
        "    #   (6) Create attention masks for [PAD] tokens\n",
        "    encoded_dict = tokenizer.encode_plus(\n",
        "                        report,                     # report to encode\n",
        "                        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n",
        "                        max_length = 512,            # Pad & truncate all reports\n",
        "                        pad_to_max_length = True,\n",
        "                        return_attention_mask = True,   # Construct attn. masks\n",
        "                        return_tensors = 'pt',          # return pytorch tensors\n",
        "\n",
        "    )\n",
        "\n",
        "    # Add the encoded report to the list \n",
        "    input_ids.append(encoded_dict['input_ids'])\n",
        "\n",
        "    # And its attention mask (simply differentiates padding from non-padding)\n",
        "    attention_masks.append(encoded_dict['attention_mask'])\n",
        "\n",
        "    # lengths.append(len(encoded_dict['input_ids']))\n",
        "    lengths.append(len(encoded_dict['input_ids'][0]))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VRmQt6adGKPW"
      },
      "source": [
        "Set which label type to train with; and convert input lists of tensors."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JxSdH_cxGN98"
      },
      "outputs": [],
      "source": [
        "# change input labels here ***\n",
        "labels = ReasoningLevels   \n",
        "num_labels = 4    #4 for ReasoningLevels #5 for ArgumentLevels\n",
        "\n",
        "\n",
        "# Convert the lists into tensors\n",
        "input_ids = torch.cat(input_ids, dim = 0)\n",
        "attention_masks = torch.cat(attention_masks, dim = 0)\n",
        "labels = torch.tensor(labels)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fBNaFKj8Hm4J"
      },
      "outputs": [],
      "source": [
        "# Print report 0, now as a list of IDs\n",
        "# print('Original:', corpus[0])\n",
        "# print('Token IDs:', input_ids[0])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XwBJenjTQSO6"
      },
      "source": [
        "## 3.2 Report Length distribution (Discarded)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TaTbhoegXmiF",
        "outputId": "758e786e-885c-4d66-84a2-f515d8d92525"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Min length: 512 tokens\n",
            "Max length: 512 tokens\n",
            "Median length: 512.0 tokens\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "print('Min length: {:,} tokens'.format(min(lengths)))\n",
        "print('Max length: {:,} tokens'.format(max(lengths)))\n",
        "print('Median length: {:,} tokens'.format(np.median(lengths)))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2EXPYdmOQR7l",
        "outputId": "aee5e636-bbcc-44ca-f585-6b2c34b73dd4"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "Text(10.314999999999998, 0.5, '# of Reports')"
            ]
          },
          "execution_count": 15,
          "metadata": {},
          "output_type": "execute_result"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 360x360 with 1 Axes>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import numpy as np \n",
        "\n",
        "sns.set(style = 'darkgrid')\n",
        "\n",
        "# Increase the plot size and font size\n",
        "sns.set(font_scale = 1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (10,5)\n",
        "\n",
        "# Truncate any report lengths greater than 512\n",
        "lengths = [min(l,512) for l in lengths]\n",
        "\n",
        "# Plot the distribution of comment lengths\n",
        "sns.displot(lengths, kde=False, rug=False)\n",
        "\n",
        "plt.title(\"Report Lengths\")\n",
        "plt.xlabel(\"Report Length\")\n",
        "plt.ylabel(\"# of Reports\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IyD2pukar1QO"
      },
      "source": [
        "How many data (reports) could be faulty? "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3yLVlc7Er8cI",
        "outputId": "79c46ce2-deb4-4a0f-b602-eb0eee349e8f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "# of possible faulty reports:  0\n"
          ]
        }
      ],
      "source": [
        "# Count the number of suspicious reports that didn't not have full text extracted \n",
        "counter = 0\n",
        "for l in lengths:\n",
        "    if l<20:\n",
        "        counter+=1\n",
        "print('# of possible faulty reports: ', counter)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6haaD5vpf-Qy"
      },
      "source": [
        "How many reports run into the 512-token limit?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SKHvKyv6gExI",
        "outputId": "aa226add-2789-4b48-da49-a5e8fbf75cdb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "96 of 96 sentences (100.0%) of corpus are longer than 512 tokens\n"
          ]
        }
      ],
      "source": [
        "# Count the number of sentences that had to be truncated to 512 tokens\n",
        "num_truncated = lengths.count(512)\n",
        "\n",
        "# Compare this to the total number of training reports\n",
        "num_reports = len(lengths)\n",
        "prcnt = float(num_truncated)/float(num_reports)\n",
        "\n",
        "print('{:,} of {:,} sentences ({:.1%}) of corpus are longer than 512 tokens'.format(\n",
        "        num_truncated, num_reports, prcnt))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytev8umirwoF"
      },
      "source": [
        "## 3.3 Training and Validation Split\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000,
          "referenced_widgets": [
            "0a1c18fe9cad4c3e94e89925623768ac",
            "0273e18cd0fc4067941bd14fd4912672",
            "6dad9135e1c74d9fb7ba0e7632f6bb4a",
            "200d806eead24cc8859fe2d46306d414",
            "cc2230712c0641b09bc7cd1d21a75ad8",
            "f43834dd2a9041b9a1682b97cb7b53ba",
            "42594abd60ca4cd99903db92603d433d",
            "69a7706ce17d4cf1817daaeecec01e91",
            "cc20592b4dbf45e99743f2831a333e79",
            "b137db95ace74a2591f64e7f2eb36975",
            "cce7f11c38a34d75ab46e3a128cc0205"
          ]
        },
        "id": "TOUbbujd7coW",
        "outputId": "28c31099-998f-4d7e-ef94-1d01f0a325af"
      },
      "outputs": [
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0a1c18fe9cad4c3e94e89925623768ac",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading:   0%|          | 0.00/440M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.weight', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.dense.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "BertForSequenceClassification(\n",
              "  (bert): BertModel(\n",
              "    (embeddings): BertEmbeddings(\n",
              "      (word_embeddings): Embedding(30522, 768, padding_idx=0)\n",
              "      (position_embeddings): Embedding(512, 768)\n",
              "      (token_type_embeddings): Embedding(2, 768)\n",
              "      (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "      (dropout): Dropout(p=0.1, inplace=False)\n",
              "    )\n",
              "    (encoder): BertEncoder(\n",
              "      (layer): ModuleList(\n",
              "        (0): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (1): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (2): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (3): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (4): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (5): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (6): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (7): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (8): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (9): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (10): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "        (11): BertLayer(\n",
              "          (attention): BertAttention(\n",
              "            (self): BertSelfAttention(\n",
              "              (query): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (key): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (value): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "            (output): BertSelfOutput(\n",
              "              (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "              (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "              (dropout): Dropout(p=0.1, inplace=False)\n",
              "            )\n",
              "          )\n",
              "          (intermediate): BertIntermediate(\n",
              "            (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
              "            (intermediate_act_fn): GELUActivation()\n",
              "          )\n",
              "          (output): BertOutput(\n",
              "            (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
              "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
              "            (dropout): Dropout(p=0.1, inplace=False)\n",
              "          )\n",
              "        )\n",
              "      )\n",
              "    )\n",
              "    (pooler): BertPooler(\n",
              "      (dense): Linear(in_features=768, out_features=768, bias=True)\n",
              "      (activation): Tanh()\n",
              "    )\n",
              "  )\n",
              "  (dropout): Dropout(p=0.1, inplace=False)\n",
              "  (classifier): Linear(in_features=768, out_features=4, bias=True)\n",
              ")"
            ]
          },
          "execution_count": 18,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# Load BertForSequenceClassification, the pre-trained BERT model with a \n",
        "# single linear classification layer on top.\n",
        "\n",
        "from transformers import BertForSequenceClassification, AdamW, BertConfig\n",
        "\n",
        "model = BertForSequenceClassification.from_pretrained(\n",
        "        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab\n",
        "        num_labels = num_labels, # the number of ourput labels -- 5 for five ArgumentLevel classification labels\n",
        "        output_attentions = False, # whether the model returns attention weights\n",
        "        output_hidden_states = False, # whether the model returns all hidden-states\n",
        ")\n",
        "\n",
        "# Tell pytorch to run this model on the GPU\n",
        "model.cuda()\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewM603LvDo02"
      },
      "outputs": [],
      "source": [
        "# Create helper function\n",
        "import numpy as np\n",
        "import random\n",
        "import time\n",
        "import datetime\n",
        "\n",
        "\n",
        "def flat_accuracy(preds, labels):\n",
        "    '''\n",
        "    Function to calculate the accuracy of our predictions vs labels\n",
        "    '''\n",
        "    pred_flat = np.argmax(preds, axis=1).flatten()\n",
        "    labels_flat = labels.flatten()\n",
        "    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n",
        "\n",
        "def format_time(elapsed):\n",
        "    '''\n",
        "    Taks a time in seconds and returns a string hh:mm:ss\n",
        "    '''\n",
        "    # Round to the nearest second\n",
        "    elapsed_rounded = int(round(elapsed))\n",
        "\n",
        "    # Format as hh:mm:ss\n",
        "    return str(datetime.timedelta(seconds = elapsed_rounded))\n",
        "\n",
        "def to_dataframe(dict):\n",
        "    # Display floats with two decimal places.\n",
        "    pd.set_option('precision', 3)\n",
        "    df_stats = pd.DataFrame(data=dict)\n",
        "    df_stats = df_stats.set_index('epoch')\n",
        "    # A hack to force the column headers to wrap.\n",
        "    # df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n",
        "    return df_stats"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1IYjV_u8AqVr"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import random\n",
        "from torch.utils.data import TensorDataset, random_split\n",
        "from tensorflow.python.ops.variables import validate_synchronization_aggregation_trainable\n",
        "from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n",
        "from transformers import get_linear_schedule_with_warmup\n",
        "\n",
        "\n",
        "# define a function for train-validation data splitting\n",
        "def train_val_split(dataset, ratio):\n",
        "    '''\n",
        "    # Create a ratio:(1-ratio) train-validation split\n",
        "    \n",
        "    dataset: tensor object\n",
        "    ratio: float <1 and >0\n",
        "    '''\n",
        "    # Calculate the number of samples to include in each set\n",
        "    train_size = int(ratio * len(dataset))\n",
        "    val_size = len(dataset) - train_size\n",
        "\n",
        "    # Divide the dataset by randomly selecting samples\n",
        "    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
        "\n",
        "    print('{:>5,} training samples'.format(train_size))\n",
        "    print('{:>5,} validation samples'.format(val_size))\n",
        "    \n",
        "    return train_dataset, val_dataset\n",
        "\n",
        "\n",
        "\n",
        "# Create the DataLoaders for our training and validation sets.\n",
        "def data_loader(train_dataset, val_dataset,\n",
        "                batch_size):\n",
        "\n",
        "    # We'll take training samples in random order\n",
        "    train_dataloader = DataLoader(\n",
        "                        train_dataset,                  # the trainig samples\n",
        "                        sampler = RandomSampler(train_dataset), # select batches randomly\n",
        "                        batch_size = batch_size,        # trains with this batch size\n",
        "    )\n",
        "\n",
        "    # For validation the order doesn't matter, so we'll just read them sequentially\n",
        "    validation_dataloader = DataLoader(\n",
        "                        val_dataset,                # the validation samples\n",
        "                        sampler = SequentialSampler(val_dataset), # Pull out batches sequentially\n",
        "                        batch_size = batch_size     # evaluate with this batch size\n",
        "    )\n",
        "    return train_dataloader, validation_dataloader\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "owT4X87kGYLf"
      },
      "outputs": [],
      "source": [
        "def runner(training_ratio):\n",
        "    # settings\n",
        "    r = training_ratio\n",
        "    batch_size = 16 # CUDA out of memory if using 32\n",
        "\n",
        "\n",
        "    # Combine the training inputs into a TensorDataset\n",
        "    dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "    # split data\n",
        "    train_dataset, val_dataset = train_val_split(dataset, r)\n",
        "    # format data\n",
        "    train_dataloader, validation_dataloader = data_loader(train_dataset, \n",
        "                                                        val_dataset,\n",
        "                                                        batch_size)\n",
        "\n",
        "\n",
        "\n",
        "    # ======================================================\n",
        "    # Main - hyperparameters\n",
        "    # ======================================================\n",
        "    # Create optimiser\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5,  # args.learning rate - default is 5e-5\n",
        "                    eps = 1e-8, # args.adam_epsilon - default is 1e-8\n",
        "    )\n",
        "\n",
        "    # Number of training epochs. The BERT authors recommended between 2 and 4. \n",
        "    epochs = 4\n",
        "\n",
        "    # Total number of training steps is [number of batches] x [number of epochs]\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Create the learning rate scheduler. # 学习率预热\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                num_training_steps = total_steps,\n",
        "                                                )\n",
        "\n",
        "\n",
        "\n",
        "    # ======================================================\n",
        "    # MAIN - Training loop\n",
        "    # ======================================================\n",
        "    # Set the seed value all over the place to make this reproducible.\n",
        "    seed_val = 42\n",
        "\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    # We'll store a number of quantities such as training and validation loss, \n",
        "    # validation accuracy, and timings.\n",
        "    training_stats = []\n",
        "\n",
        "    # Measure the total training time for the whole run.\n",
        "    total_t0 = time.time()\n",
        "\n",
        "\n",
        "\n",
        "    # For each epoch...\n",
        "    for epoch_i in range(0, epochs):\n",
        "        \n",
        "        # ========================================\n",
        "        #               Training\n",
        "        # ========================================\n",
        "        \n",
        "        # Perform one full pass over the training set.\n",
        "\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        # Measure how long the training epoch takes.\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_train_loss = 0\n",
        "\n",
        "        # Put the model into training mode. \n",
        "        model.train()\n",
        "\n",
        "\n",
        "\n",
        "        # For each batch of training data... \n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            # Progress update every 40 batches.\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "\n",
        "                # Calculate elapsed time in minutes.\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "                \n",
        "                # Report progress.\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            # Unpack this training batch from our dataloader. \n",
        "            #\n",
        "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "            # `to` method.\n",
        "            #\n",
        "            # `batch` contains three pytorch tensors:\n",
        "            #   [0]: input ids \n",
        "            #   [1]: attention masks\n",
        "            #   [2]: labels \n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            # Always clear any previously calculated gradients before performing a\n",
        "            # backward pass. \n",
        "            model.zero_grad()        \n",
        "\n",
        "            # Perform a forward pass (evaluate the model on this training batch).\n",
        "            # Specifically, we'll get the loss (because we provided labels) and the\n",
        "            # \"logits\"-- the model outputs prior to activation.\n",
        "            result = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels,\n",
        "                        return_dict=True)\n",
        "\n",
        "            loss = result.loss\n",
        "            logits = result.logits\n",
        "\n",
        "            # Accumulate the training loss over all of the batches so that we can\n",
        "            # calculate the average loss at the end. \n",
        "            total_train_loss += loss.item() # Tensor containing a single value\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and take a step using the computed gradient.\n",
        "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "            # modified based on their gradients, the learning rate, etc.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "        \n",
        "        # Measure how long this epoch took.\n",
        "        training_time = format_time(time.time() - t0)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "        # After the completion of each training epoch, measure our performance on\n",
        "        # our validation set.\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Put the model in evaluation mode--the dropout layers behave differently\n",
        "        # during evaluation.\n",
        "        model.eval()\n",
        "\n",
        "        # Tracking variables \n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:\n",
        "            \n",
        "            # Unpack this training batch from our dataloader. \n",
        "            #\n",
        "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "            # the `to` method.\n",
        "            #\n",
        "            # `batch` contains three pytorch tensors:\n",
        "            #   [0]: input ids \n",
        "            #   [1]: attention masks\n",
        "            #   [2]: labels \n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "            \n",
        "            # Tell pytorch not to bother with constructing the compute graph during\n",
        "            # the forward pass, since this is only needed for backprop (training).\n",
        "            with torch.no_grad():        \n",
        "\n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # token_type_ids is the same as the \"segment ids\", which \n",
        "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                result = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels,\n",
        "                            return_dict=True)\n",
        "\n",
        "            # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "            # output values prior to applying an activation function like the \n",
        "            # softmax.\n",
        "            loss = result.loss\n",
        "            logits = result.logits\n",
        "                \n",
        "            # Accumulate the validation loss.\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            # Calculate the accuracy for this batch of test sentences, and\n",
        "            # accumulate it over all batches.\n",
        "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "            \n",
        "\n",
        "        # Report the final accuracy for this validation run.\n",
        "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "        \n",
        "        # Measure how long the validation run took.\n",
        "        validation_time = format_time(time.time() - t0)\n",
        "        \n",
        "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "        print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # ========================================\n",
        "        #               Results\n",
        "        # ========================================\n",
        "        # Record all statistics from this epoch.\n",
        "        training_stats.append(\n",
        "            {\n",
        "                'Run Number': 0,\n",
        "                'Training ratio': r,\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss,\n",
        "                'Valid. Loss': avg_val_loss,\n",
        "                'Valid. Accur.': avg_val_accuracy,\n",
        "                'Training Time': training_time,\n",
        "                'Validation Time': validation_time,\n",
        "            }\n",
        "        )\n",
        "        df = to_dataframe(training_stats)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Training complete!\")\n",
        "\n",
        "        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n",
        "\n",
        "    return df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 488
        },
        "id": "3WtGVDAhQIGp",
        "outputId": "2a37da2b-cd53-4ab1-cebd-ffc70ff034e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   48 training samples\n",
            "   48 validation samples\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-22-8c5adab48428>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrunner\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m<ipython-input-21-33c83fd5e38d>\u001b[0m in \u001b[0;36mrunner\u001b[0;34m(training_ratio)\u001b[0m\n\u001b[1;32m    129\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m             \u001b[0;31m# Perform a backward pass to calculate the gradients.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 131\u001b[0;31m             \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m             \u001b[0;31m# Clip the norm of the gradients to 1.0.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    485\u001b[0m                 \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    486\u001b[0m             )\n\u001b[0;32m--> 487\u001b[0;31m         torch.autograd.backward(\n\u001b[0m\u001b[1;32m    488\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         )\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    195\u001b[0m     \u001b[0;31m# some Python versions print out the first line of a multi-line function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[0;31m# calls in the traceback and some print out the last line\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 197\u001b[0;31m     Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n\u001b[0m\u001b[1;32m    198\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    199\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # Calls into the C++ engine to run the backward pass\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# test\n",
        "# df = runner(0.5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JTd1IZlXHChA",
        "outputId": "3fee486a-0669-4c14-f370-11b74f44761c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "========= Running...=============\n",
            "=== ratio: 0.9, run_num:0===\n",
            "   86 training samples\n",
            "   10 validation samples\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.17\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.50\n",
            "  Validation Loss: 1.13\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:08 (h:mm:ss)\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.06\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.50\n",
            "  Validation Loss: 1.13\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:16 (h:mm:ss)\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.99\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.50\n",
            "  Validation Loss: 1.11\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:24 (h:mm:ss)\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.95\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.50\n",
            "  Validation Loss: 1.10\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:32 (h:mm:ss)\n",
            "========= Running...=============\n",
            "=== ratio: 0.9, run_num:1===\n",
            "   86 training samples\n",
            "   10 validation samples\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.01\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.80\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:08 (h:mm:ss)\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.91\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.78\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:16 (h:mm:ss)\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.85\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.76\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:24 (h:mm:ss)\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.82\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.76\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:32 (h:mm:ss)\n",
            "========= Running...=============\n",
            "=== ratio: 0.9, run_num:2===\n",
            "   86 training samples\n",
            "   10 validation samples\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.74\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.74\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:08 (h:mm:ss)\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.70\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 0.79\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:16 (h:mm:ss)\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.64\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:24 (h:mm:ss)\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.60\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.68\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:32 (h:mm:ss)\n",
            "========= Running...=============\n",
            "=== ratio: 0.9, run_num:3===\n",
            "   86 training samples\n",
            "   10 validation samples\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 0.96\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:08 (h:mm:ss)\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.61\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:16 (h:mm:ss)\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.49\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.71\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:24 (h:mm:ss)\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.52\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:32 (h:mm:ss)\n",
            "========= Running...=============\n",
            "=== ratio: 0.9, run_num:4===\n",
            "   86 training samples\n",
            "   10 validation samples\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.39\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 0.96\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:08 (h:mm:ss)\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.47\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.63\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:16 (h:mm:ss)\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.61\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:24 (h:mm:ss)\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.38\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.65\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:32 (h:mm:ss)\n",
            "========= Running...=============\n",
            "=== ratio: 0.9, run_num:5===\n",
            "   86 training samples\n",
            "   10 validation samples\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.71\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:08 (h:mm:ss)\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:16 (h:mm:ss)\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:24 (h:mm:ss)\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.63\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:32 (h:mm:ss)\n",
            "========= Running...=============\n",
            "=== ratio: 0.9, run_num:6===\n",
            "   86 training samples\n",
            "   10 validation samples\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 0.80\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:08 (h:mm:ss)\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.84\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:16 (h:mm:ss)\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.17\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.64\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:24 (h:mm:ss)\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.18\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.67\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:32 (h:mm:ss)\n",
            "========= Running...=============\n",
            "=== ratio: 0.9, run_num:7===\n",
            "   86 training samples\n",
            "   10 validation samples\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.10\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.76\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:08 (h:mm:ss)\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.11\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.77\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:16 (h:mm:ss)\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.83\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:24 (h:mm:ss)\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.09\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.87\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:32 (h:mm:ss)\n",
            "========= Running...=============\n",
            "=== ratio: 0.9, run_num:8===\n",
            "   86 training samples\n",
            "   10 validation samples\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.79\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:08 (h:mm:ss)\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.98\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:16 (h:mm:ss)\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.04\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.90\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:24 (h:mm:ss)\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.05\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.91\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:32 (h:mm:ss)\n",
            "========= Running...=============\n",
            "=== ratio: 0.9, run_num:9===\n",
            "   86 training samples\n",
            "   10 validation samples\n",
            "\n",
            "======== Epoch 1 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.03\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.03\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:08 (h:mm:ss)\n",
            "\n",
            "======== Epoch 2 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.07\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:16 (h:mm:ss)\n",
            "\n",
            "======== Epoch 3 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.00\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:24 (h:mm:ss)\n",
            "\n",
            "======== Epoch 4 / 4 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.02\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 1.07\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:00:32 (h:mm:ss)\n",
            "       Run Number  Training ratio  Training Loss  Valid. Loss  Valid. Accur.  \\\n",
            "epoch                                                                          \n",
            "1               9             0.9          0.030        1.030            0.6   \n",
            "2               9             0.9          0.024        1.074            0.6   \n",
            "3               9             0.9          0.022        0.996            0.6   \n",
            "4               9             0.9          0.024        1.075            0.6   \n",
            "1               8             0.9          0.054        0.791            0.8   \n",
            "2               8             0.9          0.053        0.976            0.7   \n",
            "3               8             0.9          0.043        0.899            0.8   \n",
            "4               8             0.9          0.050        0.911            0.7   \n",
            "1               7             0.9          0.101        0.761            0.7   \n",
            "2               7             0.9          0.108        0.774            0.8   \n",
            "3               7             0.9          0.092        0.833            0.7   \n",
            "4               7             0.9          0.089        0.870            0.7   \n",
            "1               6             0.9          0.199        0.798            0.6   \n",
            "2               6             0.9          0.187        0.836            0.7   \n",
            "3               6             0.9          0.173        0.640            0.7   \n",
            "4               6             0.9          0.178        0.670            0.7   \n",
            "1               5             0.9          0.276        0.712            0.7   \n",
            "2               5             0.9          0.315        0.686            0.8   \n",
            "3               5             0.9          0.267        0.693            0.7   \n",
            "4               5             0.9          0.272        0.632            0.7   \n",
            "1               4             0.9          0.385        0.956            0.6   \n",
            "2               4             0.9          0.473        0.633            0.8   \n",
            "3               4             0.9          0.372        0.606            0.8   \n",
            "4               4             0.9          0.381        0.651            0.8   \n",
            "1               3             0.9          0.496        0.964            0.6   \n",
            "2               3             0.9          0.609        0.690            0.8   \n",
            "3               3             0.9          0.491        0.705            0.8   \n",
            "4               3             0.9          0.519        0.717            0.8   \n",
            "1               2             0.9          0.743        0.738            0.8   \n",
            "2               2             0.9          0.696        0.789            0.6   \n",
            "3               2             0.9          0.643        0.684            0.8   \n",
            "4               2             0.9          0.602        0.683            0.8   \n",
            "1               1             0.9          1.008        0.797            0.8   \n",
            "2               1             0.9          0.912        0.778            0.7   \n",
            "3               1             0.9          0.848        0.763            0.8   \n",
            "4               1             0.9          0.819        0.756            0.7   \n",
            "1               0             0.9          1.168        1.128            0.5   \n",
            "2               0             0.9          1.061        1.125            0.5   \n",
            "3               0             0.9          0.993        1.108            0.5   \n",
            "4               0             0.9          0.955        1.095            0.5   \n",
            "\n",
            "      Training Time Validation Time  \n",
            "epoch                                \n",
            "1           0:00:08         0:00:00  \n",
            "2           0:00:08         0:00:00  \n",
            "3           0:00:08         0:00:00  \n",
            "4           0:00:08         0:00:00  \n",
            "1           0:00:08         0:00:00  \n",
            "2           0:00:08         0:00:00  \n",
            "3           0:00:08         0:00:00  \n",
            "4           0:00:08         0:00:00  \n",
            "1           0:00:08         0:00:00  \n",
            "2           0:00:08         0:00:00  \n",
            "3           0:00:08         0:00:00  \n",
            "4           0:00:08         0:00:00  \n",
            "1           0:00:08         0:00:00  \n",
            "2           0:00:08         0:00:00  \n",
            "3           0:00:08         0:00:00  \n",
            "4           0:00:08         0:00:00  \n",
            "1           0:00:08         0:00:00  \n",
            "2           0:00:08         0:00:00  \n",
            "3           0:00:08         0:00:00  \n",
            "4           0:00:08         0:00:00  \n",
            "1           0:00:08         0:00:00  \n",
            "2           0:00:08         0:00:00  \n",
            "3           0:00:08         0:00:00  \n",
            "4           0:00:08         0:00:00  \n",
            "1           0:00:08         0:00:00  \n",
            "2           0:00:08         0:00:00  \n",
            "3           0:00:08         0:00:00  \n",
            "4           0:00:08         0:00:00  \n",
            "1           0:00:08         0:00:00  \n",
            "2           0:00:08         0:00:00  \n",
            "3           0:00:08         0:00:00  \n",
            "4           0:00:08         0:00:00  \n",
            "1           0:00:08         0:00:00  \n",
            "2           0:00:08         0:00:00  \n",
            "3           0:00:08         0:00:00  \n",
            "4           0:00:08         0:00:00  \n",
            "1           0:00:08         0:00:00  \n",
            "2           0:00:08         0:00:00  \n",
            "3           0:00:08         0:00:00  \n",
            "4           0:00:08         0:00:00  \n"
          ]
        }
      ],
      "source": [
        "# set the number of loops \n",
        "run_num = 10\n",
        "# set training data ratio\n",
        "ratios = [0.9]\n",
        "for r in ratios:\n",
        "\n",
        "    # create a dataframe to saveto\n",
        "    final_df = pd.DataFrame()\n",
        "    \n",
        "    for counter in range(run_num):\n",
        "        print('========= Running...=============')\n",
        "        print('=== ratio: '+str(r)+', run_num:'+str(counter)+'===')\n",
        "\n",
        "        df = runner(r)\n",
        "        # count 'Run Number'\n",
        "        df['Run Number'] = counter\n",
        "        # append to the final dataframe\n",
        "        final_df = pd.concat([df, final_df])\n",
        "        \n",
        "    # save dataframe\n",
        "    print(final_df) \n",
        "    filepath = './Pickledfiles/bert_reasoninglevel/'+str(r)+'trainratio_'+str(run_num)+'runs_'+'4epochs.pkl'\n",
        "    final_df.to_pickle(filepath)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "X1ztpnamU7qp",
        "outputId": "680b001c-8864-49e4-b6ca-dc2a65ba4cb1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "       Run Number  Training ratio  Training Loss  Valid. Loss  Valid. Accur.  \\\n",
            "epoch                                                                          \n",
            "1               0             0.5      5.948e-05    2.286e-05            1.0   \n",
            "2               0             0.5      3.409e-05    2.309e-05            1.0   \n",
            "\n",
            "      Training Time Validation Time  \n",
            "epoch                                \n",
            "1           0:00:04         0:00:02  \n",
            "2           0:00:04         0:00:02  \n"
          ]
        }
      ],
      "source": [
        "df = pd.read_pickle(r'./Pickledfiles/bert_reasoninglevel/0.5trainratio_1runs.pkl')\n",
        "print(df)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NirHP_B-Wu8B"
      },
      "source": [
        "# OLD CODE"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "H1b87vXVDWZu",
        "outputId": "6bf54dd5-667d-492e-8334-e09752c25f34"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   48 training samples\n",
            "   48 validation samples\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "  Average training loss: 1.19\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.62\n",
            "  Validation Loss: 1.01\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.11\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.62\n",
            "  Validation Loss: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 1.07\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 0.99\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.97\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.58\n",
            "  Validation Loss: 1.03\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.92\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.60\n",
            "  Validation Loss: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.86\n",
            "  Training epcoh took: 0:00:04\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.52\n",
            "  Validation Loss: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.81\n",
            "  Training epcoh took: 0:00:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.52\n",
            "  Validation Loss: 1.00\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.77\n",
            "  Training epcoh took: 0:00:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.52\n",
            "  Validation Loss: 0.99\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.77\n",
            "  Training epcoh took: 0:00:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.52\n",
            "  Validation Loss: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.73\n",
            "  Training epcoh took: 0:00:05\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.52\n",
            "  Validation Loss: 0.98\n",
            "  Validation took: 0:00:02\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:05 (h:mm:ss)\n",
            "   57 training samples\n",
            "   39 validation samples\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.88\n",
            "  Training epcoh took: 0:00:06\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.75\n",
            "  Validation Loss: 0.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.79\n",
            "  Training epcoh took: 0:00:06\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.68\n",
            "  Validation Loss: 0.84\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.74\n",
            "  Training epcoh took: 0:00:06\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.74\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.69\n",
            "  Training epcoh took: 0:00:06\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.77\n",
            "  Validation Loss: 0.72\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.60\n",
            "  Training epcoh took: 0:00:06\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 0.78\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epcoh took: 0:00:06\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.71\n",
            "  Validation Loss: 0.82\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.51\n",
            "  Training epcoh took: 0:00:06\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.75\n",
            "  Validation Loss: 0.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epcoh took: 0:00:06\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.75\n",
            "  Validation Loss: 0.71\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.48\n",
            "  Training epcoh took: 0:00:06\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.75\n",
            "  Validation Loss: 0.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.44\n",
            "  Training epcoh took: 0:00:06\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.73\n",
            "  Validation Loss: 0.74\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:11 (h:mm:ss)\n",
            "   67 training samples\n",
            "   29 validation samples\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.62\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.41\n",
            "  Validation Loss: 1.23\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.55\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.84\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.50\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.92\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.40\n",
            "  Validation Loss: 1.21\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.33\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 0.96\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.28\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.66\n",
            "  Validation Loss: 0.94\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.27\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 1.05\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.37\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 1.08\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 1.05\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.65\n",
            "  Validation Loss: 1.05\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:17 (h:mm:ss)\n",
            "   76 training samples\n",
            "   20 validation samples\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.41\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.84\n",
            "  Validation Loss: 0.61\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.78\n",
            "  Validation Loss: 0.70\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.23\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.63\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.21\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.66\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.18\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.72\n",
            "  Validation Loss: 0.83\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.16\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.70\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.16\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.69\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.14\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.73\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.75\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:00:07\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.81\n",
            "  Validation Loss: 0.74\n",
            "  Validation took: 0:00:01\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:22 (h:mm:ss)\n",
            "   86 training samples\n",
            "   10 validation samples\n",
            "\n",
            "======== Epoch 1 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.32\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.20\n",
            "  Validation Loss: 1.79\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 2 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.31\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.55\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 3 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.61\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 4 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.19\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.71\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 5 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.20\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.65\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 6 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.16\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.90\n",
            "  Validation Loss: 0.66\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 7 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.81\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 8 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.11\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.70\n",
            "  Validation Loss: 0.87\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 9 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.13\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.84\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "======== Epoch 10 / 10 ========\n",
            "Training...\n",
            "\n",
            "  Average training loss: 0.10\n",
            "  Training epcoh took: 0:00:08\n",
            "\n",
            "Running Validation...\n",
            "  Accuracy: 0.80\n",
            "  Validation Loss: 0.80\n",
            "  Validation took: 0:00:00\n",
            "\n",
            "Training complete!\n",
            "Total training took 0:01:28 (h:mm:ss)\n"
          ]
        }
      ],
      "source": [
        "# ======================================================\n",
        "# MAIN\n",
        "# ======================================================\n",
        "# Combine the training inputs into a TensorDataset\n",
        "dataset = TensorDataset(input_ids, attention_masks, labels)\n",
        "ratios = [0.5, 0.6, 0.7, 0.8, 0.9]\n",
        "\n",
        "\n",
        "# loop training for each training-test ratios\n",
        "for r in ratios:\n",
        "\n",
        "    batch_size = 16 # CUDA out of memory if using 32\n",
        "    \n",
        "    # split data\n",
        "    train_dataset, val_dataset = train_val_split(dataset, r)\n",
        "    # format data\n",
        "    train_dataloader, validation_dataloader = data_loader(train_dataset, \n",
        "                                                          val_dataset,\n",
        "                                                          batch_size)\n",
        "\n",
        "\n",
        "\n",
        "    # ======================================================\n",
        "    # Main - hyperparameters\n",
        "    # ======================================================\n",
        "    # Create optimiser\n",
        "    optimizer = AdamW(model.parameters(),\n",
        "                    lr = 2e-5,  # args.learning rate - default is 5e-5\n",
        "                    eps = 1e-8, # args.adam_epsilon - default is 1e-8\n",
        "    )\n",
        "\n",
        "    # Number of training epochs. The BERT authors recommended between 2 and 4. \n",
        "    epochs = 2\n",
        "  \n",
        "    # Total number of training steps is [number of batches] x [number of epochs]\n",
        "    total_steps = len(train_dataloader) * epochs\n",
        "\n",
        "    # Create the learning rate scheduler. # 学习率预热\n",
        "    scheduler = get_linear_schedule_with_warmup(optimizer,\n",
        "                                                num_warmup_steps = 0, # Default value in run_glue.py\n",
        "                                                num_training_steps = total_steps,\n",
        "                                                )\n",
        "\n",
        "\n",
        "\n",
        "    # ======================================================\n",
        "    # MAIN - Training loop\n",
        "    # ======================================================\n",
        "    # Set the seed value all over the place to make this reproducible.\n",
        "    seed_val = 42\n",
        "\n",
        "    random.seed(seed_val)\n",
        "    np.random.seed(seed_val)\n",
        "    torch.manual_seed(seed_val)\n",
        "    torch.cuda.manual_seed_all(seed_val)\n",
        "\n",
        "    # We'll store a number of quantities such as training and validation loss, \n",
        "    # validation accuracy, and timings.\n",
        "    training_stats = []\n",
        "\n",
        "    # Measure the total training time for the whole run.\n",
        "    total_t0 = time.time()\n",
        "\n",
        "\n",
        "\n",
        "    # For each epoch...\n",
        "    for epoch_i in range(0, epochs):\n",
        "        \n",
        "        # ========================================\n",
        "        #               Training\n",
        "        # ========================================\n",
        "        \n",
        "        # Perform one full pass over the training set.\n",
        "\n",
        "        print(\"\")\n",
        "        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n",
        "        print('Training...')\n",
        "\n",
        "        # Measure how long the training epoch takes.\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Reset the total loss for this epoch.\n",
        "        total_train_loss = 0\n",
        "\n",
        "        # Put the model into training mode. \n",
        "        model.train()\n",
        "\n",
        "\n",
        "\n",
        "        # For each batch of training data... \n",
        "        for step, batch in enumerate(train_dataloader):\n",
        "\n",
        "            # Progress update every 40 batches.\n",
        "            if step % 40 == 0 and not step == 0:\n",
        "\n",
        "                # Calculate elapsed time in minutes.\n",
        "                elapsed = format_time(time.time() - t0)\n",
        "                \n",
        "                # Report progress.\n",
        "                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n",
        "\n",
        "            # Unpack this training batch from our dataloader. \n",
        "            #\n",
        "            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n",
        "            # `to` method.\n",
        "            #\n",
        "            # `batch` contains three pytorch tensors:\n",
        "            #   [0]: input ids \n",
        "            #   [1]: attention masks\n",
        "            #   [2]: labels \n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "\n",
        "            # Always clear any previously calculated gradients before performing a\n",
        "            # backward pass. \n",
        "            model.zero_grad()        \n",
        "\n",
        "            # Perform a forward pass (evaluate the model on this training batch).\n",
        "            # Specifically, we'll get the loss (because we provided labels) and the\n",
        "            # \"logits\"-- the model outputs prior to activation.\n",
        "            result = model(b_input_ids, \n",
        "                        token_type_ids=None, \n",
        "                        attention_mask=b_input_mask, \n",
        "                        labels=b_labels,\n",
        "                        return_dict=True)\n",
        "\n",
        "            loss = result.loss\n",
        "            logits = result.logits\n",
        "\n",
        "            # Accumulate the training loss over all of the batches so that we can\n",
        "            # calculate the average loss at the end. \n",
        "            total_train_loss += loss.item() # Tensor containing a single value\n",
        "\n",
        "            # Perform a backward pass to calculate the gradients.\n",
        "            loss.backward()\n",
        "\n",
        "            # Clip the norm of the gradients to 1.0.\n",
        "            # This is to help prevent the \"exploding gradients\" problem.\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "\n",
        "            # Update parameters and take a step using the computed gradient.\n",
        "            # The optimizer dictates the \"update rule\"--how the parameters are\n",
        "            # modified based on their gradients, the learning rate, etc.\n",
        "            optimizer.step()\n",
        "\n",
        "            # Update the learning rate\n",
        "            scheduler.step()\n",
        "\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_train_loss = total_train_loss / len(train_dataloader)            \n",
        "        \n",
        "        # Measure how long this epoch took.\n",
        "        training_time = format_time(time.time() - t0)\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n",
        "        print(\"  Training epcoh took: {:}\".format(training_time))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # ========================================\n",
        "        #               Validation\n",
        "        # ========================================\n",
        "        # After the completion of each training epoch, measure our performance on\n",
        "        # our validation set.\n",
        "\n",
        "        print(\"\")\n",
        "        print(\"Running Validation...\")\n",
        "\n",
        "        t0 = time.time()\n",
        "\n",
        "        # Put the model in evaluation mode--the dropout layers behave differently\n",
        "        # during evaluation.\n",
        "        model.eval()\n",
        "\n",
        "        # Tracking variables \n",
        "        total_eval_accuracy = 0\n",
        "        total_eval_loss = 0\n",
        "        nb_eval_steps = 0\n",
        "\n",
        "        # Evaluate data for one epoch\n",
        "        for batch in validation_dataloader:\n",
        "            \n",
        "            # Unpack this training batch from our dataloader. \n",
        "            #\n",
        "            # As we unpack the batch, we'll also copy each tensor to the GPU using \n",
        "            # the `to` method.\n",
        "            #\n",
        "            # `batch` contains three pytorch tensors:\n",
        "            #   [0]: input ids \n",
        "            #   [1]: attention masks\n",
        "            #   [2]: labels \n",
        "            b_input_ids = batch[0].to(device)\n",
        "            b_input_mask = batch[1].to(device)\n",
        "            b_labels = batch[2].to(device)\n",
        "            \n",
        "            # Tell pytorch not to bother with constructing the compute graph during\n",
        "            # the forward pass, since this is only needed for backprop (training).\n",
        "            with torch.no_grad():        \n",
        "\n",
        "                # Forward pass, calculate logit predictions.\n",
        "                # token_type_ids is the same as the \"segment ids\", which \n",
        "                # differentiates sentence 1 and 2 in 2-sentence tasks.\n",
        "                result = model(b_input_ids, \n",
        "                            token_type_ids=None, \n",
        "                            attention_mask=b_input_mask,\n",
        "                            labels=b_labels,\n",
        "                            return_dict=True)\n",
        "\n",
        "            # Get the loss and \"logits\" output by the model. The \"logits\" are the \n",
        "            # output values prior to applying an activation function like the \n",
        "            # softmax.\n",
        "            loss = result.loss\n",
        "            logits = result.logits\n",
        "                \n",
        "            # Accumulate the validation loss.\n",
        "            total_eval_loss += loss.item()\n",
        "\n",
        "            # Move logits and labels to CPU\n",
        "            logits = logits.detach().cpu().numpy()\n",
        "            label_ids = b_labels.to('cpu').numpy()\n",
        "\n",
        "            # Calculate the accuracy for this batch of test sentences, and\n",
        "            # accumulate it over all batches.\n",
        "            total_eval_accuracy += flat_accuracy(logits, label_ids)\n",
        "            \n",
        "\n",
        "        # Report the final accuracy for this validation run.\n",
        "        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n",
        "        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n",
        "\n",
        "        # Calculate the average loss over all of the batches.\n",
        "        avg_val_loss = total_eval_loss / len(validation_dataloader)\n",
        "        \n",
        "        # Measure how long the validation run took.\n",
        "        validation_time = format_time(time.time() - t0)\n",
        "        \n",
        "        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n",
        "        print(\"  Validation took: {:}\".format(validation_time))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "        # Record all statistics from this epoch.\n",
        "        training_stats.append(\n",
        "            {\n",
        "                'epoch': epoch_i + 1,\n",
        "                'Training Loss': avg_train_loss,\n",
        "                'Valid. Loss': avg_val_loss,\n",
        "                'Valid. Accur.': avg_val_accuracy,\n",
        "                'Training Time': training_time,\n",
        "                'Validation Time': validation_time\n",
        "            }\n",
        "        )\n",
        "\n",
        "        # save dataframe\n",
        "        df = to_dataframe(training_stats)\n",
        "        filepath = 'Pickledfiles/bert_reasoninglevel_trainratio'+str(r)+'_epochs'+str(epochs)+'_seedval42'+'_batchsize16'\n",
        "        df.to_pickle(filepath)\n",
        "\n",
        "    print(\"\")\n",
        "    print(\"Training complete!\")\n",
        "\n",
        "    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-arhBqEY4I-r"
      },
      "source": [
        "\n",
        "4.3 Training loop\n",
        "\n",
        "Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. \n",
        "\n",
        "> *Thank you to [Stas Bekman](https://ca.linkedin.com/in/stasbekman) for contributing the insights and code for using validation loss to detect over-fitting!*\n",
        "\n",
        "Training: \n",
        "* Unpack our data inputs and labels \n",
        "* Load data onto GPU for acceleration\n",
        "* Clear out the gradients calculated in the previous pass\n",
        "    * In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out\n",
        "* Forward pass (feed input data through the network)\n",
        "* Backward pass (backpropagation)\n",
        "* Tell the network to update parameters with optimizer.step()\n",
        "* Track variables for minitoring progress\n",
        "\n",
        "Evaluation:\n",
        "* Unpack our data inputs and labels\n",
        "* Load data onto the GPU for acceleration\n",
        "* Forward pass (feed input data through the network)\n",
        "* Compute loss on our validation data and track variables for monitoring process\n",
        "\n",
        "\n",
        "Pytorch hides all the detailed calculations from us, but we've commented the code to print out which of the above steps are happening on each line.\n",
        "\n",
        "> *PyTorch also has some [beginner tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) which you may also find helpful.*"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-eJilZ9_1W7A"
      },
      "source": [
        "Let's view the summary of the training process."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 450
        },
        "id": "ZJgACViioLT1",
        "outputId": "972986e9-cec9-4941-cdb4-236abdb0f892"
      },
      "outputs": [
        {
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "output_type": "error",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-21-32c8bb6fecb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mratios\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# path = 'Pickledfiles/bert_reasoninglevel_trainratio'+str(i)+'_epochs1_seedval42_batchsize16.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert_reasoninglevel_trainratio0.5_epochs1_seedval42_batchsize16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvalidacc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Valid. Accur.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \"\"\"\n\u001b[1;32m    195\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bert_reasoninglevel_trainratio0.5_epochs1_seedval42_batchsize16'"
          ]
        }
      ],
      "source": [
        "import pandas as pd\n",
        "ratios = [0.5,0.6,0.7,0.8,0.9]\n",
        "validacc_list = []\n",
        "\n",
        "for i in ratios:\n",
        "    # path = 'Pickledfiles/bert_reasoninglevel_trainratio'+str(i)+'_epochs1_seedval42_batchsize16.pkl'\n",
        "    df = pd.read_pickle('bert_reasoninglevel_trainratio0.5_epochs1_seedval42_batchsize16')\n",
        "    validacc_list.append(df.iloc[0]['Valid. Accur.'])\n",
        "    print(df)\n",
        "\n",
        "print(validacc_list)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a1gSwd0WwLTf"
      },
      "outputs": [],
      "source": [
        "pip install git+https://github.com/garrettj403/SciencePlots"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WtqOdDP5vt4N"
      },
      "outputs": [],
      "source": [
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rBwtofW32kdt"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "# Use plot styling from seaborn.\n",
        "sns.set(style='darkgrid')\n",
        "\n",
        "# Increase the plot size and font size.\n",
        "sns.set(font_scale=1.5)\n",
        "plt.rcParams[\"figure.figsize\"] = (12,6)\n",
        "\n",
        "# Plot the learning curve.\n",
        "plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n",
        "plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n",
        "\n",
        "# Label the plot.\n",
        "plt.title(\"Training & Validation Loss\")\n",
        "plt.xlabel(\"Epoch\")\n",
        "plt.ylabel(\"Loss\")\n",
        "plt.legend()\n",
        "plt.xticks([1, 2, 3, 4])\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsdFtpZ94gPu"
      },
      "source": [
        "5. Performance on Test Set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rU_BBWgy4lK_"
      },
      "source": [
        "5.1 Data Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PRXxjlsM5RZ2"
      },
      "outputs": [],
      "source": [
        "# Create the DataLoader.\n",
        "prediction_data = val_dataset\n",
        "prediction_sampler = SequentialSampler(prediction_data)\n",
        "prediction_dataloader = validation_dataloader"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DHFs1AkB434s"
      },
      "source": [
        "5.2 Evaluate on Test Set"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KMdiBA9422k"
      },
      "outputs": [],
      "source": [
        "# Prediction on test set\n",
        "\n",
        "print('Predicting labels for {:,} test reports...'.format(len(val_dataset)))\n",
        "\n",
        "# Put model in evaluation mode\n",
        "model.eval()\n",
        "\n",
        "# Tracking variables \n",
        "predictions , true_labels = [], []\n",
        "\n",
        "# Predict \n",
        "for batch in prediction_dataloader:\n",
        "  # Add batch to GPU\n",
        "  batch = tuple(t.to(device) for t in batch)\n",
        "  \n",
        "  # Unpack the inputs from our dataloader\n",
        "  b_input_ids, b_input_mask, b_labels = batch\n",
        "  \n",
        "  # Telling the model not to compute or store gradients, saving memory and \n",
        "  # speeding up prediction\n",
        "  with torch.no_grad():\n",
        "      # Forward pass, calculate logit predictions.\n",
        "      result = model(b_input_ids, \n",
        "                     token_type_ids=None, \n",
        "                     attention_mask=b_input_mask,\n",
        "                     return_dict=True)\n",
        "\n",
        "  logits = result.logits\n",
        "\n",
        "  # Move logits and labels to CPU\n",
        "  logits = logits.detach().cpu().numpy()\n",
        "  label_ids = b_labels.to('cpu').numpy()\n",
        "  \n",
        "  # Store predictions and true labels\n",
        "  predictions.append(logits)\n",
        "  true_labels.append(label_ids)\n",
        "\n",
        "print('    DONE.')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SXlId81P6Kh2"
      },
      "source": [
        "Accuracy on the CoLA benchmark is measured using the \"[Matthews correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)\" (MCC).\n",
        "\n",
        "We use MCC here because the classes are imbalanced:\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jJD1D-g86NUs"
      },
      "outputs": [],
      "source": [
        "from sklearn.metrics import matthews_corrcoef\n",
        "\n",
        "matthews_set = []\n",
        "\n",
        "# Evaluate each test batch using Matthew's correlation coefficient\n",
        "print('Calculating Matthews Corr. Coef. for each batch...')\n",
        "\n",
        "# For each input batch...\n",
        "for i in range(len(true_labels)):\n",
        "  \n",
        "  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n",
        "  # and one column for \"1\"). Pick the label with the highest value and turn this\n",
        "  # in to a list of 0s and 1s.\n",
        "  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n",
        "  \n",
        "  # Calculate and store the coef for this batch.  \n",
        "  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n",
        "  matthews_set.append(matthews)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kx7kqXn6D6VY"
      },
      "source": [
        "The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches. \n",
        "\n",
        "Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "poxIZ-UiD-GZ"
      },
      "outputs": [],
      "source": [
        "# Create a barplot showing the MCC score for each batch of test samples.\n",
        "ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n",
        "\n",
        "plt.title('MCC Score per Batch')\n",
        "plt.ylabel('MCC Score (-1 to +1)')\n",
        "plt.xlabel('Batch #')\n",
        "\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Nyuwg22cEFWy"
      },
      "outputs": [],
      "source": [
        "# Combine the results across all batches. \n",
        "flat_predictions = np.concatenate(predictions, axis=0)\n",
        "\n",
        "# For each sample, pick the label (0 or 1) with the higher score.\n",
        "flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n",
        "\n",
        "# Combine the correct labels for each batch into a single list.\n",
        "flat_true_labels = np.concatenate(true_labels, axis=0)\n",
        "\n",
        "# Calculate the MCC\n",
        "mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n",
        "\n",
        "print('Total MCC: %.3f' % mcc)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dJOEigZaddtC"
      },
      "source": [
        "When we actually convert all of our sentences, we'll use the `tokenize.encode` function to handle both steps, rather than calling `tokenize` and `convert_tokens_to_ids` separately. \n",
        "\n",
        "Before we can do that, though, we need to talk about some of BERT's formatting requirements."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aGYv9paXSDm9"
      },
      "outputs": [],
      "source": [
        "# Print the original report.\n",
        "print('length:',len(corpus[0]),';  Original: ', corpus[0])\n",
        "\n",
        "# Print the report split into tokens.\n",
        "print('length:',len(tokenizer.tokenize(corpus[0])),';  Tokenized: ', tokenizer.tokenize(corpus[0]))\n",
        "\n",
        "# Print the report mapped to token ids.\n",
        "print('length:',len(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(corpus[0]))) , ';  Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(corpus[0])))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RgPW9ujVSFLv"
      },
      "source": [
        "Let's apply the tokenizer to one report just to see the output."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hrik8ScLRlkd"
      },
      "outputs": [],
      "source": [
        "from transformers import BertTokenizer\n",
        "\n",
        "# Load the BERT tokenizer.\n",
        "print('Loading BERT tokenizer...')\n",
        "tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "XwBJenjTQSO6",
        "NirHP_B-Wu8B"
      ],
      "provenance": [],
      "mount_file_id": "1zl0AkYFg34_0Z5JQHg4nSJD93v_ugfuZ",
      "authorship_tag": "ABX9TyNuukhYy9+Iaq/vtK5sSA46"
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.13 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.13"
    },
    "vscode": {
      "interpreter": {
        "hash": "397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "0273e18cd0fc4067941bd14fd4912672": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f43834dd2a9041b9a1682b97cb7b53ba",
            "placeholder": "​",
            "style": "IPY_MODEL_42594abd60ca4cd99903db92603d433d",
            "value": "Downloading: 100%"
          }
        },
        "0a1c18fe9cad4c3e94e89925623768ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_0273e18cd0fc4067941bd14fd4912672",
              "IPY_MODEL_6dad9135e1c74d9fb7ba0e7632f6bb4a",
              "IPY_MODEL_200d806eead24cc8859fe2d46306d414"
            ],
            "layout": "IPY_MODEL_cc2230712c0641b09bc7cd1d21a75ad8"
          }
        },
        "200d806eead24cc8859fe2d46306d414": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b137db95ace74a2591f64e7f2eb36975",
            "placeholder": "​",
            "style": "IPY_MODEL_cce7f11c38a34d75ab46e3a128cc0205",
            "value": " 440M/440M [00:11&lt;00:00, 38.7MB/s]"
          }
        },
        "42594abd60ca4cd99903db92603d433d": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "69a7706ce17d4cf1817daaeecec01e91": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6dad9135e1c74d9fb7ba0e7632f6bb4a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_69a7706ce17d4cf1817daaeecec01e91",
            "max": 440473133,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_cc20592b4dbf45e99743f2831a333e79",
            "value": 440473133
          }
        },
        "b137db95ace74a2591f64e7f2eb36975": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cc20592b4dbf45e99743f2831a333e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "cc2230712c0641b09bc7cd1d21a75ad8": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "cce7f11c38a34d75ab46e3a128cc0205": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f43834dd2a9041b9a1682b97cb7b53ba": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "3fe9622160bf4b4aa346dfdb786236dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_5104b3f7576b4c95a3fae8d37fd47534",
              "IPY_MODEL_9e699d84c85546f2ba82dff6a2ec3a19",
              "IPY_MODEL_79fb072b5d88410385dcaa5c0fb740f9"
            ],
            "layout": "IPY_MODEL_c82804405e8e41069403e19c510def6e"
          }
        },
        "5104b3f7576b4c95a3fae8d37fd47534": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_e3ab37cd6bf0421a8af668235997c01b",
            "placeholder": "​",
            "style": "IPY_MODEL_eea4c1878d0c479986b552931a6f7b8b",
            "value": "Downloading: 100%"
          }
        },
        "9e699d84c85546f2ba82dff6a2ec3a19": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f9b7ebb484264912a65a4b85d7895338",
            "max": 231508,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_970c1296a3ca415fb017f4a45031f9ed",
            "value": 231508
          }
        },
        "79fb072b5d88410385dcaa5c0fb740f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_f949c3c595d9428f9b8cb56298a571ba",
            "placeholder": "​",
            "style": "IPY_MODEL_d9a49b3bc8114ca296a76006ff32160c",
            "value": " 232k/232k [00:00&lt;00:00, 308kB/s]"
          }
        },
        "c82804405e8e41069403e19c510def6e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "e3ab37cd6bf0421a8af668235997c01b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "eea4c1878d0c479986b552931a6f7b8b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "f9b7ebb484264912a65a4b85d7895338": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "970c1296a3ca415fb017f4a45031f9ed": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "f949c3c595d9428f9b8cb56298a571ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "d9a49b3bc8114ca296a76006ff32160c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "9f62793bcc7a430b9ceccac72d501012": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_8f33f55e537841ecaf8db46c0c05e80a",
              "IPY_MODEL_2dd28c97230748c6812f4334deb41e47",
              "IPY_MODEL_9f90c62b0f174472ab95e18d9b8cf8a9"
            ],
            "layout": "IPY_MODEL_d7b144ebb0df4c91af832295a7509df9"
          }
        },
        "8f33f55e537841ecaf8db46c0c05e80a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_7e9a4c2984954664b3e7bc3cf0661aa9",
            "placeholder": "​",
            "style": "IPY_MODEL_ea68e35c95e34b459fa92257cb4949fa",
            "value": "Downloading: 100%"
          }
        },
        "2dd28c97230748c6812f4334deb41e47": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a87e716ae8824411866de1929a75eefd",
            "max": 28,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_aab2eaf84f1c4588ab9b786d140c51a4",
            "value": 28
          }
        },
        "9f90c62b0f174472ab95e18d9b8cf8a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_865480437aeb4f53912502d4565c30dd",
            "placeholder": "​",
            "style": "IPY_MODEL_ced7bedc6a7b44569050c76938d56008",
            "value": " 28.0/28.0 [00:00&lt;00:00, 652B/s]"
          }
        },
        "d7b144ebb0df4c91af832295a7509df9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7e9a4c2984954664b3e7bc3cf0661aa9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ea68e35c95e34b459fa92257cb4949fa": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a87e716ae8824411866de1929a75eefd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "aab2eaf84f1c4588ab9b786d140c51a4": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "865480437aeb4f53912502d4565c30dd": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ced7bedc6a7b44569050c76938d56008": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "82f28769103b47c0999e476c6894e69c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_457659cc5c1d4b399061715e2764f6a1",
              "IPY_MODEL_e5571e309d364b2a843467fa2c74e4f9",
              "IPY_MODEL_7312aaed002c4579b8ab6fe018b70a2d"
            ],
            "layout": "IPY_MODEL_be0be0d2337e41c29c63dd1bfd898a6d"
          }
        },
        "457659cc5c1d4b399061715e2764f6a1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_214d8fa8f39b46d1a7741652adefed8f",
            "placeholder": "​",
            "style": "IPY_MODEL_6f6c6b44a9ab44a59cd5352d8001d649",
            "value": "Downloading: 100%"
          }
        },
        "e5571e309d364b2a843467fa2c74e4f9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a1d1e3fdd83f430884d691e25cd329a5",
            "max": 570,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_5e8b6b69e28149df9a3edd507d7ce05e",
            "value": 570
          }
        },
        "7312aaed002c4579b8ab6fe018b70a2d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_6a7caf1697dd4ced89210ca3b8fd2b4c",
            "placeholder": "​",
            "style": "IPY_MODEL_f866772200e04950b7da19f8f5f9b669",
            "value": " 570/570 [00:00&lt;00:00, 37.7kB/s]"
          }
        },
        "be0be0d2337e41c29c63dd1bfd898a6d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "214d8fa8f39b46d1a7741652adefed8f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6f6c6b44a9ab44a59cd5352d8001d649": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "a1d1e3fdd83f430884d691e25cd329a5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "5e8b6b69e28149df9a3edd507d7ce05e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "6a7caf1697dd4ced89210ca3b8fd2b4c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "f866772200e04950b7da19f8f5f9b669": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}