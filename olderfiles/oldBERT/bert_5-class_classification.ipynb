{"cells":[{"cell_type":"markdown","metadata":{"id":"PIZC4JRqGhP5"},"source":["# 1. Setup"]},{"cell_type":"markdown","metadata":{"id":"LNeYfLE7mGul"},"source":["## 1.1 Using Colab GPU for Training\n"]},{"cell_type":"markdown","metadata":{"id":"N3upHHQ1m6Zb"},"source":["Run the following the cell to confirm the GPU is detected."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":7647,"status":"ok","timestamp":1676021671607,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"0eZDJWhjmOcC","outputId":"ce4db14e-4702-4dc5-8cf5-d31d7335c6f0"},"outputs":[{"output_type":"stream","name":"stdout","text":["Found GPU at: /device:GPU:0\n"]}],"source":["import tensorflow as tf\n","\n","# Get the GPU device name.\n","device_name = tf.test.gpu_device_name()\n","\n","# The device name should look like the following:\n","if device_name == '/device:GPU:0':\n","    print('Found GPU at: {}'.format(device_name))\n","else:\n","    raise SystemError('GPU device not found')"]},{"cell_type":"markdown","metadata":{"id":"_bgovg_6hNCU"},"source":["In order for torch to use the GPU, we need to identify and specify the GPU as the device. Later, in out training loop"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14508,"status":"ok","timestamp":1676021686111,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"3n7WnLmpg_Hj","outputId":"726cc2d7-d4ea-4564-a1da-c2b5b6481bac"},"outputs":[{"output_type":"stream","name":"stdout","text":["There are 1 GPU(s) available.\n","We will use the GPU: Tesla T4\n"]}],"source":["import torch\n","\n","# If there's a GPU available...\n","if torch.cuda.is_available():    \n","\n","    # Tell PyTorch to use the GPU.    \n","    device = torch.device(\"cuda\")\n","\n","    print('There are %d GPU(s) available.' % torch.cuda.device_count())\n","\n","    print('We will use the GPU:', torch.cuda.get_device_name(0))\n","\n","# If not...\n","else:\n","    print('No GPU available, using the CPU instead.')\n","    device = torch.device(\"cpu\")"]},{"cell_type":"markdown","metadata":{"id":"BH8PQKEhF0YD"},"source":["## 1.2 Installing Hugging Face Library"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":28829,"status":"ok","timestamp":1676021714937,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"n6DvZooRmKrn","outputId":"c7055e1f-134b-433e-e495-6773d06e7e62"},"outputs":[{"output_type":"stream","name":"stdout","text":["Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting transformers\n","  Downloading transformers-4.26.1-py3-none-any.whl (6.3 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m40.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.8/dist-packages (from transformers) (3.9.0)\n","Requirement already satisfied: requests in /usr/local/lib/python3.8/dist-packages (from transformers) (2.25.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (1.21.6)\n","Collecting huggingface-hub<1.0,>=0.11.0\n","  Downloading huggingface_hub-0.12.0-py3-none-any.whl (190 kB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m190.3/190.3 KB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.8/dist-packages (from transformers) (6.0)\n","Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.8/dist-packages (from transformers) (4.64.1)\n","Collecting tokenizers!=0.11.3,<0.14,>=0.11.1\n","  Downloading tokenizers-0.13.2-cp38-cp38-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (7.6 MB)\n","\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.6/7.6 MB\u001b[0m \u001b[31m28.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hRequirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.8/dist-packages (from transformers) (2022.6.2)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.8/dist-packages (from transformers) (23.0)\n","Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.8/dist-packages (from huggingface-hub<1.0,>=0.11.0->transformers) (4.4.0)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2.10)\n","Requirement already satisfied: chardet<5,>=3.0.2 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (4.0.0)\n","Requirement already satisfied: urllib3<1.27,>=1.21.1 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (1.24.3)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.8/dist-packages (from requests->transformers) (2022.12.7)\n","Installing collected packages: tokenizers, huggingface-hub, transformers\n","Successfully installed huggingface-hub-0.12.0 tokenizers-0.13.2 transformers-4.26.1\n"]}],"source":["pip install transformers"]},{"cell_type":"markdown","metadata":{"id":"TxWOmaB9GT4q"},"source":["# 2. Retrieve Dataset"]},{"cell_type":"markdown","metadata":{"id":"L5UDujTsGY00"},"source":["## 2.1 Mount Google Drive"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":22439,"status":"ok","timestamp":1676021737368,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"4atrLvNzb0b3","outputId":"1e8d4dd0-c996-4a54-b6e2-d29aef129662"},"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/gdrive/\n"]}],"source":["from google.colab import drive\n","drive.mount('/content/gdrive/')"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":5,"status":"ok","timestamp":1676021737368,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"CtXpJC70HM-H","outputId":"b61349f2-6635-4307-a22b-0b35664d1c83"},"outputs":[{"output_type":"stream","name":"stdout","text":["/root\n","/root\n","/content/gdrive/MyDrive/nlp-physicseducation\n"]}],"source":["%cd\n","!pwd\n","%cd /content/gdrive/MyDrive/nlp-physicseducation/"]},{"cell_type":"markdown","metadata":{"id":"5pHTEYcyG-D9"},"source":["## 2.2 Parse Data"]},{"cell_type":"markdown","metadata":{"id":"Ln_QvsjDuynn"},"source":["Specify the directories"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"s4EMjNyhu3bW"},"outputs":[],"source":["dir_csv = 'outputs/sections/labels_cleaned_y2.csv'"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":397},"executionInfo":{"elapsed":9,"status":"ok","timestamp":1676023242734,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"4-gB9Op4Fts5","outputId":"51b7a9db-7bf7-4efb-a711-68415471285d"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of reports: 83\n","\n"]},{"output_type":"execute_result","data":{"text/plain":["                 StudentID                                            Content  \\\n","30  2ndINT_TKK454_Redacted  this experiment have demonstrate the power of ...   \n","0   2ndINT_MHH822_Redacted  a michelson interferometer be built and use to...   \n","22  2ndINT_QMC249_Redacted  a. result and error 1 mean wavelength table 1 ...   \n","31  2ndINT_BQD756_Redacted  to conclude a michelson interferometer be buil...   \n","18  2ndINT_JUA192_Redacted  four source be investigate a white lead a blue...   \n","28  2ndINT_XUL605_Redacted  have found the null point data can now be coll...   \n","10  2ndINT_XUQ367_Redacted  a white lead be make from a blue lead with a y...   \n","53  2ndINT_CCY232_Redacted  the main aim of the project be to build a mich...   \n","4   2ndINT_HBQ145_Redacted  the aim of the experiment be to set-up and use...   \n","12  2ndINT_VEJ488_Redacted  the interferogram around the null point for th...   \n","\n","   ArgumentLevel ReasoningLevel  \n","30          deep            exp  \n","0     prediction            the  \n","22      extended            bal  \n","31          deep            exp  \n","18   superficial            bal  \n","28      extended            bal  \n","10      extended            the  \n","53   superficial            the  \n","4           deep           none  \n","12   superficial            exp  "],"text/html":["\n","  <div id=\"df-67306712-1241-4a54-8e0f-5fd4bf8bfdea\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>StudentID</th>\n","      <th>Content</th>\n","      <th>ArgumentLevel</th>\n","      <th>ReasoningLevel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>30</th>\n","      <td>2ndINT_TKK454_Redacted</td>\n","      <td>this experiment have demonstrate the power of ...</td>\n","      <td>deep</td>\n","      <td>exp</td>\n","    </tr>\n","    <tr>\n","      <th>0</th>\n","      <td>2ndINT_MHH822_Redacted</td>\n","      <td>a michelson interferometer be built and use to...</td>\n","      <td>prediction</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>22</th>\n","      <td>2ndINT_QMC249_Redacted</td>\n","      <td>a. result and error 1 mean wavelength table 1 ...</td>\n","      <td>extended</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>31</th>\n","      <td>2ndINT_BQD756_Redacted</td>\n","      <td>to conclude a michelson interferometer be buil...</td>\n","      <td>deep</td>\n","      <td>exp</td>\n","    </tr>\n","    <tr>\n","      <th>18</th>\n","      <td>2ndINT_JUA192_Redacted</td>\n","      <td>four source be investigate a white lead a blue...</td>\n","      <td>superficial</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>28</th>\n","      <td>2ndINT_XUL605_Redacted</td>\n","      <td>have found the null point data can now be coll...</td>\n","      <td>extended</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>10</th>\n","      <td>2ndINT_XUQ367_Redacted</td>\n","      <td>a white lead be make from a blue lead with a y...</td>\n","      <td>extended</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>53</th>\n","      <td>2ndINT_CCY232_Redacted</td>\n","      <td>the main aim of the project be to build a mich...</td>\n","      <td>superficial</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>4</th>\n","      <td>2ndINT_HBQ145_Redacted</td>\n","      <td>the aim of the experiment be to set-up and use...</td>\n","      <td>deep</td>\n","      <td>none</td>\n","    </tr>\n","    <tr>\n","      <th>12</th>\n","      <td>2ndINT_VEJ488_Redacted</td>\n","      <td>the interferogram around the null point for th...</td>\n","      <td>superficial</td>\n","      <td>exp</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-67306712-1241-4a54-8e0f-5fd4bf8bfdea')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-67306712-1241-4a54-8e0f-5fd4bf8bfdea button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-67306712-1241-4a54-8e0f-5fd4bf8bfdea');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":28}],"source":["import pandas as pd\n","\n","# load the dataset into a pandas dataframe\n","df = pd.read_csv(\n","        dir_csv, \n","        encoding='utf-8', \n","        skiprows = 1, \n","        names=['StudentID', 'Content', 'ArgumentLevel', 'ReasoningLevel']\n",")\n","\n","# Report the number of reports\n","print('Number of reports: {:,}\\n'.format(df.shape[0]))\n","\n","# Display 10 random rows from the data\n","df.sample(10)"]},{"cell_type":"markdown","metadata":{"id":"ZfT4uprKJsRG"},"source":["The label 'ArgumentLevel' and 'ReasoningLevel' are mapped to numbers.\n","\n","Argument Level labels {'bal': 0, 'the': 1, 'exp': 2, 'none': 3}\n","\n","Reasoning Level labels {'extended': 0, 'deep': 1, 'expert': 2, 'superficial': 3, 'prediction': 4}"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":363},"executionInfo":{"elapsed":3,"status":"ok","timestamp":1676023245832,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"3upygP5AH0NL","outputId":"42b02c36-3f0f-443a-97a6-775edfcc8088"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["                 StudentID                                            Content  \\\n","80  2ndINT_HHM124_Redacted  to obtain the interferogram for the white lead...   \n","42  2ndINT_BPP712_Redacted  a michelson interferometer be use to inspect t...   \n","61  2ndINT_OQP549_Redacted  we found that the null point corresponds to a ...   \n","60  2ndINT_WSY182_Redacted  the large source of error in this experiment b...   \n","49  2ndINT_JSR480_Redacted  a spike in signal intensity be observe in the ...   \n","20  2ndINT_KGL587_Redacted  the initial scan over the null point with gree...   \n","41  2ndINT_FYO506_Redacted  the data take have be present in the figure to...   \n","15  2ndINT_KUY555_Redacted  we have use the michelson interferometer with ...   \n","17  2ndINT_KSN764_Redacted  the investigation into the fourier transform s...   \n","77  2ndINT_SKD544_Redacted  the interferograms of a tungsten light source ...   \n","\n","    ArgumentLevel ReasoningLevel  \n","80              2            bal  \n","42              4            the  \n","61              1            exp  \n","60              1            exp  \n","49              4           none  \n","20              3            the  \n","41              3            the  \n","15              2            bal  \n","17              2            bal  \n","77              0            exp  "],"text/html":["\n","  <div id=\"df-ed5404a6-e324-4d10-82df-4ca56986a6fe\">\n","    <div class=\"colab-df-container\">\n","      <div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>StudentID</th>\n","      <th>Content</th>\n","      <th>ArgumentLevel</th>\n","      <th>ReasoningLevel</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>80</th>\n","      <td>2ndINT_HHM124_Redacted</td>\n","      <td>to obtain the interferogram for the white lead...</td>\n","      <td>2</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>42</th>\n","      <td>2ndINT_BPP712_Redacted</td>\n","      <td>a michelson interferometer be use to inspect t...</td>\n","      <td>4</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>61</th>\n","      <td>2ndINT_OQP549_Redacted</td>\n","      <td>we found that the null point corresponds to a ...</td>\n","      <td>1</td>\n","      <td>exp</td>\n","    </tr>\n","    <tr>\n","      <th>60</th>\n","      <td>2ndINT_WSY182_Redacted</td>\n","      <td>the large source of error in this experiment b...</td>\n","      <td>1</td>\n","      <td>exp</td>\n","    </tr>\n","    <tr>\n","      <th>49</th>\n","      <td>2ndINT_JSR480_Redacted</td>\n","      <td>a spike in signal intensity be observe in the ...</td>\n","      <td>4</td>\n","      <td>none</td>\n","    </tr>\n","    <tr>\n","      <th>20</th>\n","      <td>2ndINT_KGL587_Redacted</td>\n","      <td>the initial scan over the null point with gree...</td>\n","      <td>3</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>41</th>\n","      <td>2ndINT_FYO506_Redacted</td>\n","      <td>the data take have be present in the figure to...</td>\n","      <td>3</td>\n","      <td>the</td>\n","    </tr>\n","    <tr>\n","      <th>15</th>\n","      <td>2ndINT_KUY555_Redacted</td>\n","      <td>we have use the michelson interferometer with ...</td>\n","      <td>2</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>17</th>\n","      <td>2ndINT_KSN764_Redacted</td>\n","      <td>the investigation into the fourier transform s...</td>\n","      <td>2</td>\n","      <td>bal</td>\n","    </tr>\n","    <tr>\n","      <th>77</th>\n","      <td>2ndINT_SKD544_Redacted</td>\n","      <td>the interferograms of a tungsten light source ...</td>\n","      <td>0</td>\n","      <td>exp</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>\n","      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-ed5404a6-e324-4d10-82df-4ca56986a6fe')\"\n","              title=\"Convert this dataframe to an interactive table.\"\n","              style=\"display:none;\">\n","        \n","  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n","       width=\"24px\">\n","    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n","    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n","  </svg>\n","      </button>\n","      \n","  <style>\n","    .colab-df-container {\n","      display:flex;\n","      flex-wrap:wrap;\n","      gap: 12px;\n","    }\n","\n","    .colab-df-convert {\n","      background-color: #E8F0FE;\n","      border: none;\n","      border-radius: 50%;\n","      cursor: pointer;\n","      display: none;\n","      fill: #1967D2;\n","      height: 32px;\n","      padding: 0 0 0 0;\n","      width: 32px;\n","    }\n","\n","    .colab-df-convert:hover {\n","      background-color: #E2EBFA;\n","      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n","      fill: #174EA6;\n","    }\n","\n","    [theme=dark] .colab-df-convert {\n","      background-color: #3B4455;\n","      fill: #D2E3FC;\n","    }\n","\n","    [theme=dark] .colab-df-convert:hover {\n","      background-color: #434B5C;\n","      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n","      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n","      fill: #FFFFFF;\n","    }\n","  </style>\n","\n","      <script>\n","        const buttonEl =\n","          document.querySelector('#df-ed5404a6-e324-4d10-82df-4ca56986a6fe button.colab-df-convert');\n","        buttonEl.style.display =\n","          google.colab.kernel.accessAllowed ? 'block' : 'none';\n","\n","        async function convertToInteractive(key) {\n","          const element = document.querySelector('#df-ed5404a6-e324-4d10-82df-4ca56986a6fe');\n","          const dataTable =\n","            await google.colab.kernel.invokeFunction('convertToInteractive',\n","                                                     [key], {});\n","          if (!dataTable) return;\n","\n","          const docLinkHtml = 'Like what you see? Visit the ' +\n","            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n","            + ' to learn more about interactive tables.';\n","          element.innerHTML = '';\n","          dataTable['output_type'] = 'display_data';\n","          await google.colab.output.renderOutput(dataTable, element);\n","          const docLink = document.createElement('div');\n","          docLink.innerHTML = docLinkHtml;\n","          element.appendChild(docLink);\n","        }\n","      </script>\n","    </div>\n","  </div>\n","  "]},"metadata":{},"execution_count":29}],"source":["# define dict to code labels to numbers\n","ArgumentLevel_dict = {'extended': 0, 'deep': 1, 'expert': 2, 'superficial': 3, 'prediction': 4}\n","\n","# replace to number labels\n","df['ArgumentLevel'].replace(list(ArgumentLevel_dict.keys()), list(ArgumentLevel_dict.values()),inplace=True) \n","\n","\n","# Display 10 random rows from the data\n","df.sample(10)"]},{"cell_type":"markdown","metadata":{"id":"OmzBpfb-KMe1"},"source":["Let's extract the sentences and labels of our training set as numpy ndarrys."]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":838,"status":"ok","timestamp":1676023263641,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"e9U4SpCcQRJ8","outputId":"384d33b5-fa93-48aa-96ce-462d3514ee45"},"outputs":[{"output_type":"stream","name":"stdout","text":["Number of reports: 83\n","\n","First report:     a michelson interferometer be built and use to analyze a laser led a tungsten lamp and a mercury lamp the movement of mirror close to the null point allow for the use of the michelson interferometer a a spectrometer use a silicon pin photodiode detector and measurement program operating on linus o inteferograms be produce fourier transform with method detailed be use to find wavelength the property of the light source be found and discuss white light source be usually found to have shorter coherence length than colour source due to frequency superposition the tungsten lamp exhibit a black-body emission spectrum with a calculate operating temperature of 3420±40k through comparison with expect result and far data limitation in apparatus be discover and where appropriate correct for mercury lamp data displayed a large amount of noise in the reading and thus the detector use be deem unsuitable for the analysis of this emission i nvented in 1880 the michelson interferometer have form an integral part of many scientific discovery and discipline include special relativity spectroscopy and interferometry 1 the simple set up allows for record of the interference of electromagnetic wave a single light source be input and the beam split use a beamsplitter this be follow by beam recombination with the use of two mirror at the detector the movement of one of the two mirror adjusts the path difference between the two light beam this creates interference pattern from which conclusion can be drawn about the source and it constituent frequency a michelson interferometer be built and use to analyze four different type of light source with vary filter these include a green laser two led a mercury lamp and a tungsten lamp the filter use be green and yellow the variation in light production technique and coherence among source allow for analysis of the suitability of this interferometer for different situation a schematic of a michelson interferometer be show in fig 1. when a beam of light be incident on a beamsplitter it be split into two separate beam one reflect one transmit the reflect beam a travel to mirror m1 and back and the transmit beam b travel to mirror m2 and back for the extend source use in this investigation the interference pattern be dependent on the angle θ between the incoming fig 1 a michelson interferometer m1 and m2 be mirror m2 represent equivalent position of m2 in a parallel setup d be the distance between the beamsplitter and closest mirror and t be difference in parallel position of the mirror 2 p.4 ray and the optical axis through basic trigonometric manipulation it can be see that difference in path length φ of the two beam be where t be the difference in length of the interferometer arm a show this path difference cause interference upon beam recombination the principle of linear superposition state constructive interference occurs when φ be an integer multiple of wavelength and complete destructive interference occurs when φ be an integer plus a half wavelength the interference pattern vary with orientation of the mirror if m1 and m2 be at perfect right angle concentric circular ring call haidinger fringe will be see if they be not at right angle symmetry will be broken and straight bright line of uniform thickness form call frizeau fringe the interferometer may be use a a spectrometer when the distance between the two mirror be approximately equal the null point when use monochromatic light this be see a haidinger fringe reduce to a singularity 3 by take measurement close to the null point the coherence length l of a source may be see on an interferogram though a precise definition of coherence length be not give this be the length over which one arm of the michelson interferometer may be scan before the contrast of the interferogram becomes poor the spectral width ∆ν of the source be a measure of the range of frequency emit by the source and be where c be the speed of light and l be in metre spectral width be measure in hz 2 wavelength can be found by fourier transform of the interferogram an electromagnetic wave with electric field strength e may be express in the form where r be the position vector ω be angular frequency and t be time k be the wave vector whose modulus be 2πν where ν be inverse wavelength or wavenumber use euler 's identity the time dependant part of this function may be represent a a summation of trignometric function ie the potential for representation of any function use sine and cosine allows for fourier transform 4 the shape of an interferogram be dependant on the frequency present and the amplitude or intensity of each frequency the intensity of a signal be proportional to the square of it amplitude the interferogram contains all physical information about a wave fourier transform be use to make interferograms directly interpretable though provide no extra information a fourier transform f ω of the interferogram function in the time domain f t be use to represent data in the frequency domain these two function form a fourier transform pair and be related by this fourier transform return both positive and negative frequency the negative frequency be due to contribution from the imaginary part of the wave take an inverse function of positive frequency reconstructs the original wavelength spectrum fourier transform be carry out on a computer use summation over a large but finite number of point n 5 an interferometer be built follow the configuration in fig 1. the two mirror use be a manually adjust kinematic mirror and a motorize mirror adjust through input command in the linus terminal a thorlab 30 mm cage cube-mounted non-polarizing beamsplitter be use manufacturer specification describe a dielectric anti-reflective coat apply face with a great transmission tolerance to wavelength in the 400 -700 nm range the dielectric coat cause a phase change for one beam lead to destructive interference at zero path difference and constructive interference when λ 2 6 the silicon pin photodiode dectector use have a detection range of 400 -1100nm 7 light source be mount on a v-clamp depend on the intensity of the light source plano-convex lens of focal length 24.5 and 100.0mm be insert between source and beamsplitter to widen beam and between beamsplitter and detector to refocus the alignment of the interferometer be checked and adjust use green laser light point beam be project onto a screen place in front of the detector and the kinematic mirror move until fizeau fringe be see a magnify lens be then insert and the motorize mirror adjust until a minimum of haidinger fringe be see a white light source be then insert and an interferogram take the peak of this interferogram be indicative of a null point at the position −0.6663 ± 0.0005mm and all subsequent data be take in a 100µm range about this point the velocity of the motorize mirror be set to 0.5µms −1 theoretical calculation use the nyquist theorem 4 of the minimum speed imply a much great speed may be use however a the oscillation record by the interferogram do not form perfect sine wave many more than two data point per oscillation be necessary to ensure complete information the velocity of the mirror be also slow to reduce beating between frequency present beating be a result of the superposition of the two interference pattern this beating form an envelop function that corrupt the data follow collection of data interferograms be plot and gaussian fit overlaid the coherence length be deduce to be roughly equal to the full width half maximum of the gaussian curve and from this spectral width calculate where appropriate mean wavelength be found use a similar gaussian-overlay method on the fourier transform and estimate the mean value in case where this be not possible a value be estimate use visual approximation fourier transform spectroscopy approximates light source a point source in many case this be a good approximation however it would be beneficial to insert a small aperture between the source and the beamsplitter fig 2 show an example interferogram table 1 show key result relate to property of light source error be usually calculate through visual estimation of goodness of fit where wavelength be a single value this be the mean wavelength take at peak spectral intensity and error be the standard deviation approximate through a gaussian fit for the blue lead it be possible to computationally fit a gaussian function this produce a covarence matrix from which error parameter be found where wavelength be a range of value this be the range over which there be high spectral intensity error on the coherence length of the white lead be high due to the presence of a secondary gaussian envelope function on the interferogram error on the coherence length of unfiltered tungsten be high due minimal peak in the secondary gaussian lead to highly subjective fitting of the gaussian overlay the coherence length for the mercury lamp be not present due to excessive noise in data no data be take for unfiltered mercury this would be beneficial a a comparative tool the range of wavelength detect whilst use green and yellow filter on the tungsten and mercury lamp be surpasses what be expect for the respective light range despite this clear cut off frequency be demonstrate by the sharp depreciation in spectral intensity at the edge of the allow range a show in fig 4. far discussion on filter be in present in section 4b the white lead exhibit two peak of wavelength a show in fig 3 with wavelength document in table 1. this be indicative of the lead be make up of a monochromatic light source and fluorescent phosphorous coat the agreement of wavelength within range of error of the blue peak of the white lead and the blue lead imply similar source be use when look at the white lead the light emit be obviously non-uniform imply vary thickness in the phosphorus coat a ground glass diffuser be add between the source and beamsplitter in an attempt to minimize this inconsistency and blend frequency despite this when compare interferograms of the light with and without the diffuser no significant change be see it would be beneficial to reduce the aperture of the incident white light use a pin hole to ensure uniformity the coherence length of colour source be see to be large than for white light in most case this be due to the extensive superposition of multiple wave of different frequency a show by large spectral width for white light source many frequency be superimpose to contribute to the final pulse on the interferogram a the number of contribute frequency to a source tends to infinity the resultant pulse of intensity in the time domain narrow the tungsten lamp produce a black body emission spectrum partially show in fig 5 with a peak at 845 ± 10nm use wien 's displacement law where t be absolute temperature of the source the temperature of the tungsten lamp be found to be 3420 ± 40k this value be outside the range expect 2500 -3300 k however it be important to note that the nature of emission of a blackbody mean wavelength deep into the infrared will be emit and will give rise to intensity peak outside of the detector 's sensitivity so this peak in intensity in the visible zone should be treat with some skepticism 8 the majority of discharge of a mercury lamp be in the ultraviolet zone 10 and thus not picked up by the detector this be due to the de-excitation of electron from quantize energy level cause highly energetic photon emission emission in the visible zone be weak and thus highly susceptible to noise when record this be demonstrate by fig 4a data take for mercury be include however should be treat with caution for this reason the expect presence of a doublet under the cut of frequency be demonstrate by sharp decrease in spectral intensity however be different to those expect from manufacturer specification 9 yellow filter be not distinguishable a more sensitive detector be necessary for accurate analysis of mercury emission for low input velocity of the motorize mirror the motion of the mirror be not constant a the input velocity use be 0.5µs −1 the expect positional change between sample be 0.001µm through analysis of a data set of 43422 measurement the mean positional deviation be found to be 1.02 ± 0.64 × 10 −2 µm the error on this result be the standard deviation in addition to this large standard deviation the stage do not move between reading in 4.46 of case this lack of movement demonstrates the lack of suitability of the motorize mirror extensive analysis of light source the inconsistent motion of the mirror may have result in false peak of spectral intensity the beamsplitter use be coat in an anti-reflection coat state to transmit between 40 and 55 of incident light in the wavelength range 400-700nm below this range percentage transmission decay exponentially however above there be a gradual roughly linear decrease the lack of equality between reflection and transmission of wave will effect interference pattern the resultant effect in the fourier transforms have be ignore however quantify resultant error would be beneficial the spectral sensitivity s of the detector be a function of wavelength this lead to a skewed weight and bias towards longer wavelength in the final wavelength fourier transforms of the interferograms through manual fitting of the manufacturer publish graph of spectral sensitivity against wavelength 7 it be possible to approximate the function to where f w be an inverse quadratic function of wavelength w the technique use to fit the curve be estimate through visual analysis to have an error of ±5 by multiply the fourier transform wavelength spectrum by this function intensity be alter to demonstrate 'true experimental situation this be demonstrate in fig 3 where fig 3a be the spectral intensity prior to correction and fig 3b be the spectral intensity after the large range of wavelength show in fig 4 for both the mercury and tungsten lamp be not wholly indicative of the presence of a yellow filter the filter use be thorlab uv/vis bandpass laser line filter the manufacturer specification state a tolerance of 500 − 590 ± 2nm for yellow light the rejection of wavelength outside this range be state to be 99.9 the state lifetime of the filter be two year due to break down of a component dielectric 9 far information be require on the age of the filter to determine the extent this contributes to error in result v. conclusion a michelson interferometer have be create use standard laboratory equipment various light source have be analyze and equipment suitability assess for this line of investigation whilst high quality interferograms have be create and use to find wavelength coherence length and spectral width of light high error be present limitation in various apparatus affect result and reduce precision the filter use have a great tolerance than expect and the motorize mirror have inconstant motion for the mercury lamp weak emission in the range of visible light result in error too great for exact analysis coherence length have be found to be great for colour light source and the reason understood in the majority of case emission have reflect know result within boundary of error far investigation should be undertaken with more suit equipment to fully ass inaccuracy approximation in calculation of wavelength should be far minimize and a wider range of light source and filter analyze\n","Second report:    firstly from the interferogram produce from tlte white lead show in figure 7 it be possible to find the null point compare this to the simulated interferogram in figure 6 it be of a similar shape and symmetly just with more fringe the cenfl•e of this graph and hence the position of the null point of the interferometer lie at -2.872±0.0005mm a gaussian be fit to each of the peak in the spectrum the first one centi•ed at 450nm coll'esponds to the light from the blue lead and the second one centre at 585nm coll'esponds to the mixture of wavelength emit from the phosphor compare this to the theoretical spectrum in figure 3 the position of the spectrum be accurate however the relative intensity of the peak be unexpected this however be a direct result of the responsivity of the silicon detector this be it effectiveness at produce an electi•ic signal at different wavelength this varies with each detector but in general be high for large wavelength a show in figure 9 the aim of this experiment be to use a michelson interferometer a a fourier transform spectrometer in order to obtain and compare spectrum of different light source the interferometer be successful in obtain fairly accurate spectrum for each source and the characteristic of each spectrum i.e mean wavelength and spectral width varied in agreement with the theory the second aspect of this experiment be to show how light filter change the shape of the light spectrum and how the bandwidth of the filter change the spectral width both the blue and yellow filter create a large decrease in the spectral width of the tungsten spectrum and shift the peak accordingly to the wavelength that align with their colour\n"]}],"source":["corpus = df.Content.values\n","ArgumentLevels = df.ArgumentLevel.values\n","\n","print('Number of reports: {:,}\\n'.format(len(corpus)))\n","print('First report:    ',corpus[0])\n","print('Second report:   ',corpus[1])"]},{"cell_type":"markdown","metadata":{"id":"PmOSw5EXRmPf"},"source":["# 3. Tokenization & Input Formatting\n","\n","In this section, we'll transform our dataset into the format that BERT can be trained on."]},{"cell_type":"markdown","metadata":{"id":"kxDtbEM0H186"},"source":["## 3.1 Tokenisation"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":956,"status":"ok","timestamp":1676023265926,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"gx5dgBinVS2N","outputId":"1653d222-47b0-4478-fec1-28446aa2114d"},"outputs":[{"output_type":"stream","name":"stdout","text":["loading BERT tokenizer...\n"]}],"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer\n","print('loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]},{"cell_type":"markdown","metadata":{"id":"nfxyOJYug5F1"},"source":["Now we're ready to perform the real tokenization.\n","\n","The `tokenizer.encode_plus` function combines multiple steps for us:\n","\n","1. Split the sentence into tokens.\n","2. Add the special `[CLS]` and `[SEP]` tokens.\n","3. Map the tokens to their IDs.\n","4. Pad or truncate all sentences to the same length.\n","5. Create the attention masks which explicitly differentiate real tokens from `[PAD]` tokens.\n","\n","The first four features are in `tokenizer.encode`, but I'm using `tokenizer.encode_plus` to get the fifth item (attention masks). Documentation is [here](https://huggingface.co/transformers/main_classes/tokenizer.html?highlight=encode_plus#transformers.PreTrainedTokenizer.encode_plus).\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1505,"status":"ok","timestamp":1676023271761,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"4M7BBNLMg4op","outputId":"0f51a577-3e3b-40b9-f6aa-49b2b2353e39"},"outputs":[{"output_type":"stream","name":"stderr","text":["Truncation was not explicitly activated but `max_length` is provided a specific value, please use `truncation=True` to explicitly truncate examples to max length. Defaulting to 'longest_first' truncation strategy. If you encode pairs of sequences (GLUE-style) with the tokenizer you can select this strategy more precisely by providing a specific strategy to `truncation`.\n","/usr/local/lib/python3.8/dist-packages/transformers/tokenization_utils_base.py:2339: FutureWarning: The `pad_to_max_length` argument is deprecated and will be removed in a future version, use `padding=True` or `padding='longest'` to pad to the longest sequence in the batch, or use `padding='max_length'` to pad to a max length. In this case, you can give a specific length with `max_length` (e.g. `max_length=45`) or leave max_length to None to pad to the maximal input size of the model (e.g. 512 for Bert).\n","  warnings.warn(\n"]}],"source":["# Tokenize all of the reports and map the tokens to their word IDs.\n","input_ids, attention_masks, lengths = [], [], []\n","\n","# For every report ...\n","for report in corpus:\n","    # 'encode_plus' will:\n","    #   (1) Tokenise the sentence.\n","    #   (2) Prepend the '[CLS]' token to the start\n","    #   (3) Append the '[SEP]' token to the end\n","    #   (4) Map tokens to their IDs\n","    #   (5) Pad or truncate the report to 'max_length'\n","    #   (6) Create attention masks for [PAD] tokens\n","    encoded_dict = tokenizer.encode_plus(\n","                        report,                     # report to encode\n","                        add_special_tokens = True,  # Add '[CLS]' and '[SEP]'\n","                        max_length = 512,            # Pad & truncate all reports\n","                        pad_to_max_length = True,\n","                        return_attention_mask = True,   # Construct attn. masks\n","                        return_tensors = 'pt',          # return pytorch tensors\n","\n","    )\n","\n","    # Add the encoded report to the list \n","    input_ids.append(encoded_dict['input_ids'])\n","\n","    # And its attention mask (simply differentiates padding from non-padding)\n","    attention_masks.append(encoded_dict['attention_mask'])\n","\n","    # lengths.append(len(encoded_dict['input_ids']))\n","    lengths.append(len(encoded_dict['input_ids'][0]))\n"]},{"cell_type":"markdown","metadata":{"id":"VRmQt6adGKPW"},"source":["Set which label type to train with; and convert input lists of tensors."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"JxSdH_cxGN98"},"outputs":[],"source":["# change input labels here ***\n","labels = ArgumentLevels   \n","num_labels = 5    #4 for ReasoningLevels #5 for ArgumentLevels\n","\n","\n","# Convert the lists into tensors\n","input_ids = torch.cat(input_ids, dim = 0)\n","attention_masks = torch.cat(attention_masks, dim = 0)\n","labels = torch.tensor(labels)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"fBNaFKj8Hm4J"},"outputs":[],"source":["# Print report 0, now as a list of IDs\n","# print('Original:', corpus[0])\n","# print('Token IDs:', input_ids[0])"]},{"cell_type":"markdown","metadata":{"id":"XwBJenjTQSO6"},"source":["## 3.2 Report Length distribution (Discarded)\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":8,"status":"ok","timestamp":1676021751925,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"TaTbhoegXmiF","outputId":"5806a25c-941d-4c61-996d-df40b1dddc88"},"outputs":[{"output_type":"stream","name":"stdout","text":["Min length: 512 tokens\n","Max length: 512 tokens\n","Median length: 512.0 tokens\n"]}],"source":["import numpy as np\n","print('Min length: {:,} tokens'.format(min(lengths)))\n","print('Max length: {:,} tokens'.format(max(lengths)))\n","print('Median length: {:,} tokens'.format(np.median(lengths)))\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1301,"status":"ok","timestamp":1676021753220,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"2EXPYdmOQR7l","outputId":"26863824-4ad5-45a6-9b4d-b5e0fa562895"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["Text(20.689999999999998, 0.5, '# of Reports')"]},"metadata":{},"execution_count":15},{"output_type":"display_data","data":{"text/plain":["<Figure size 360x360 with 1 Axes>"],"image/png":"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\n"},"metadata":{}}],"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","import numpy as np \n","\n","sns.set(style = 'darkgrid')\n","\n","# Increase the plot size and font size\n","sns.set(font_scale = 1.5)\n","plt.rcParams[\"figure.figsize\"] = (10,5)\n","\n","# Truncate any report lengths greater than 512\n","lengths = [min(l,512) for l in lengths]\n","\n","# Plot the distribution of comment lengths\n","sns.displot(lengths, kde=False, rug=False)\n","\n","plt.title(\"Report Lengths\")\n","plt.xlabel(\"Report Length\")\n","plt.ylabel(\"# of Reports\")\n"]},{"cell_type":"markdown","metadata":{"id":"IyD2pukar1QO"},"source":["How many data (reports) could be faulty? "]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1676021753220,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"3yLVlc7Er8cI","outputId":"a45b07db-64ce-4b6e-e745-3a51ac16e55f"},"outputs":[{"output_type":"stream","name":"stdout","text":["# of possible faulty reports:  0\n"]}],"source":["# Count the number of suspicious reports that didn't not have full text extracted \n","counter = 0\n","for l in lengths:\n","    if l<20:\n","        counter+=1\n","print('# of possible faulty reports: ', counter)\n"]},{"cell_type":"markdown","metadata":{"id":"6haaD5vpf-Qy"},"source":["How many reports run into the 512-token limit?"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":6,"status":"ok","timestamp":1676021753221,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"SKHvKyv6gExI","outputId":"bf6ceba6-8fac-4bf5-a439-3be69b5f9560"},"outputs":[{"output_type":"stream","name":"stdout","text":["83 of 83 sentences (100.0%) of corpus are longer than 512 tokens\n"]}],"source":["# Count the number of sentences that had to be truncated to 512 tokens\n","num_truncated = lengths.count(512)\n","\n","# Compare this to the total number of training reports\n","num_reports = len(lengths)\n","prcnt = float(num_truncated)/float(num_reports)\n","\n","print('{:,} of {:,} sentences ({:.1%}) of corpus are longer than 512 tokens'.format(\n","        num_truncated, num_reports, prcnt))\n"]},{"cell_type":"markdown","metadata":{"id":"ytev8umirwoF"},"source":["## 3.3 Training and Validation Split\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TOUbbujd7coW"},"outputs":[],"source":["# Load BertForSequenceClassification, the pre-trained BERT model with a \n","# single linear classification layer on top.\n","\n","from transformers import BertForSequenceClassification, AdamW, BertConfig\n","\n","model = BertForSequenceClassification.from_pretrained(\n","        \"bert-base-uncased\", # Use the 12-layer BERT model, with an uncased vocab\n","        num_labels = num_labels, # the number of ourput labels -- 5 for five ArgumentLevel classification labels\n","        output_attentions = False, # whether the model returns attention weights\n","        output_hidden_states = False, # whether the model returns all hidden-states\n",")\n","\n","# Tell pytorch to run this model on the GPU\n","model.cuda()\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ewM603LvDo02"},"outputs":[],"source":["# Create helper function\n","import numpy as np\n","import random\n","import time\n","import datetime\n","\n","\n","def flat_accuracy(preds, labels):\n","    '''\n","    Function to calculate the accuracy of our predictions vs labels\n","    '''\n","    pred_flat = np.argmax(preds, axis=1).flatten()\n","    labels_flat = labels.flatten()\n","    return np.sum(pred_flat == labels_flat) / len(labels_flat)\n","\n","def format_time(elapsed):\n","    '''\n","    Taks a time in seconds and returns a string hh:mm:ss\n","    '''\n","    # Round to the nearest second\n","    elapsed_rounded = int(round(elapsed))\n","\n","    # Format as hh:mm:ss\n","    return str(datetime.timedelta(seconds = elapsed_rounded))\n","\n","def to_dataframe(dict):\n","    # Display floats with two decimal places.\n","    pd.set_option('precision', 3)\n","    df_stats = pd.DataFrame(data=dict)\n","    df_stats = df_stats.set_index('epoch')\n","    # A hack to force the column headers to wrap.\n","    # df = df.style.set_table_styles([dict(selector=\"th\",props=[('max-width', '70px')])])\n","    return df_stats"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1IYjV_u8AqVr"},"outputs":[],"source":["import numpy as np\n","import random\n","from torch.utils.data import TensorDataset, random_split\n","from tensorflow.python.ops.variables import validate_synchronization_aggregation_trainable\n","from torch.utils.data import DataLoader, RandomSampler, SequentialSampler\n","from transformers import get_linear_schedule_with_warmup\n","\n","\n","# define a function for train-validation data splitting\n","def train_val_split(dataset, ratio):\n","    '''\n","    # Create a ratio:(1-ratio) train-validation split\n","    \n","    dataset: tensor object\n","    ratio: float <1 and >0\n","    '''\n","    # Calculate the number of samples to include in each set\n","    train_size = int(ratio * len(dataset))\n","    val_size = len(dataset) - train_size\n","\n","    # Divide the dataset by randomly selecting samples\n","    train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n","\n","    print('{:>5,} training samples'.format(train_size))\n","    print('{:>5,} validation samples'.format(val_size))\n","    \n","    return train_dataset, val_dataset\n","\n","\n","\n","# Create the DataLoaders for our training and validation sets.\n","def data_loader(train_dataset, val_dataset,\n","                batch_size):\n","\n","    # We'll take training samples in random order\n","    train_dataloader = DataLoader(\n","                        train_dataset,                  # the trainig samples\n","                        sampler = RandomSampler(train_dataset), # select batches randomly\n","                        batch_size = batch_size,        # trains with this batch size\n","    )\n","\n","    # For validation the order doesn't matter, so we'll just read them sequentially\n","    validation_dataloader = DataLoader(\n","                        val_dataset,                # the validation samples\n","                        sampler = SequentialSampler(val_dataset), # Pull out batches sequentially\n","                        batch_size = batch_size     # evaluate with this batch size\n","    )\n","    return train_dataloader, validation_dataloader\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"owT4X87kGYLf"},"outputs":[],"source":["def runner(training_ratio, epochs = 4,batch_size = 16, lr = 2e-5, seed_val = 42):\n","    # settings\n","    r = training_ratio\n","    # batch_size = 16 # CUDA out of memory if using 32\n","\n","\n","    # Combine the training inputs into a TensorDataset\n","    dataset = TensorDataset(input_ids, attention_masks, labels)\n","    # split data\n","    train_dataset, val_dataset = train_val_split(dataset, r)\n","    # format data\n","    train_dataloader, validation_dataloader = data_loader(train_dataset, \n","                                                        val_dataset,\n","                                                        batch_size)\n","\n","\n","\n","    # ======================================================\n","    # Main - hyperparameters\n","    # ======================================================\n","    # Create optimiser\n","    optimizer = AdamW(model.parameters(),\n","                    lr,  # args.learning rate - default is 5e-5\n","                    eps = 1e-8, # args.adam_epsilon - default is 1e-8\n","    )\n","\n","    # Number of training epochs. The BERT authors recommended between 2 and 4. \n","    # epochs = 4\n","\n","    # Total number of training steps is [number of batches] x [number of epochs]\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Create the learning rate scheduler. # 学习率预热\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps = 0, # Default value in run_glue.py\n","                                                num_training_steps = total_steps,\n","                                                )\n","\n","\n","\n","    # ======================================================\n","    # MAIN - Training loop\n","    # ======================================================\n","    # Set the seed value all over the place to make this reproducible.\n","    # seed_val = 42\n","\n","    random.seed(seed_val)\n","    np.random.seed(seed_val)\n","    torch.manual_seed(seed_val)\n","    torch.cuda.manual_seed_all(seed_val)\n","\n","    # We'll store a number of quantities such as training and validation loss, \n","    # validation accuracy, and timings.\n","    training_stats = []\n","\n","    # Measure the total training time for the whole run.\n","    total_t0 = time.time()\n","\n","\n","\n","    # For each epoch...\n","    for epoch_i in range(0, epochs):\n","        \n","        # ========================================\n","        #               Training\n","        # ========================================\n","        \n","        # Perform one full pass over the training set.\n","\n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        # Measure how long the training epoch takes.\n","        t0 = time.time()\n","\n","        # Reset the total loss for this epoch.\n","        total_train_loss = 0\n","\n","        # Put the model into training mode. \n","        model.train()\n","\n","\n","\n","        # For each batch of training data... \n","        for step, batch in enumerate(train_dataloader):\n","\n","            # Progress update every 40 batches.\n","            if step % 40 == 0 and not step == 0:\n","\n","                # Calculate elapsed time in minutes.\n","                elapsed = format_time(time.time() - t0)\n","                \n","                # Report progress.\n","                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","            # `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            # Always clear any previously calculated gradients before performing a\n","            # backward pass. \n","            model.zero_grad()        \n","\n","            # Perform a forward pass (evaluate the model on this training batch).\n","            # Specifically, we'll get the loss (because we provided labels) and the\n","            # \"logits\"-- the model outputs prior to activation.\n","            result = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels,\n","                        return_dict=True)\n","\n","            loss = result.loss\n","            logits = result.logits\n","\n","            # Accumulate the training loss over all of the batches so that we can\n","            # calculate the average loss at the end. \n","            total_train_loss += loss.item() # Tensor containing a single value\n","\n","            # Perform a backward pass to calculate the gradients.\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and take a step using the computed gradient.\n","            # The optimizer dictates the \"update rule\"--how the parameters are\n","            # modified based on their gradients, the learning rate, etc.\n","            optimizer.step()\n","\n","            # Update the learning rate\n","            scheduler.step()\n","\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_train_loss = total_train_loss / len(train_dataloader)            \n","        \n","        # Measure how long this epoch took.\n","        training_time = format_time(time.time() - t0)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","\n","\n","\n","\n","\n","        # ========================================\n","        #               Validation\n","        # ========================================\n","        # After the completion of each training epoch, measure our performance on\n","        # our validation set.\n","\n","        print(\"\")\n","        print(\"Running Validation...\")\n","\n","        t0 = time.time()\n","\n","        # Put the model in evaluation mode--the dropout layers behave differently\n","        # during evaluation.\n","        model.eval()\n","\n","        # Tracking variables \n","        total_eval_accuracy = 0\n","        total_eval_loss = 0\n","        nb_eval_steps = 0\n","\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            \n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using \n","            # the `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","            \n","            # Tell pytorch not to bother with constructing the compute graph during\n","            # the forward pass, since this is only needed for backprop (training).\n","            with torch.no_grad():        \n","\n","                # Forward pass, calculate logit predictions.\n","                # token_type_ids is the same as the \"segment ids\", which \n","                # differentiates sentence 1 and 2 in 2-sentence tasks.\n","                result = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask,\n","                            labels=b_labels,\n","                            return_dict=True)\n","\n","            # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","            # output values prior to applying an activation function like the \n","            # softmax.\n","            loss = result.loss\n","            logits = result.logits\n","                \n","            # Accumulate the validation loss.\n","            total_eval_loss += loss.item()\n","\n","            # Move logits and labels to CPU\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","\n","            # Calculate the accuracy for this batch of test sentences, and\n","            # accumulate it over all batches.\n","            total_eval_accuracy += flat_accuracy(logits, label_ids)\n","            \n","\n","        # Report the final accuracy for this validation run.\n","        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","        \n","        # Measure how long the validation run took.\n","        validation_time = format_time(time.time() - t0)\n","        \n","        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","        print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","\n","\n","        # ========================================\n","        #               Results\n","        # ========================================\n","        # Record all statistics from this epoch.\n","        training_stats.append(\n","            {\n","                'Run Number': 0,\n","                'Training ratio': r,\n","                'epoch': epoch_i + 1,\n","                # 再加上其他variables\n","                'Training Loss': avg_train_loss,\n","                'Valid. Loss': avg_val_loss,\n","                'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                'Validation Time': validation_time,\n","                \n","            }\n","        )\n","        df = to_dataframe(training_stats)\n","\n","        print(\"\")\n","        print(\"Training complete!\")\n","\n","        print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))\n","\n","    return df\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"3WtGVDAhQIGp"},"outputs":[],"source":["# test\n","# df = runner(0.5)"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"JTd1IZlXHChA","outputId":"48dc3d56-9e6d-41d4-b580-72ed6f5d1d2d","executionInfo":{"status":"ok","timestamp":1676023686348,"user_tz":0,"elapsed":390183,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["========= Running...  ratio: 0.5, run_num:0=============\n","========= variable:4=============\n","   41 training samples\n","   42 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Average training loss: 1.65\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.31\n","  Validation Loss: 1.48\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:05 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 1.49\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.31\n","  Validation Loss: 1.40\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:10 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 1.52\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.31\n","  Validation Loss: 1.41\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:15 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 1.52\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.31\n","  Validation Loss: 1.44\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:20 (h:mm:ss)\n","========= Running...  ratio: 0.5, run_num:1=============\n","========= variable:4=============\n","   41 training samples\n","   42 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 1.39\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.36\n","  Validation Loss: 1.53\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:05 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 1.39\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.25\n","  Validation Loss: 1.57\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:10 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 1.37\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.40\n","  Validation Loss: 1.52\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:16 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 1.21\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 1.51\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:21 (h:mm:ss)\n","========= Running...  ratio: 0.5, run_num:2=============\n","========= variable:4=============\n","   41 training samples\n","   42 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 1.30\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.33\n","  Validation Loss: 1.55\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:05 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 1.04\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.40\n","  Validation Loss: 1.62\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:11 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 1.12\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.38\n","  Validation Loss: 1.69\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:16 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.84\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.40\n","  Validation Loss: 1.61\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:21 (h:mm:ss)\n","       Run Number  Training ratio  Training Loss  Valid. Loss  Valid. Accur.  \\\n","epoch                                                                          \n","1               2             0.5          1.301        1.554          0.329   \n","2               2             0.5          1.042        1.620          0.404   \n","3               2             0.5          1.121        1.691          0.383   \n","4               2             0.5          0.839        1.608          0.404   \n","1               1             0.5          1.391        1.534          0.362   \n","2               1             0.5          1.393        1.570          0.254   \n","3               1             0.5          1.370        1.522          0.396   \n","4               1             0.5          1.212        1.509          0.375   \n","1               0             0.5          1.647        1.484          0.308   \n","2               0             0.5          1.494        1.400          0.312   \n","3               0             0.5          1.520        1.407          0.308   \n","4               0             0.5          1.523        1.441          0.308   \n","\n","      Training Time Validation Time  \n","epoch                                \n","1           0:00:04         0:00:01  \n","2           0:00:04         0:00:01  \n","3           0:00:04         0:00:01  \n","4           0:00:04         0:00:01  \n","1           0:00:04         0:00:01  \n","2           0:00:04         0:00:01  \n","3           0:00:04         0:00:01  \n","4           0:00:04         0:00:01  \n","1           0:00:04         0:00:01  \n","2           0:00:04         0:00:01  \n","3           0:00:04         0:00:01  \n","4           0:00:04         0:00:01  \n","========= Running...  ratio: 0.6, run_num:0=============\n","========= variable:4=============\n","   49 training samples\n","   34 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Average training loss: 1.55\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.08\n","  Validation Loss: 1.64\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:06 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 1.60\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.52\n","  Validation Loss: 1.36\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:12 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 1.21\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.52\n","  Validation Loss: 1.34\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:18 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 1.47\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.54\n","  Validation Loss: 1.35\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:24 (h:mm:ss)\n","========= Running...  ratio: 0.6, run_num:1=============\n","========= variable:4=============\n","   49 training samples\n","   34 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 1.28\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.17\n","  Validation Loss: 1.68\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:06 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 1.31\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.19\n","  Validation Loss: 1.46\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:12 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 1.08\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.58\n","  Validation Loss: 1.30\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:18 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 1.10\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.56\n","  Validation Loss: 1.27\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:24 (h:mm:ss)\n","========= Running...  ratio: 0.6, run_num:2=============\n","========= variable:4=============\n","   49 training samples\n","   34 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.96\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.17\n","  Validation Loss: 1.78\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:06 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 1.18\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.58\n","  Validation Loss: 1.37\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:12 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.68\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.44\n","  Validation Loss: 1.37\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:18 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.58\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.48\n","  Validation Loss: 1.40\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:24 (h:mm:ss)\n","       Run Number  Training ratio  Training Loss  Valid. Loss  Valid. Accur.  \\\n","epoch                                                                          \n","1               2             0.6          0.956        1.781          0.167   \n","2               2             0.6          1.176        1.375          0.583   \n","3               2             0.6          0.683        1.366          0.438   \n","4               2             0.6          0.579        1.398          0.479   \n","1               1             0.6          1.282        1.676          0.167   \n","2               1             0.6          1.311        1.456          0.188   \n","3               1             0.6          1.084        1.298          0.583   \n","4               1             0.6          1.095        1.267          0.562   \n","1               0             0.6          1.547        1.645          0.083   \n","2               0             0.6          1.600        1.359          0.521   \n","3               0             0.6          1.208        1.342          0.521   \n","4               0             0.6          1.470        1.354          0.542   \n","\n","      Training Time Validation Time  \n","epoch                                \n","1           0:00:05         0:00:01  \n","2           0:00:05         0:00:01  \n","3           0:00:05         0:00:01  \n","4           0:00:05         0:00:01  \n","1           0:00:05         0:00:01  \n","2           0:00:05         0:00:01  \n","3           0:00:05         0:00:01  \n","4           0:00:05         0:00:01  \n","1           0:00:05         0:00:01  \n","2           0:00:05         0:00:01  \n","3           0:00:05         0:00:01  \n","4           0:00:05         0:00:01  \n","========= Running...  ratio: 0.7, run_num:0=============\n","========= variable:4=============\n","   58 training samples\n","   25 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Average training loss: 1.02\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.39\n","  Validation Loss: 2.11\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:06 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.83\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.22\n","  Validation Loss: 1.76\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:13 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.68\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.36\n","  Validation Loss: 1.81\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:19 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.56\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.36\n","  Validation Loss: 1.91\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:26 (h:mm:ss)\n","========= Running...  ratio: 0.7, run_num:1=============\n","========= variable:4=============\n","   58 training samples\n","   25 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.50\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.39\n","  Validation Loss: 2.30\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:07 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.48\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.33\n","  Validation Loss: 2.48\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:13 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.35\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.20\n","  Validation Loss: 2.62\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:20 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.27\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.20\n","  Validation Loss: 2.40\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:26 (h:mm:ss)\n","========= Running...  ratio: 0.7, run_num:2=============\n","========= variable:4=============\n","   58 training samples\n","   25 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.27\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.22\n","  Validation Loss: 2.20\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:07 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.24\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.28\n","  Validation Loss: 2.71\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:13 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.18\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.40\n","  Validation Loss: 2.77\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:20 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.12\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.37\n","  Validation Loss: 2.54\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:26 (h:mm:ss)\n","       Run Number  Training ratio  Training Loss  Valid. Loss  Valid. Accur.  \\\n","epoch                                                                          \n","1               2             0.7          0.268        2.203          0.219   \n","2               2             0.7          0.237        2.709          0.281   \n","3               2             0.7          0.177        2.770          0.399   \n","4               2             0.7          0.116        2.538          0.368   \n","1               1             0.7          0.502        2.299          0.392   \n","2               1             0.7          0.477        2.476          0.330   \n","3               1             0.7          0.349        2.617          0.205   \n","4               1             0.7          0.269        2.396          0.205   \n","1               0             0.7          1.024        2.114          0.385   \n","2               0             0.7          0.832        1.756          0.219   \n","3               0             0.7          0.677        1.815          0.361   \n","4               0             0.7          0.560        1.908          0.361   \n","\n","      Training Time Validation Time  \n","epoch                                \n","1           0:00:06         0:00:01  \n","2           0:00:06         0:00:01  \n","3           0:00:06         0:00:01  \n","4           0:00:06         0:00:01  \n","1           0:00:06         0:00:01  \n","2           0:00:06         0:00:01  \n","3           0:00:06         0:00:01  \n","4           0:00:06         0:00:01  \n","1           0:00:06         0:00:01  \n","2           0:00:06         0:00:01  \n","3           0:00:06         0:00:01  \n","4           0:00:06         0:00:01  \n","========= Running...  ratio: 0.8, run_num:0=============\n","========= variable:4=============\n","   66 training samples\n","   17 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Average training loss: 0.38\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.16\n","  Validation Loss: 1.77\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:07 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.15\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.72\n","  Validation Loss: 1.68\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:14 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.32\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.22\n","  Validation Loss: 3.77\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:21 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.04\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.19\n","  Validation Loss: 4.52\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:28 (h:mm:ss)\n","========= Running...  ratio: 0.8, run_num:1=============\n","========= variable:4=============\n","   66 training samples\n","   17 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.05\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.22\n","  Validation Loss: 4.80\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:07 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.03\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.19\n","  Validation Loss: 5.62\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:14 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.15\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.19\n","  Validation Loss: 5.33\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:21 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.01\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.19\n","  Validation Loss: 5.66\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:28 (h:mm:ss)\n","========= Running...  ratio: 0.8, run_num:2=============\n","========= variable:4=============\n","   66 training samples\n","   17 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.01\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.19\n","  Validation Loss: 6.90\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:07 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.01\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.19\n","  Validation Loss: 6.53\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:14 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.22\n","  Validation Loss: 6.39\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:21 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.22\n","  Validation Loss: 6.34\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:28 (h:mm:ss)\n","       Run Number  Training ratio  Training Loss  Valid. Loss  Valid. Accur.  \\\n","epoch                                                                          \n","1               2             0.8          0.007        6.898          0.188   \n","2               2             0.8          0.008        6.532          0.188   \n","3               2             0.8          0.002        6.393          0.219   \n","4               2             0.8          0.002        6.338          0.219   \n","1               1             0.8          0.050        4.804          0.219   \n","2               1             0.8          0.033        5.623          0.188   \n","3               1             0.8          0.146        5.327          0.188   \n","4               1             0.8          0.010        5.661          0.188   \n","1               0             0.8          0.383        1.771          0.156   \n","2               0             0.8          0.154        1.677          0.719   \n","3               0             0.8          0.322        3.774          0.219   \n","4               0             0.8          0.039        4.523          0.188   \n","\n","      Training Time Validation Time  \n","epoch                                \n","1           0:00:06         0:00:01  \n","2           0:00:06         0:00:01  \n","3           0:00:06         0:00:01  \n","4           0:00:06         0:00:01  \n","1           0:00:06         0:00:01  \n","2           0:00:06         0:00:01  \n","3           0:00:06         0:00:01  \n","4           0:00:06         0:00:01  \n","1           0:00:06         0:00:01  \n","2           0:00:06         0:00:01  \n","3           0:00:06         0:00:01  \n","4           0:00:06         0:00:01  \n","========= Running...  ratio: 0.9, run_num:0=============\n","========= variable:4=============\n","   74 training samples\n","    9 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Average training loss: 0.33\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.22\n","  Validation Loss: 4.89\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:08 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.06\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.00\n","  Validation Loss: 7.06\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:15 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.12\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.11\n","  Validation Loss: 6.90\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:23 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.11\n","  Validation Loss: 6.86\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:31 (h:mm:ss)\n","========= Running...  ratio: 0.9, run_num:1=============\n","========= variable:4=============\n","   74 training samples\n","    9 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.22\n","  Validation Loss: 6.70\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:08 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.07\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.11\n","  Validation Loss: 7.18\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:15 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.11\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.11\n","  Validation Loss: 6.92\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:23 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.11\n","  Validation Loss: 7.29\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:31 (h:mm:ss)\n","========= Running...  ratio: 0.9, run_num:2=============\n","========= variable:4=============\n","   74 training samples\n","    9 validation samples\n","\n","======== Epoch 1 / 4 ========\n","Training...\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.00\n","  Validation Loss: 10.44\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:08 (h:mm:ss)\n","\n","======== Epoch 2 / 4 ========\n","Training...\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.00\n","  Validation Loss: 9.91\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:15 (h:mm:ss)\n","\n","======== Epoch 3 / 4 ========\n","Training...\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.11\n","  Validation Loss: 9.73\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:23 (h:mm:ss)\n","\n","======== Epoch 4 / 4 ========\n","Training...\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.11\n","  Validation Loss: 9.77\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:30 (h:mm:ss)\n","       Run Number  Training ratio  Training Loss  Valid. Loss  Valid. Accur.  \\\n","epoch                                                                          \n","1               2             0.9      3.084e-04       10.436          0.000   \n","2               2             0.9      2.250e-04        9.913          0.000   \n","3               2             0.9      1.111e-04        9.734          0.111   \n","4               2             0.9      8.663e-05        9.768          0.111   \n","1               1             0.9      1.568e-03        6.702          0.222   \n","2               1             0.9      6.946e-02        7.176          0.111   \n","3               1             0.9      1.100e-01        6.917          0.111   \n","4               1             0.9      4.511e-04        7.292          0.111   \n","1               0             0.9      3.272e-01        4.887          0.222   \n","2               0             0.9      6.424e-02        7.056          0.000   \n","3               0             0.9      1.199e-01        6.897          0.111   \n","4               0             0.9      2.704e-03        6.863          0.111   \n","\n","      Training Time Validation Time  \n","epoch                                \n","1           0:00:07         0:00:00  \n","2           0:00:07         0:00:00  \n","3           0:00:07         0:00:00  \n","4           0:00:07         0:00:00  \n","1           0:00:07         0:00:00  \n","2           0:00:07         0:00:00  \n","3           0:00:07         0:00:00  \n","4           0:00:07         0:00:00  \n","1           0:00:07         0:00:00  \n","2           0:00:07         0:00:00  \n","3           0:00:07         0:00:00  \n","4           0:00:07         0:00:00  \n"]}],"source":["# ****Classifier Parameters****\n","# set epochs\n","epochs = 4\n","# set batch size\n","batch_size = 16\n","# set random seed\n","seed_val = 42\n","# set learnning rate\n","lr = 1e-4   # args.learning rate - default is 5e-5\n","\n","# ****Training Parameters****\n","# set the number of loops \n","run_num = 3\n","# set training data ratio\n","ratio_list = [0.5,0.6,0.7,0.8,0.9]\n","\n","\n","for ratio in ratio_list:\n","    # create a dataframe to saveto\n","    final_df = pd.DataFrame()\n","    \n","    for counter in range(run_num):\n","        print('========= Running...  ratio: '+str(ratio)+', run_num:'+str(counter)+'=============')\n","        print('========= variable:'+str(epochs)+'=============')\n","\n","        df = runner(ratio, epochs, batch_size, lr, seed_val)\n","        # count 'Run Number'\n","        df['Run Number'] = counter\n","        # append to the final dataframe\n","        final_df = pd.concat([df, final_df])\n","        \n","    # save dataframe\n","    print(final_df) \n","    filepath = './Pickledfiles/bert_argumentlevel/year2/'+str(ratio)+'trainratio_'+str(run_num)+'runs_'+str(epochs)+'epochs_'+str(batch_size)+'batch_size_'+str(lr)+'lr_'+str(seed_val)+'seed_val_.pkl'\n","    final_df.to_pickle(filepath)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"NirHP_B-Wu8B"},"source":["# OLD CODE"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":64203,"status":"ok","timestamp":1676022504439,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"H1b87vXVDWZu","colab":{"base_uri":"https://localhost:8080/"},"outputId":"166bebe6-4a5f-492d-ac0e-b9055eecdcb9"},"outputs":[{"output_type":"stream","name":"stdout","text":["   41 training samples\n","   42 validation samples\n","\n","======== Epoch 1 / 2 ========\n","Training...\n"]},{"output_type":"stream","name":"stderr","text":["/usr/local/lib/python3.8/dist-packages/transformers/optimization.py:306: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n","  warnings.warn(\n"]},{"output_type":"stream","name":"stdout","text":["\n","  Average training loss: 0.00\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.83\n","  Validation Loss: 0.95\n","  Validation took: 0:00:01\n","\n","======== Epoch 2 / 2 ========\n","Training...\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:00:04\n","\n","Running Validation...\n","  Accuracy: 0.83\n","  Validation Loss: 1.00\n","  Validation took: 0:00:02\n","\n","Training complete!\n","Total training took 0:00:11 (h:mm:ss)\n","   49 training samples\n","   34 validation samples\n","\n","======== Epoch 1 / 2 ========\n","Training...\n","\n","  Average training loss: 0.32\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.96\n","  Validation Loss: 0.15\n","  Validation took: 0:00:01\n","\n","======== Epoch 2 / 2 ========\n","Training...\n","\n","  Average training loss: 0.22\n","  Training epcoh took: 0:00:05\n","\n","Running Validation...\n","  Accuracy: 0.98\n","  Validation Loss: 0.14\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:12 (h:mm:ss)\n","   58 training samples\n","   25 validation samples\n","\n","======== Epoch 1 / 2 ========\n","Training...\n","\n","  Average training loss: 0.01\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.91\n","  Validation Loss: 0.25\n","  Validation took: 0:00:01\n","\n","======== Epoch 2 / 2 ========\n","Training...\n","\n","  Average training loss: 0.10\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.91\n","  Validation Loss: 0.30\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:13 (h:mm:ss)\n","   66 training samples\n","   17 validation samples\n","\n","======== Epoch 1 / 2 ========\n","Training...\n","\n","  Average training loss: 0.12\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.97\n","  Validation Loss: 0.04\n","  Validation took: 0:00:01\n","\n","======== Epoch 2 / 2 ========\n","Training...\n","\n","  Average training loss: 0.04\n","  Training epcoh took: 0:00:06\n","\n","Running Validation...\n","  Accuracy: 0.97\n","  Validation Loss: 0.06\n","  Validation took: 0:00:01\n","\n","Training complete!\n","Total training took 0:00:14 (h:mm:ss)\n","   74 training samples\n","    9 validation samples\n","\n","======== Epoch 1 / 2 ========\n","Training...\n","\n","  Average training loss: 0.01\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.89\n","  Validation Loss: 0.24\n","  Validation took: 0:00:00\n","\n","======== Epoch 2 / 2 ========\n","Training...\n","\n","  Average training loss: 0.00\n","  Training epcoh took: 0:00:07\n","\n","Running Validation...\n","  Accuracy: 0.89\n","  Validation Loss: 0.32\n","  Validation took: 0:00:00\n","\n","Training complete!\n","Total training took 0:00:15 (h:mm:ss)\n"]}],"source":["# ======================================================\n","# MAIN\n","# ======================================================\n","# Combine the training inputs into a TensorDataset\n","dataset = TensorDataset(input_ids, attention_masks, labels)\n","ratios = [0.5, 0.6, 0.7, 0.8, 0.9]\n","\n","\n","# loop training for each training-test ratios\n","for r in ratios:\n","\n","    batch_size = 16 # CUDA out of memory if using 32\n","    \n","    # split data\n","    train_dataset, val_dataset = train_val_split(dataset, r)\n","    # format data\n","    train_dataloader, validation_dataloader = data_loader(train_dataset, \n","                                                          val_dataset,\n","                                                          batch_size)\n","\n","\n","\n","    # ======================================================\n","    # Main - hyperparameters\n","    # ======================================================\n","    # Create optimiser\n","    optimizer = AdamW(model.parameters(),\n","                    lr = 2e-5,  # args.learning rate - default is 5e-5\n","                    eps = 1e-8, # args.adam_epsilon - default is 1e-8\n","    )\n","\n","    # Number of training epochs. The BERT authors recommended between 2 and 4. \n","    epochs = 2\n","  \n","    # Total number of training steps is [number of batches] x [number of epochs]\n","    total_steps = len(train_dataloader) * epochs\n","\n","    # Create the learning rate scheduler. # 学习率预热\n","    scheduler = get_linear_schedule_with_warmup(optimizer,\n","                                                num_warmup_steps = 0, # Default value in run_glue.py\n","                                                num_training_steps = total_steps,\n","                                                )\n","\n","\n","\n","    # ======================================================\n","    # MAIN - Training loop\n","    # ======================================================\n","    # Set the seed value all over the place to make this reproducible.\n","    seed_val = 42\n","\n","    random.seed(seed_val)\n","    np.random.seed(seed_val)\n","    torch.manual_seed(seed_val)\n","    torch.cuda.manual_seed_all(seed_val)\n","\n","    # We'll store a number of quantities such as training and validation loss, \n","    # validation accuracy, and timings.\n","    training_stats = []\n","\n","    # Measure the total training time for the whole run.\n","    total_t0 = time.time()\n","\n","\n","\n","    # For each epoch...\n","    for epoch_i in range(0, epochs):\n","        \n","        # ========================================\n","        #               Training\n","        # ========================================\n","        \n","        # Perform one full pass over the training set.\n","\n","        print(\"\")\n","        print('======== Epoch {:} / {:} ========'.format(epoch_i + 1, epochs))\n","        print('Training...')\n","\n","        # Measure how long the training epoch takes.\n","        t0 = time.time()\n","\n","        # Reset the total loss for this epoch.\n","        total_train_loss = 0\n","\n","        # Put the model into training mode. \n","        model.train()\n","\n","\n","\n","        # For each batch of training data... \n","        for step, batch in enumerate(train_dataloader):\n","\n","            # Progress update every 40 batches.\n","            if step % 40 == 0 and not step == 0:\n","\n","                # Calculate elapsed time in minutes.\n","                elapsed = format_time(time.time() - t0)\n","                \n","                # Report progress.\n","                print('  Batch {:>5,}  of  {:>5,}.    Elapsed: {:}.'.format(step, len(train_dataloader), elapsed))\n","\n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using the \n","            # `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","\n","            # Always clear any previously calculated gradients before performing a\n","            # backward pass. \n","            model.zero_grad()        \n","\n","            # Perform a forward pass (evaluate the model on this training batch).\n","            # Specifically, we'll get the loss (because we provided labels) and the\n","            # \"logits\"-- the model outputs prior to activation.\n","            result = model(b_input_ids, \n","                        token_type_ids=None, \n","                        attention_mask=b_input_mask, \n","                        labels=b_labels,\n","                        return_dict=True)\n","\n","            loss = result.loss\n","            logits = result.logits\n","\n","            # Accumulate the training loss over all of the batches so that we can\n","            # calculate the average loss at the end. \n","            total_train_loss += loss.item() # Tensor containing a single value\n","\n","            # Perform a backward pass to calculate the gradients.\n","            loss.backward()\n","\n","            # Clip the norm of the gradients to 1.0.\n","            # This is to help prevent the \"exploding gradients\" problem.\n","            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n","\n","            # Update parameters and take a step using the computed gradient.\n","            # The optimizer dictates the \"update rule\"--how the parameters are\n","            # modified based on their gradients, the learning rate, etc.\n","            optimizer.step()\n","\n","            # Update the learning rate\n","            scheduler.step()\n","\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_train_loss = total_train_loss / len(train_dataloader)            \n","        \n","        # Measure how long this epoch took.\n","        training_time = format_time(time.time() - t0)\n","\n","        print(\"\")\n","        print(\"  Average training loss: {0:.2f}\".format(avg_train_loss))\n","        print(\"  Training epcoh took: {:}\".format(training_time))\n","\n","\n","\n","\n","\n","\n","        # ========================================\n","        #               Validation\n","        # ========================================\n","        # After the completion of each training epoch, measure our performance on\n","        # our validation set.\n","\n","        print(\"\")\n","        print(\"Running Validation...\")\n","\n","        t0 = time.time()\n","\n","        # Put the model in evaluation mode--the dropout layers behave differently\n","        # during evaluation.\n","        model.eval()\n","\n","        # Tracking variables \n","        total_eval_accuracy = 0\n","        total_eval_loss = 0\n","        nb_eval_steps = 0\n","\n","        # Evaluate data for one epoch\n","        for batch in validation_dataloader:\n","            \n","            # Unpack this training batch from our dataloader. \n","            #\n","            # As we unpack the batch, we'll also copy each tensor to the GPU using \n","            # the `to` method.\n","            #\n","            # `batch` contains three pytorch tensors:\n","            #   [0]: input ids \n","            #   [1]: attention masks\n","            #   [2]: labels \n","            b_input_ids = batch[0].to(device)\n","            b_input_mask = batch[1].to(device)\n","            b_labels = batch[2].to(device)\n","            \n","            # Tell pytorch not to bother with constructing the compute graph during\n","            # the forward pass, since this is only needed for backprop (training).\n","            with torch.no_grad():        \n","\n","                # Forward pass, calculate logit predictions.\n","                # token_type_ids is the same as the \"segment ids\", which \n","                # differentiates sentence 1 and 2 in 2-sentence tasks.\n","                result = model(b_input_ids, \n","                            token_type_ids=None, \n","                            attention_mask=b_input_mask,\n","                            labels=b_labels,\n","                            return_dict=True)\n","\n","            # Get the loss and \"logits\" output by the model. The \"logits\" are the \n","            # output values prior to applying an activation function like the \n","            # softmax.\n","            loss = result.loss\n","            logits = result.logits\n","                \n","            # Accumulate the validation loss.\n","            total_eval_loss += loss.item()\n","\n","            # Move logits and labels to CPU\n","            logits = logits.detach().cpu().numpy()\n","            label_ids = b_labels.to('cpu').numpy()\n","\n","            # Calculate the accuracy for this batch of test sentences, and\n","            # accumulate it over all batches.\n","            total_eval_accuracy += flat_accuracy(logits, label_ids)\n","            \n","\n","        # Report the final accuracy for this validation run.\n","        avg_val_accuracy = total_eval_accuracy / len(validation_dataloader)\n","        print(\"  Accuracy: {0:.2f}\".format(avg_val_accuracy))\n","\n","        # Calculate the average loss over all of the batches.\n","        avg_val_loss = total_eval_loss / len(validation_dataloader)\n","        \n","        # Measure how long the validation run took.\n","        validation_time = format_time(time.time() - t0)\n","        \n","        print(\"  Validation Loss: {0:.2f}\".format(avg_val_loss))\n","        print(\"  Validation took: {:}\".format(validation_time))\n","\n","\n","\n","\n","\n","        # Record all statistics from this epoch.\n","        training_stats.append(\n","            {\n","                'epoch': epoch_i + 1,\n","                'Training Loss': avg_train_loss,\n","                'Valid. Loss': avg_val_loss,\n","                'Valid. Accur.': avg_val_accuracy,\n","                'Training Time': training_time,\n","                'Validation Time': validation_time\n","            }\n","        )\n","\n","        # save dataframe\n","        df = to_dataframe(training_stats)\n","        filepath = 'Pickledfiles/bert_reasoninglevel_trainratio'+str(r)+'_epochs'+str(epochs)+'_seedval42'+'_batchsize16'\n","        df.to_pickle(filepath)\n","\n","    print(\"\")\n","    print(\"Training complete!\")\n","\n","    print(\"Total training took {:} (h:mm:ss)\".format(format_time(time.time()-total_t0)))"]},{"cell_type":"markdown","metadata":{"id":"-arhBqEY4I-r"},"source":["\n","4.3 Training loop\n","\n","Below is our training loop. There's a lot going on, but fundamentally for each pass in our loop we have a trianing phase and a validation phase. \n","\n","> *Thank you to [Stas Bekman](https://ca.linkedin.com/in/stasbekman) for contributing the insights and code for using validation loss to detect over-fitting!*\n","\n","Training: \n","* Unpack our data inputs and labels \n","* Load data onto GPU for acceleration\n","* Clear out the gradients calculated in the previous pass\n","    * In pytorch the gradients accumulate by default (useful for things like RNNs) unless you explicitly clear them out\n","* Forward pass (feed input data through the network)\n","* Backward pass (backpropagation)\n","* Tell the network to update parameters with optimizer.step()\n","* Track variables for minitoring progress\n","\n","Evaluation:\n","* Unpack our data inputs and labels\n","* Load data onto the GPU for acceleration\n","* Forward pass (feed input data through the network)\n","* Compute loss on our validation data and track variables for monitoring process\n","\n","\n","Pytorch hides all the detailed calculations from us, but we've commented the code to print out which of the above steps are happening on each line.\n","\n","> *PyTorch also has some [beginner tutorials](https://pytorch.org/tutorials/beginner/blitz/cifar10_tutorial.html#sphx-glr-beginner-blitz-cifar10-tutorial-py) which you may also find helpful.*"]},{"cell_type":"markdown","metadata":{"id":"-eJilZ9_1W7A"},"source":["Let's view the summary of the training process."]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":668,"status":"error","timestamp":1676022505089,"user":{"displayName":"Jiayang Zhang","userId":"01870170909349403002"},"user_tz":0},"id":"ZJgACViioLT1","colab":{"base_uri":"https://localhost:8080/","height":433},"outputId":"382dff15-5582-4ea9-c9c6-44d916fad344"},"outputs":[{"output_type":"error","ename":"FileNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)","\u001b[0;32m<ipython-input-25-32c8bb6fecb0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mratios\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;31m# path = 'Pickledfiles/bert_reasoninglevel_trainratio'+str(i)+'_epochs1_seedval42_batchsize16.pkl'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_pickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'bert_reasoninglevel_trainratio0.5_epochs1_seedval42_batchsize16'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mvalidacc_list\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miloc\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'Valid. Accur.'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/pickle.py\u001b[0m in \u001b[0;36mread_pickle\u001b[0;34m(filepath_or_buffer, compression, storage_options)\u001b[0m\n\u001b[1;32m    194\u001b[0m     \"\"\"\n\u001b[1;32m    195\u001b[0m     \u001b[0mexcs_to_catch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mAttributeError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mImportError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModuleNotFoundError\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mTypeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 196\u001b[0;31m     with get_handle(\n\u001b[0m\u001b[1;32m    197\u001b[0m         \u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    198\u001b[0m         \u001b[0;34m\"rb\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;32m/usr/local/lib/python3.8/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    709\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    710\u001b[0m             \u001b[0;31m# Binary mode\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 711\u001b[0;31m             \u001b[0mhandle\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    712\u001b[0m         \u001b[0mhandles\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhandle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    713\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'bert_reasoninglevel_trainratio0.5_epochs1_seedval42_batchsize16'"]}],"source":["import pandas as pd\n","ratios = [0.5,0.6,0.7,0.8,0.9]\n","validacc_list = []\n","\n","for i in ratios:\n","    # path = 'Pickledfiles/bert_reasoninglevel_trainratio'+str(i)+'_epochs1_seedval42_batchsize16.pkl'\n","    df = pd.read_pickle('bert_reasoninglevel_trainratio0.5_epochs1_seedval42_batchsize16')\n","    validacc_list.append(df.iloc[0]['Valid. Accur.'])\n","    print(df)\n","\n","print(validacc_list)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a1gSwd0WwLTf"},"outputs":[],"source":["pip install git+https://github.com/garrettj403/SciencePlots"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WtqOdDP5vt4N"},"outputs":[],"source":["\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rBwtofW32kdt"},"outputs":[],"source":["import matplotlib.pyplot as plt\n","%matplotlib inline\n","import seaborn as sns\n","\n","\n","# Use plot styling from seaborn.\n","sns.set(style='darkgrid')\n","\n","# Increase the plot size and font size.\n","sns.set(font_scale=1.5)\n","plt.rcParams[\"figure.figsize\"] = (12,6)\n","\n","# Plot the learning curve.\n","plt.plot(df_stats['Training Loss'], 'b-o', label=\"Training\")\n","plt.plot(df_stats['Valid. Loss'], 'g-o', label=\"Validation\")\n","\n","# Label the plot.\n","plt.title(\"Training & Validation Loss\")\n","plt.xlabel(\"Epoch\")\n","plt.ylabel(\"Loss\")\n","plt.legend()\n","plt.xticks([1, 2, 3, 4])\n","\n","plt.show()"]},{"cell_type":"markdown","metadata":{"id":"YsdFtpZ94gPu"},"source":["5. Performance on Test Set"]},{"cell_type":"markdown","metadata":{"id":"rU_BBWgy4lK_"},"source":["5.1 Data Preparation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PRXxjlsM5RZ2"},"outputs":[],"source":["# Create the DataLoader.\n","prediction_data = val_dataset\n","prediction_sampler = SequentialSampler(prediction_data)\n","prediction_dataloader = validation_dataloader"]},{"cell_type":"markdown","metadata":{"id":"DHFs1AkB434s"},"source":["5.2 Evaluate on Test Set"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8KMdiBA9422k"},"outputs":[],"source":["# Prediction on test set\n","\n","print('Predicting labels for {:,} test reports...'.format(len(val_dataset)))\n","\n","# Put model in evaluation mode\n","model.eval()\n","\n","# Tracking variables \n","predictions , true_labels = [], []\n","\n","# Predict \n","for batch in prediction_dataloader:\n","  # Add batch to GPU\n","  batch = tuple(t.to(device) for t in batch)\n","  \n","  # Unpack the inputs from our dataloader\n","  b_input_ids, b_input_mask, b_labels = batch\n","  \n","  # Telling the model not to compute or store gradients, saving memory and \n","  # speeding up prediction\n","  with torch.no_grad():\n","      # Forward pass, calculate logit predictions.\n","      result = model(b_input_ids, \n","                     token_type_ids=None, \n","                     attention_mask=b_input_mask,\n","                     return_dict=True)\n","\n","  logits = result.logits\n","\n","  # Move logits and labels to CPU\n","  logits = logits.detach().cpu().numpy()\n","  label_ids = b_labels.to('cpu').numpy()\n","  \n","  # Store predictions and true labels\n","  predictions.append(logits)\n","  true_labels.append(label_ids)\n","\n","print('    DONE.')"]},{"cell_type":"markdown","metadata":{"id":"SXlId81P6Kh2"},"source":["Accuracy on the CoLA benchmark is measured using the \"[Matthews correlation coefficient](https://scikit-learn.org/stable/modules/generated/sklearn.metrics.matthews_corrcoef.html)\" (MCC).\n","\n","We use MCC here because the classes are imbalanced:\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"jJD1D-g86NUs"},"outputs":[],"source":["from sklearn.metrics import matthews_corrcoef\n","\n","matthews_set = []\n","\n","# Evaluate each test batch using Matthew's correlation coefficient\n","print('Calculating Matthews Corr. Coef. for each batch...')\n","\n","# For each input batch...\n","for i in range(len(true_labels)):\n","  \n","  # The predictions for this batch are a 2-column ndarray (one column for \"0\" \n","  # and one column for \"1\"). Pick the label with the highest value and turn this\n","  # in to a list of 0s and 1s.\n","  pred_labels_i = np.argmax(predictions[i], axis=1).flatten()\n","  \n","  # Calculate and store the coef for this batch.  \n","  matthews = matthews_corrcoef(true_labels[i], pred_labels_i)                \n","  matthews_set.append(matthews)"]},{"cell_type":"markdown","metadata":{"id":"Kx7kqXn6D6VY"},"source":["The final score will be based on the entire test set, but let's take a look at the scores on the individual batches to get a sense of the variability in the metric between batches. \n","\n","Each batch has 32 sentences in it, except the last batch which has only (516 % 32) = 4 test sentences in it.\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"poxIZ-UiD-GZ"},"outputs":[],"source":["# Create a barplot showing the MCC score for each batch of test samples.\n","ax = sns.barplot(x=list(range(len(matthews_set))), y=matthews_set, ci=None)\n","\n","plt.title('MCC Score per Batch')\n","plt.ylabel('MCC Score (-1 to +1)')\n","plt.xlabel('Batch #')\n","\n","plt.show()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Nyuwg22cEFWy"},"outputs":[],"source":["# Combine the results across all batches. \n","flat_predictions = np.concatenate(predictions, axis=0)\n","\n","# For each sample, pick the label (0 or 1) with the higher score.\n","flat_predictions = np.argmax(flat_predictions, axis=1).flatten()\n","\n","# Combine the correct labels for each batch into a single list.\n","flat_true_labels = np.concatenate(true_labels, axis=0)\n","\n","# Calculate the MCC\n","mcc = matthews_corrcoef(flat_true_labels, flat_predictions)\n","\n","print('Total MCC: %.3f' % mcc)"]},{"cell_type":"markdown","metadata":{"id":"dJOEigZaddtC"},"source":["When we actually convert all of our sentences, we'll use the `tokenize.encode` function to handle both steps, rather than calling `tokenize` and `convert_tokens_to_ids` separately. \n","\n","Before we can do that, though, we need to talk about some of BERT's formatting requirements."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGYv9paXSDm9"},"outputs":[],"source":["# Print the original report.\n","print('length:',len(corpus[0]),';  Original: ', corpus[0])\n","\n","# Print the report split into tokens.\n","print('length:',len(tokenizer.tokenize(corpus[0])),';  Tokenized: ', tokenizer.tokenize(corpus[0]))\n","\n","# Print the report mapped to token ids.\n","print('length:',len(tokenizer.convert_tokens_to_ids(tokenizer.tokenize(corpus[0]))) , ';  Token IDs: ', tokenizer.convert_tokens_to_ids(tokenizer.tokenize(corpus[0])))"]},{"cell_type":"markdown","metadata":{"id":"RgPW9ujVSFLv"},"source":["Let's apply the tokenizer to one report just to see the output."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hrik8ScLRlkd"},"outputs":[],"source":["from transformers import BertTokenizer\n","\n","# Load the BERT tokenizer.\n","print('Loading BERT tokenizer...')\n","tokenizer = BertTokenizer.from_pretrained('bert-base-uncased', do_lower_case=True)"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["XwBJenjTQSO6"],"provenance":[{"file_id":"1ebp9aVPlje_yA1gfGveLdjmBTCbyT-8d","timestamp":1670259501349}],"mount_file_id":"1zl0AkYFg34_0Z5JQHg4nSJD93v_ugfuZ","authorship_tag":"ABX9TyMdnirTqdy7TcleQL3D8zs6"},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.9.13 64-bit","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.9.13 (main, Aug 29 2022, 05:50:54) \n[Clang 13.1.6 (clang-1316.0.21.2.5)]"},"vscode":{"interpreter":{"hash":"397704579725e15f5c7cb49fe5f0341eb7531c82d19f2c29d197e8b64ab5776b"}}},"nbformat":4,"nbformat_minor":0}