{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0a7a56b1",
   "metadata": {},
   "source": [
    "# Revised Odden's LDA \n",
    "(adapted for nlp in physics education project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4b2f2f1",
   "metadata": {},
   "source": [
    "# 05 - Data Cleaning for Science Education Articles"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "33c9cda5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>.container { width:95% !important; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Print out  all expressions\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\" #default 'last_expr'\n",
    "# Wider cells\n",
    "from IPython.core.display import display, HTML\n",
    "display(HTML(\"<style>.container { width:95% !important; }</style>\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "54be48c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Imports\n",
    "import pickle\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "#Import regular expressions, for data processing\n",
    "import re\n",
    "\n",
    "import gensim\n",
    "from gensim.parsing.preprocessing import STOPWORDS\n",
    "from gensim.corpora import Dictionary\n",
    "import nltk\n",
    "from nltk.corpus import wordnet\n",
    "from nltk import pos_tag\n",
    "nltk.download('wordnet',quiet=True)\n",
    "nltk.download('punkt',quiet=True)   #required by word_tokenize method\n",
    "nltk.download('averaged_perceptron_tagger',quiet=True) #required by pos_tag method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "2723d93f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import find_in_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "580116eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nimport importlib\\nimport helpers\\nimportlib.reload(helpers)\\n'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# reload any module\n",
    "'''\n",
    "import importlib\n",
    "import helpers\n",
    "importlib.reload(helpers)\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e074a8",
   "metadata": {},
   "source": [
    "## Reading the datafile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "afd8bb10",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ML_process(text):\n",
    "    filt_text = text\n",
    "    #remove 'cid'\n",
    "    filt_text = re.sub('\\W(cid:\\d{0,3})\\W', '', filt_text) #Symbols such as @    \n",
    "    #remove some words in all-caps\n",
    "    #USELESS NOW cause RawTextProcesser_* converted everything to lower case\n",
    "    filt_text = re.sub(r'(?<=\\W)(INTRODUCTION|CONCLUSION[S]?|BACKGROUND|ABSTRACT|ANALYSIS|EXPERIMENTAL|METHOD[S]?|METHODOLOGY|MOTIVATION[S]?|PRELIMINARY|RESULTS|APPLICATIONS|CONCLUDING|IMPLEMENTATION|EVALUATION|REMARKS|DISCUSSION[S]?|ACKNOWLEDGEMENTS|FUTURE PLANS|FUTURE WORK|FUTURE REASEARCH|SUMMARY|FIGURE[S]?|FIG|TABLE|I\\.|II|III|IV|VI{0,3}|IX|X|XI{0,3})(?=\\W)', \n",
    "                       '', filt_text)\n",
    "    #remove newlines, tabs, etc. also remove digits (\\d) and bullet points (\\uf0b7)\n",
    "    filt_text = re.sub('[\\t\\n\\r\\f\\v\\d\\uf0b7]', ' ', filt_text)\n",
    "    #removes all special characters that aren't numbers or letters\n",
    "    filt_text = re.sub('[^A-Za-z0-9]+', ' ', filt_text)\n",
    "    #split lines\n",
    "    filt_text = re.sub('- ', '', filt_text)\n",
    "    #to lower case\n",
    "    filt_text = filt_text.lower()\n",
    "    \n",
    "    #tlie -> the\n",
    "    filt_text = re.sub(' tlie ', ' the ', filt_text)\n",
    "    #per cent -> percent\n",
    "    filt_text = re.sub(' per cent ', ' percent ', filt_text)\n",
    "    # )ed -> fied\n",
    "    #filt_text = re.sub(re.escape(' \\)ed '), 'fied ', filt_text)\n",
    "    # - cation -> cation\n",
    "    #filt_text = re.sub('- cation ', 'cation ', filt_text)\n",
    "    return filt_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c10ef9c",
   "metadata": {},
   "source": [
    "# 06 - Tokenize_MakeBigrams"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b6f259e",
   "metadata": {},
   "source": [
    "## Removing stopwords and stemming\n",
    "\n",
    "Now, we can remove the stopwords and do the stemming, leaving us with a list of documents, each of which is essentially a tokenized list of words."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c5083b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sent_to_words(list_sentences):\n",
    "    return [gensim.utils.simple_preprocess(str(sentence), deacc=True) for sentence in list_sentences]  #deacc=True removes accent marks from tokens (incl. punctuations)\n",
    "\n",
    "def remove_stopwords(tokens):\n",
    "    return [[word for word in gensim.utils.simple_preprocess(str(doc)) if word not in STOPWORDS ] for doc in tokens]\n",
    "\n",
    "def get_wordnet_pos(word): #Provide a POS tag\n",
    "    \"\"\"Map POS tag to first character lemmatize() accepts\"\"\"\n",
    "    tag = pos_tag([word])[0][1][0].upper()\n",
    "    tag_dict = {\"J\": wordnet.ADJ,\n",
    "                \"N\": wordnet.NOUN,\n",
    "                \"V\": wordnet.VERB,\n",
    "                \"R\": wordnet.ADV}\n",
    "    return tag_dict.get(tag, wordnet.NOUN) #return NOUN by default\n",
    "\n",
    "def lemmatize_token(token):\n",
    "    return nltk.stem.WordNetLemmatizer().lemmatize(token, get_wordnet_pos(token))\n",
    "\n",
    "def lemmatize(token_list):\n",
    "    '''Input example: [\"he\", \"matches\", \"the\", \"profile\"]'''\n",
    "    return [lemmatize_token(token) for token in token_list]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abd6b3cb",
   "metadata": {},
   "source": [
    "# 07 - Choose_noabove_nobelow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec6a699a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Import usual data analysis tools\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import numpy as np\n",
    "#np.random.seed(2018)\n",
    "\n",
    "import pickle\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=DeprecationWarning);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55619aa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helpers import plot_freq_dist,get_top_n_words,plot_words_freq"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f2de39",
   "metadata": {},
   "source": [
    "## II. Loading in and filtering the data\n",
    "\n",
    "The datafile we use for this analysis is a pickle file containing processed versions of PERC papers from 2001 to 2018. We have scraped the available PDFs, then done the following data cleaning on the scraped text:\n",
    "1. Removed references, acknowledgments, keywords, and PACS \n",
    "2. Removed all numbers, symbols, punctuation, characters, and section headers\n",
    "3. Removed \"stop words\" (words like \"and\", \"or\", \"is\", etc. which do not carry specific meaning)\n",
    "4. Lowercased all words\n",
    "5. Lemmatized all words, reducing them to their more basic form (for example, reducing \"tests\", \"testing\", and \"tested\" to \"test\")\n",
    "6. Created bi-grams: combining commonly-associated words into one (for example, \"problem\" and \"solving\" into \"problem_solving\")\n",
    "7. Turned the resulting text into a list of individual words, or \"tokens\"\n",
    "\n",
    "This processed data was then stored in a datafile, which we now load in:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e960af4",
   "metadata": {},
   "source": [
    "data_words_bigrams = pd.read_pickle(path_pkl+'scied_words_bigrams_V5.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd7a48a",
   "metadata": {},
   "source": [
    "### A. Plotting top words in all documents\n",
    "\n",
    "Now, we will do some investigation and filtering based on word frequency. Our goal is to filter out the words that occur in a large number of documents, which are less likely to carry any distinct meaning for any specific theories, methods, or research traditions in PER. For example, most people in the PER community talk about \"physics\", \"education\", and \"students\" in one form or another. Those words do not carry much meaning, and so should be removed from our dataset in order to make sure that the more interesting, distinct, and meaningful words are prioritized in the analysis.\n",
    "\n",
    "We start by defining and implementing some functions to plot the word frequency distribution in the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef6e92c8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
